数据集配置文件示例（dataset.yaml）：
train: path/to/your/train/images  # 训练图像文件夹路径
val: path/to/your/val/images      # 验证图像文件夹路径

names:
  0: defect_type_1
  1: defect_type_2
  # 添加更多类别



目录结构示例：
datasets/
    my_dataset/
        images/
            train/  # 训练图片
            val/    # 验证图片
        labels/
            train/  # 训练标签 (YOLO格式)
            val/    # 验证标签 (YOLO格式)











########################yolov11 使用預測多種類/整體   缺陷情況 sample code###########################

import torch
from yolov11 import YOLOv11  # 假设有一个 YOLOv11 的 Python 实现

# 加载预训练的 YOLOv11 模型
model = YOLOv11(pretrained=True)
model.eval()

# 定义验证数据集的路径
val_data_path = 'path/to/your/validation/dataset'  # 替换为您的验证数据集路径

# 执行验证
results = model.val(data=val_data_path)

# 输出整体评估指标
print(f"Overall mAP@0.5: {results['mAP_0.5']:.4f}")
print(f"Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
print(f"Overall Precision: {results['precision']:.4f}")
print(f"Overall Recall: {results['recall']:.4f}")
print(f"Overall F1 Score: {results['f1']:.4f}")

# 输出每个类别的评估指标
for idx, class_name in enumerate(results['names']):
    class_precision = results['per_class_precision'][idx]
    class_recall = results['per_class_recall'][idx]
    class_f1 = results['per_class_f1'][idx]
    class_ap = results['per_class_ap'][idx]
    print(f"Class: {class_name}")
    print(f"  Precision: {class_precision:.4f}")
    print(f"  Recall: {class_recall:.4f}")
    print(f"  F1 Score: {class_f1:.4f}")
    print(f"  AP@0.5: {class_ap:.4f}")



注意事项：

结果字典结构：上述代码假设 results 字典包含以下键：

'mAP_0.5'：整体 mAP@0.5 值。
'mAP_0.5:0.95'：整体 mAP@0.5:0.95 值。
'precision'：整体精度。
'recall'：整体召回率。
'f1'：整体 F1 分数。
'names'：类别名称列表。
'per_class_precision'：每个类别的精度列表。
'per_class_recall'：每个类别的召回率列表。
'per_class_f1'：每个类别的 F1 分数列表。
'per_class_ap'：每个类别的 AP@0.5 值列表。
模型实现：确保您使用的 YOLOv11 实现的 val 方法返回的 results 字典包含上述键。如果没有，您可能需要查看模型的源码，了解如何提取每个类别的评估指标。

评估指标计算：如果模型未提供每个类别的指标，您可能需要手动计算。这涉及将预测结果与真实标签进行比较，计算每个类别的 TP、FP、FN，然后根据这些值计算精度、召回率和 F1 分数。这可能需要深入了解模型的输出格式和评估流程。

通过上述修改，您的代码将能够输出每个缺陷类别的评估指标，帮助您更细致地评估模型在各个类别上的性能。




在目标检测任务中，评估模型性能的指标主要包括精确率（Precision）、召回率（Recall）、F1 分数（F1 Score）、平均精度（AP）和平均精度均值（mAP）。理解这些指标的计算公式和目的，有助于判断模型的优劣，并向他人清晰地解释模型性能。

1. 精确率（Precision）

计算公式：

Precision
=
𝑇
𝑃
𝑇
𝑃
+
𝐹
𝑃
Precision= 
TP+FP
TP
​
 

其中，TP（True Positive）表示被正确检测为目标的数量，FP（False Positive）表示被错误检测为目标的数量。

目的：

精确率衡量模型预测的准确性，即模型预测为目标的实例中，有多少是真正的目标。高精确率表示模型的误报率低。

2. 召回率（Recall）

计算公式：

Recall
=
𝑇
𝑃
𝑇
𝑃
+
𝐹
𝑁
Recall= 
TP+FN
TP
​
 

其中，FN（False Negative）表示被漏检的目标数量。

目的：

召回率衡量模型的检测能力，即实际存在的目标中，有多少被模型成功检测出来。高召回率表示模型的漏检率低。

3. F1 分数（F1 Score）

计算公式：

𝐹
1
=
2
×
Precision
×
Recall
Precision
+
Recall
F1=2× 
Precision+Recall
Precision×Recall
​
 

目的：

F1 分数是精确率和召回率的调和平均，提供了两者之间的平衡评估。当需要在精确率和召回率之间取得平衡时，F1 分数是一个有效的指标。

4. 平均精度（AP）

计算方法：

AP 是 Precision-Recall 曲线下的面积。通过改变检测阈值，绘制出不同的精确率和召回率组合，形成曲线，然后计算该曲线下的面积即为 AP。

目的：

AP 衡量模型在特定类别上的检测性能，综合考虑了不同阈值下的精确率和召回率。

5. 平均精度均值（mAP）

计算方法：

mAP 是对所有类别的 AP 取平均值。

目的：

mAP 提供了模型在所有类别上的总体检测性能，是评估多类别目标检测模型的关键指标。

如何判断模型的好坏

在评估模型性能时，应综合考虑上述指标：

高精确率和高召回率：理想情况下，模型应具有高精确率和高召回率，即误报和漏检都少。

F1 分数：当需要在精确率和召回率之间取得平衡时，关注 F1 分数。

AP 和 mAP：对于多类别检测任务，关注每个类别的 AP 和整体的 mAP，以评估模型在各类别上的性能和总体表现。

通过全面分析这些指标，可以更准确地判断模型的优劣，并为模型的改进提供指导。
############################################################################################################################








##########################################
import torch
from yolov11 import YOLOv11  # 假设有一个 YOLOv11 的 Python 实现

# 加载预训练的 YOLOv11 模型
model = YOLOv11(pretrained=True)
model.eval()

# 定义验证和测试数据集的路径
val_data_path = 'path/to/your/validation/dataset'  # 替换为您的验证数据集路径
test_data_path = 'path/to/your/test/dataset'       # 替换为您的测试数据集路径

def evaluate_model(data_path, dataset_type='Validation'):
    """
    使用指定的数据集评估模型性能。

    参数：
    - data_path: 数据集的路径
    - dataset_type: 数据集类型（'Validation' 或 'Test'）
    """
    print(f"Evaluating on {dataset_type} Dataset...")
    results = model.val(data=data_path)

    # 输出整体评估指标
    print(f"{dataset_type} Dataset - Overall mAP@0.5: {results['mAP_0.5']:.4f}")
    print(f"{dataset_type} Dataset - Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
    print(f"{dataset_type} Dataset - Overall Precision: {results['precision']:.4f}")
    print(f"{dataset_type} Dataset - Overall Recall: {results['recall']:.4f}")
    print(f"{dataset_type} Dataset - Overall F1 Score: {results['f1']:.4f}")

    # 输出每个类别的评估指标
    for idx, class_name in enumerate(results['names']):
        class_precision = results['per_class_precision'][idx]
        class_recall = results['per_class_recall'][idx]
        class_f1 = results['per_class_f1'][idx]
        class_ap = results['per_class_ap'][idx]
        print(f"Class: {class_name}")
        print(f"  Precision: {class_precision:.4f}")
        print(f"  Recall: {class_recall:.4f}")
        print(f"  F1 Score: {class_f1:.4f}")
        print(f"  AP@0.5: {class_ap:.4f}")

# 对验证数据集进行评估
evaluate_model(val_data_path, 'Validation')

# 对测试数据集进行评估
evaluate_model(test_data_path, 'Test')


注意事项：

数据集配置：确保 val_data_path 和 test_data_path 分别指向验证和测试数据集的配置文件（通常为 .yaml 格式）。这些配置文件应包含数据集的相关信息，如图像路径、标签路径和类别名称等。

评估方法：上述代码假设 YOLOv11 类提供了 val 方法，该方法可以接受数据集配置文件的路径，并返回包含评估指标的结果字典。如果您的实现有所不同，请根据实际情况进行调整。

性能指标：results 字典应包含整体和每个类别的评估指标，如 mAP、Precision、Recall、F1 Score 等。如果某些指标不可用，请根据您的需求和模型实现进行相应的修改。

通过上述代码，您可以对验证和测试数据集进行评估，并输出模型在不同数据集上的性能指标。这有助于全面了解模型的表现，并为进一步的优化提供参考。
##########################################################################################################







#####################################################################################################

1. 数据集目录结构示例
假设您的数据集存放在一个根目录下，目录结构如下：


dataset/
├── images/
│   ├── train/     # 训练图像
│   ├── val/       # 验证图像
│   └── test/      # 测试图像
└── labels/
    ├── train/     # 训练标签
    ├── val/       # 验证标签
    └── test/      # 测试标签
images/ 文件夹中存放所有图像文件。

train/ 用于训练的图像。
val/ 用于验证的图像。
test/ 用于测试的图像。
labels/ 文件夹中存放图像对应的标签文件。

每个标签文件的文件名与对应图像相同（但扩展名为 .txt），文件内容采用 YOLO 格式。
2. 标签文件格式说明
每个标签文件采用 YOLO 格式，一行对应图像中的一个目标，格式如下：


<class_id> <x_center> <y_center> <width> <height>
其中：
<class_id>：目标类别的索引（整数），对应配置文件中 names 列表的索引（从 0 开始）。
<x_center> 和 <y_center>：边界框中心点的归一化坐标（相对于图像宽度和高度，取值范围 0～1）。
<width> 和 <height>：边界框的归一化宽度和高度（相对于图像尺寸，取值范围 0～1）。
示例：
假设有一张验证图像 image1.jpg，对应的标签文件 image1.txt 内容如下：

0 0.5 0.5 0.2 0.3
1 0.7 0.8 0.1 0.1
表示图像中有两个目标，第一个目标类别为 0，中心在图像正中央，宽度为图像宽度的 20%、高度为 30%；第二个目标类别为 1，中心位于图像宽度 70% 和高度 80% 的位置，边界框尺寸为图像尺寸的 10%。

3. 数据集配置文件（dataset.yaml）
创建一个名为 dataset.yaml 的文件，用于指定数据集根目录、各子目录及类别信息。例如：

# 数据集配置文件

# 图像和标签的根目录（请替换为您的实际路径）
path: path/to/your/dataset

# 训练、验证和测试图像的子目录（相对于 path）
train: images/train
val: images/val
test: images/test

# 类别数（请替换为您的类别数量）
nc: 3

# 类别名称列表（按索引顺序对应标签中的 class_id）
names:
  0: defect_type_1
  1: defect_type_2
  2: defect_type_3
请确保将 path/to/your/dataset 替换为您的数据集根目录实际路径。

4. 评估代码示例
下面的代码使用您提供的评估代码，并演示如何利用数据集配置文件（例如 dataset.yaml）对验证集和测试集进行评估：




import torch
from yolov11 import YOLOv11  # 假设有一个 YOLOv11 的 Python 实现

# 加载预训练的 YOLOv11 模型
model = YOLOv11(pretrained=True)
model.eval()

# 定义数据集配置文件的路径（包含验证和测试集的配置信息）
data_config_path = 'path/to/your/dataset.yaml'  # 替换为您的数据集配置文件路径

def evaluate_model(data_path, dataset_type='Validation'):
    """
    使用指定的数据集评估模型性能。

    参数：
    - data_path: 数据集配置文件的路径
    - dataset_type: 数据集类型（'Validation' 或 'Test'）
    """
    print(f"Evaluating on {dataset_type} Dataset...")
    results = model.val(data=data_path)

    # 输出整体评估指标
    print(f"{dataset_type} Dataset - Overall mAP@0.5: {results['mAP_0.5']:.4f}")
    print(f"{dataset_type} Dataset - Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
    print(f"{dataset_type} Dataset - Overall Precision: {results['precision']:.4f}")
    print(f"{dataset_type} Dataset - Overall Recall: {results['recall']:.4f}")
    print(f"{dataset_type} Dataset - Overall F1 Score: {results['f1']:.4f}")

    # 输出每个类别的评估指标
    for idx, class_name in enumerate(results['names']):
        class_precision = results['per_class_precision'][idx]
        class_recall = results['per_class_recall'][idx]
        class_f1 = results['per_class_f1'][idx]
        class_ap = results['per_class_ap'][idx]
        print(f"Class: {class_name}")
        print(f"  Precision: {class_precision:.4f}")
        print(f"  Recall: {class_recall:.4f}")
        print(f"  F1 Score: {class_f1:.4f}")
        print(f"  AP@0.5: {class_ap:.4f}")

# 示例用法：分别对验证和测试数据集进行评估
# 如果验证集和测试集都在 dataset.yaml 中定义，可以直接用同一个配置文件进行评估，
# 模型内部会根据配置文件中相应的 'val' 或 'test' 字段加载对应数据。

print("----- 验证集评估 -----")
evaluate_model(data_config_path, 'Validation')

print("\n----- 测试集评估 -----")
evaluate_model(data_config_path, 'Test')
说明：

在上面的代码中，data_config_path 指向的是数据集配置文件（dataset.yaml），该文件中已经包含了训练、验证和测试集的图像目录信息以及类别设置。
evaluate_model 函数调用 model.val(data=data_path)，这假设您的 YOLOv11 实现支持直接使用配置文件进行评估，并根据配置文件加载相应的数据集。
根据实际实现，如果需要分别指定验证集和测试集的路径，可以调整配置文件或分别传入不同的路径。


#########################################################




from ultralytics import YOLO  # 使用官方API
import torch
from tabulate import tabulate

# 加载官方预训练模型 (注意正确模型名称)
model = YOLO('yolov11n.pt')  # 官方模型名为yolov11n而非yolo11n
model.eval()

def evaluate_model(data_config, dataset_type='val'):
    """
    优化后的多分类评估函数（符合Ultralytics标准）
    
    参数：
    - data_config: 数据集配置文件路径 (.yaml)
    - dataset_type: 数据集类型 ('val' 或 'test')
    """
    print(f"\n{'='*30} {dataset_type.upper()} 评估 {'='*30}")
    
    # 执行验证（添加关键参数）
    results = model.val(
        data=data_config,
        split=dataset_type,  # 官方使用split参数
        plots=True,          # 生成可视化图表
        save_json=True,      # 保存JSON格式结果
        conf=0.01,           # 置信度阈值
        iou=0.6              # IoU阈值
    )
    
    # 全局指标输出
    print(f"\n[全局指标]")
    print(f"mAP@0.5:0.95 | {results.box.map:.4f} (COCO标准)")
    print(f"mAP@0.5      | {results.box.map50:.4f}")
    print(f"Precision    | {results.box.precision.mean():.4f}")
    print(f"Recall       | {results.box.recall.mean():.4f}")
    print(f"推理速度     | {results.speed['inference']:.2f} ms/张")

    # 多分类详细指标（表格化输出）
    print("\n[分类别指标]")
    table_data = []
    for idx, name in enumerate(results.names):
        row = [
            name,
            f"{results.box.ap50[idx]:.4f}",
            f"{results.box.ap75[idx]:.4f}",
            f"{results.box.ap[idx]:.4f}",
            f"{results.box.precision[idx]:.4f}",
            f"{results.box.recall[idx]:.4f}"
        ]
        table_data.append(row)
    
    print(tabulate(table_data,
                 headers=['类别', 'AP50', 'AP75', 'AP50-95', 'Precision', 'Recall'],
                 tablefmt='github'))
    
    # 可视化保存
    print("\n生成可视化文件：")
    print(f"- 混淆矩阵: runs/detect/val/confusion_matrix.png")
    print(f"- 预测示例: runs/detect/val/val_batch_pred.jpg")
    print(f"- JSON结果: runs/detect/val/results.json")

# 配置文件路径（示例）
data_config_path = 'path/to/your/dataset.yaml'

# 执行验证
if __name__ == "__main__":
    # 验证集评估
    evaluate_model(data_config_path, 'val')
    
    # 测试集评估（需要配置文件中定义test路径）
    evaluate_model(data_config_path, 'test')



主要改进点说明：
API标准化：
使用from ultralytics import YOLO替代自定义导入
修正模型名称yolov11n.pt（官方命名规范）
使用split参数替代自定义类型判断
指标输出优化：
增加AP75指标输出
添加推理速度显示
使用表格化输出分类别指标
符合COCO标准指标命名（box.map / box.map50）
参数优化：
添加save_json=True保存可解析结果
设置合理默认值conf=0.01和iou=0.6
自动生成可视化文件路径提示
多分类支持强化：
显示每个类别的AP50/AP75/AP50-95
分类别精确率和召回率单独显示
兼容不同数据分割（val/test）
使用前需确保：
bash
pip install ultralytics tabulate
注意：实际使用时请确保：
dataset.yaml中正确定义test路径
测试集标签文件符合YOLO格式
模型文件与任务匹配（分类数一致）
############################################






YOLOv11 多分类物体检测评估完整指南
一、环境准备
bash
# 安装核心依赖
pip install ultralytics tabulate matplotlib torch
二、数据集配置标准
1. 目录结构规范
bash
datasets/
└── defect_detection/
    ├── train/
    │   ├── images/  # 训练集图像
    │   └── labels/  # YOLO格式标签
    ├── val/
    │   ├── images/  # 验证集图像
    │   └── labels/
    └── test/
        ├── images/  # 测试集图像
        └── labels/
2. 标签文件示例
labels/train/image001.txt:
text
0 0.347656 0.489583 0.128906 0.239583
2 0.712891 0.581250 0.089844 0.166667
3. 数据集配置文件 (defect_config.yaml)
text
path: /projects/datasets/defect_detection
train: train/images
val: val/images
test: test/images

nc: 3
names:
  0: scratch
  1: dent
  2: crack
三、评估代码实现
python
from ultralytics import YOLO
from tabulate import tabulate

def main():
    # 初始化模型
    model = YOLO('yolov11n.pt')
    
    # 执行评估流程
    run_evaluation(model, 'defect_config.yaml')

def run_evaluation(model, config_path):
    """全流程评估函数"""
    # 验证集评估
    print("\n" + "="*40)
    print("Starting Validation Evaluation")
    results_val = evaluate_model(model, config_path, 'val')
    
    # 测试集评估
    print("\n" + "="*40)
    print("Starting Test Evaluation")
    results_test = evaluate_model(model, config_path, 'test')
    
    return results_val, results_test

def evaluate_model(model, data_config, dataset_type='val'):
    """模型评估核心函数"""
    print(f"\n{'='*30} {dataset_type.upper()} EVALUATION {'='*30}")
    
    # 执行验证流程
    results = model.val(
        data=data_config,
        split=dataset_type,
        plots=True,
        save_json=True,
        conf=0.01,
        iou=0.6,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # 输出评估指标
    print_metrics(results)
    
    return results

def print_metrics(results):
    """结构化输出评估指标"""
    # 全局指标
    print("\n[Global Metrics]")
    print(f"mAP@0.5:0.95 | {results.box.map:.4f} (COCO Primary)")
    print(f"mAP@0.5      | {results.box.map50:.4f}")
    print(f"Precision    | {results.box.precision.mean():.4f} ±{results.box.precision.std():.4f}")
    print(f"Recall       | {results.box.recall.mean():.4f} ±{results.box.recall.std():.4f}")
    print(f"Inference Speed | {results.speed['inference']:.2f} ms/img")
    
    # 分类别指标表格
    print("\n[Per-Class Metrics]")
    table_data = [
        [
            name,
            f"{results.box.ap50[idx]:.4f}",
            f"{results.box.ap75[idx]:.4f}",
            f"{results.box.ap[idx]:.4f}",
            f"{results.box.precision[idx]:.4f}",
            f"{results.box.recall[idx]:.4f}"
        ]
        for idx, name in enumerate(results.names)
    ]
    
    print(tabulate(table_data,
                 headers=['Class', 'AP50', 'AP75', 'AP50-95', 'Precision', 'Recall'],
                 tablefmt='github'))
    
    # 输出文件路径
    print("\nGenerated Files:")
    print(f"- 混淆矩阵: runs/detect/{results.save_dir}/confusion_matrix.png")
    print(f"- 预测可视化: runs/detect/{results.save_dir}/val_batch_pred.jpg")
    print(f"- 详细结果: runs/detect/{results.save_dir}/results.json")

if __name__ == "__main__":
    main()
四、关键参数说明
参数	类型	默认值	说明
split	str	'val'	数据集分割类型 (val/test)
plots	bool	True	生成可视化图表
save_json	bool	True	保存JSON格式结果
conf	float	0.01	检测置信度阈值
iou	float	0.6	IoU阈值
device	str	auto	计算设备自动选择
五、执行与输出解读
1. 运行命令
bash
python evaluate.py
2. 典型输出示例
text
============================== VAL EVALUATION =============================

[Global Metrics]
mAP@0.5:0.95 | 0.6723 (COCO Primary)
mAP@0.5      | 0.8521
Precision    | 0.7812 ±0.032
Recall       | 0.6934 ±0.041
Inference Speed | 4.23 ms/img

[Per-Class Metrics]
| Class    |   AP50 |   AP75 |   AP50-95 |   Precision |   Recall |
|----------|--------|--------|-----------|-------------|----------|
| scratch  | 0.8723 | 0.7021 |    0.6423 |      0.8023 |   0.7123 |
| dent     | 0.8345 | 0.6532 |    0.5934 |      0.7623 |   0.6834 |
| crack    | 0.8012 | 0.5921 |    0.5432 |      0.7321 |   0.6532 |
3. 输出文件说明
confusion_matrix.png: 类别混淆矩阵
val_batch_pred.jpg: 典型检测结果可视化
results.json: 包含所有指标的JSON文件
六、高级配置建议
多GPU评估加速：
python
model.val(..., device=[0,1,2,3])  # 使用4块GPU
批量大小优化：
python
model.val(..., batch=64)  # 根据显存调整
特定类别分析：
python
# 在print_metrics函数中添加
print(f"\nClass 'crack'详细指标:")
print(f"- 查准率: {results.box.precision[2]:.4f}")
print(f"- 漏检率: {1 - results.box.recall[2]:.4f}")
本指南完整实现了YOLOv11的评估流程，包含多分类检测的关键指标分析，可满足工业级缺陷检测、商品识别等场景的评估需求。






第二部分：差异图计算

metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = get_diff_map(ref, test, fn_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_thres
###############################




###############################lrf 回家作業#############################################
def get_lrf_file(data_dir, selected_lot_id):
    if os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_ADD.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_ADD.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_Classified.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_Classified.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_classified.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_classified.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}.lrf")
    else:
        lrf_file = None

    return lrf_file

def read_lrf_file(file_path):
    with open(file_path, 'r') as file:
        content = file.read()
    return content

def detect_defect_list(content):
    pattern = re.compile(r'\[DefectList\]\s+DefectDataColumn.*?DefectDataList\s+\d+\s+(.*?)\s+(?=\[|$)', re.DOTALL)
    match = pattern.search(content)
    if match:
        # logger.debug("DefectList found in the LRF file.")
        return match.group(1)
    # logger.debug("No DefectList found in the LRF file.")
    return None

def extract_no_and_classtype(defect_data):
    # logger.debug(f"defect_data: {defect_data}")

    lines = defect_data.strip().split('\n')
    results = []
    for line in lines:
        parts = line.split()
        if len(parts) >= 9:
            no = parts[0]
            classtype = parts[8]
            results.append((no, classtype))
    # logger.debug(f"results: {results}")
    return results

def get_defect_list(lrf_path):
    # if data_type == "old_data":
    file_path = lrf_path
    content = read_lrf_file(file_path)
    defect_data = detect_defect_list(content)
    image_list = []
    defect_type = []
    if defect_data:
        results = extract_no_and_classtype(defect_data)
        for no, classtype in results:
            # if classtype == '0' or classtype == '1':
            image_list.append(no)
            defect_type.append(classtype)
    # logger.info(f"image_list:{image_list}")
    return image_list, defect_type 




[DefectList]
DefectDataColumn No X Y W H Kind Ch PixelCount ClassType U/L ShfitDir KindBits MaskToMask Block SubBlock StripeNo Label ChgDiff Comment;
DefectDataList 2518
         1 -65575.3190  48572.2706      2.5000      0.8996 0000000000008c8c 00600000 254   0   2   0 0d000000000d00000008011881100f00000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 
         2 -65509.8190  13637.9665      1.2000      0.8496 0000000000000b08 00000060 185   0   2   0 00000000000000c40008011881100f00000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 
         3 -65505.8690  45174.8051      0.5000      0.1999 0000000000004b00 00000030  17   0   2   0 0080000000c004c40008011000000000000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 

        2514  63532.3185  -7693.8569      0.3250      0.1499 0000000000000b00 02000000   6   0   2   0 0000000000800c040008000000000000000000000000000000000000000000000000000000000000   1   0  -1 1076       -1     -999 
        2515  63758.7310   9989.8003      0.7500      0.5997 000000000000000b 10000000  11  10   2   0 00000000000000330000000081100100000000000000000000000000000000000000000000000000   1   0  -1 1078       -1     -999 
        2516  64450.4435  14399.3323      1.0750      0.7496 0000000000000b0b 00000020  60   0   2   0 00000000000000f70008211081100f00000000000000000000000000000000000000000000000000   1   0  -1 1083       -1     -999 
        2517  64592.7935  38925.8217      2.6250      0.9495 0000000000004b4b 60000000 403   0   2   0 00b0000000c004f70008011881100f00000000000000000000000000000000000000000000000000   1   0  -1 1085       -1     -999 
        2518  64941.1060  42573.1883      0.3500      0.4998 0000000000000100 0000000c  21  10   2   0 00000000000004040000000000000000000000000000000000000000000000000000000000000000   1   0  -1 1087       -1     -999

######################################labeling config

interfaces = [
  "panel",
  "update",
  "controls",
  "side-column",
  "completions:menu",
  "completions:add-new",
  "completions:delete",
  "predictions:menu",
],

user = {
  'pk': 1,
  'firstName': "Labeler",
  'lastName': "",
},
###############################json function

interfaces = [
  "panel",
  "update",
  "controls",
  "side-column",
  "completions:menu",
  "completions:add-new",
  "completions:delete",
  "predictions:menu",
],

user = {
  'pk': 1,
  'firstName': "Labeler",
  'lastName': "",
},

################## image processor
from PIL import Image
import base64
from io import BytesIO
import numpy as np
import json
import cv2
from alignment.alignment import load_defect_pair, get_diff_map
import matplotlib.pyplot as plt
# Function to convert a NumPy array to a Base64 encoded string

def numpy_to_base64(img_array):
    pil_img = Image.fromarray(img_array)
    buffer = BytesIO()
    pil_img.save(buffer, format="JPEG")
    return base64.b64encode(buffer.getvalue()).decode("utf-8")


# Function to load images from a JSON file line by line and save into a dictionary
def load_images_from_json(json_file):
    images = {}
    with open(json_file, 'r') as f:
        for line in f:
            data = json.loads(line)
            lot_id = data['lot_id']
            images[lot_id] = data
    return images

def apply_colormap(np_array, colormap, vmin=None, vmax=None):
    # norm_array = (np_array - np_array.min()) / (np_array.max() - np_array.min())
    if vmin and vmax:
        norm_array = (np_array - vmin) / (vmax - vmin)
    else:
        norm_array = np_array
    colormap_func = plt.get_cmap(colormap)
    colored_array = colormap_func(norm_array)
    return (colored_array[:, :, :3] * 255).astype(np.uint8)

# Define a 2x2 convolution kernel
def conv_kernel(kernel_size):
    return np.ones((kernel_size, kernel_size), dtype=np.float32) / (kernel_size ** 2)

def get_image_pair_for_studio_input(data_dir, selected_lot_id, selected_image_id, selected_image_type, vmin_level, max_features, max_shift, ransac_reproj_threshold, selected_alignment_method, conv_kernel_size):
    img_dir = f"{data_dir}/{selected_lot_id}/{selected_lot_id}/Images/InstantReview{selected_image_type}"
    fn_id = selected_image_id
    ref, test, is_median_ref = load_defect_pair(img_dir, fn_id)
    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
        get_diff_map(ref, test, fn_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method=selected_alignment_method)
    print(f"max_pos:{metadata['max_pos']}")
    print(f"min_pos:{metadata['min_pos']}")

    ref_colored = apply_colormap(proc_ref, 'gray')
    test_colored = apply_colormap(proc_test, 'gray')
    diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)
    conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))
    img1_base64 = numpy_to_base64(ref_colored)
    img2_base64 = numpy_to_base64(test_colored)
    img3_base64 = numpy_to_base64(diff_colored)
    img4_base64 = numpy_to_base64(conv_diff_image)
    return [img1_base64, img2_base64, img4_base64], metadata

##############inference ########################################


def load_image_as_numpy(image_path):
    img = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format
    return np.array(img)

def preprocess_image(img_np):
    if img_np.ndim == 2:  # If the image is grayscale, convert to RGB
        img_np = np.stack((img_np,) * 3, axis=-1)
    elif img_np.shape[2] == 4:  # If the image has an alpha channel, remove it
        img_np = img_np[:, :, :3]
    return img_np

def inference(img_np, ckpt="/home/hubert007/Code/label_tool/labeling/runs/detect/train3/weights/best.pt"):
    # Load the finetuned model checkpoint
    model = YOLO(ckpt)
    
    # Preprocess the image
    img_np = preprocess_image(img_np)
    
    # Perform object detection
    results = model(img_np)
    # print(results['precision'])
    # print(results['recall'])

    # logger.info(f"inference results: {results}")
    # Extract bounding box information
    if results and len(results[0].boxes) > 0:
        boxs_and_labels = []
        # logger.debug(f"cls length: {len(results[0].boxes.cls)}")
        # logger.debug(f"cls shape: {results[0].boxes.cls.shape}")
        for i in range(len(results[0].boxes.cls)):

            # logger.info(f"Detected boxs: {results[0].boxes}")

            box = results[0].boxes  # Assuming we take the first detected box
            label = box.cls[i]
            x1, y1, x2, y2 = box.xyxy[i]
            width = x2 - x1
            height = y2 - y1
            boxs_and_labels.append((x1, y1, width, height, label))
        return boxs_and_labels
    else:
        return None

# Example usage
if __name__ == "__main__":
    image_path = "/mnt/fs0/dataset/Layer_M/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/1411L.png"
    np_image = load_image_as_numpy(image_path)
    bbox = inference(np_image)
    # print('test result', bbox[0][0], bbox[0][1], bbox[0][2], bbox[0][3], bbox[0][4])  #ok  (x1, y1, width, height, label)

    
    if bbox:
        print(f"Top-left: ({bbox[0][0]}, {bbox[0][1]}), Width: {bbox[0][2]}, Height: {bbox[0][3]}, label: {bbox[0][4]}")
    else:
        print("No bounding box detected.")


######################################fine tune
from ultralytics import YOLO
import os
import argparse
from loguru import logger
import yaml


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=200, help='number of epochs')
    parser.add_argument('--imgsz', type=int, default=256, help='image size')
    parser.add_argument('--exp_name', type=str, default=None, help='name of your experiment')
    parser.add_argument('--dataset_path', type=str, required=True, help='dataset path')
    args = parser.parse_args()



    # Create the directory if it doesn't exist
    os.makedirs('prelabel/model_based/YOLO/train_settings', exist_ok=True)

    # Define the content of the YAML file
    yaml_content = {
        'path': args.dataset_path,
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images',
        'nc': 1,
        'names': {
            0: 'defect'
        }
    }

    # Write the content to the YAML file
    with open(f'prelabel/model_based/YOLO/train_settings/{args.exp_name}.yaml', 'w') as yaml_file:
        yaml.dump(yaml_content, yaml_file, default_flow_style=False)


    # Load a model
    model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

    train_results = model.train(data=f"prelabel/model_based/YOLO/train_settings/{args.exp_name}.yaml", epochs=args.epochs, imgsz=args.imgsz, name=args.exp_name)




    # Export the model to ONNX format
    path = model.export(format="onnx")  # return path to exported model

if __name__ == "__main__":
    main()



#########  dataset#################################################


import shutil
from sklearn.model_selection import train_test_split
import json
import os
from PIL import Image
import numpy as np
from torchvision.utils import save_image
import torch
import cv2
import re
import sqlite3
from PIL import Image, ImageDraw
from alignment.alignment import load_defect_pair, get_diff_map
from frontend.image_processor import numpy_to_base64, load_images_from_json, apply_colormap, conv_kernel
import argparse
from loguru import logger
from tqdm import tqdm
import albumentations as A
from albumentations.pytorch import ToTensorV2

def get_augmentation_pipeline(aug_params):
    """
    Create albumentations augmentation pipeline.
    We use "albumentations" because it can handle the augmentations of bounding boxes. For example, when we rotate the image,
    the bounding boxes should also be rotated.
    :param aug_params: dict of augmentation parameters
    :return: albumentations.Compose object with augmentation pipeline
    """
    return A.Compose([
        A.RandomRotate90(p=aug_params.get('rotation_prob', 0.5)),
        A.HorizontalFlip(p=aug_params.get('flip_prob', 0.5)),
        A.VerticalFlip(p=aug_params.get('flip_prob', 0.5)),
        A.Affine(
            scale=aug_params.get('scale', (1.0, 1.2)),
            translate_percent=(0.0, 0.0),
            p=1.0
        ),
        # A.ColorJitter(
        #     brightness=aug_params.get('brightness', 0.2),
        #     contrast=aug_params.get('contrast', 0.2),
        #     p=1.0
        # ),
        ToTensorV2()
    ], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))


def process_and_augment_images(image_array, bounding_boxes, aug_params, img_path, original=False):
    """
    augment the images and bounding boxes.
    """
    # Ensure the image is in RGB format
    if image_array.shape[2] == 4:  # If the image has an alpha channel
        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGRA2RGB)
    else:
        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)

    # Prepare bounding boxes and labels
    bboxes = []
    category_ids = []
    for bbox in bounding_boxes:
        x_center = (bbox['x']+bbox['width']/2) / image_array.shape[1]
        y_center = (bbox['y']+bbox['height']/2) / image_array.shape[0]
        width = bbox['width'] / image_array.shape[1]
        height = bbox['height'] / image_array.shape[0]
        x_center = min(1.0, max(0.0, x_center))
        y_center = min(1.0, max(0.0, y_center))
        width = min(1.0, max(0.0, width))
        height = min(1.0, max(0.0, height))

        label = bbox['label']
        
        if label == "Defect_right" or label == "Defect_left":
            category_ids.append(0)
        elif label == "4D":
            return None
        
        bboxes.append([x_center, y_center, width, height])

    # Apply augmentations
    if original == True:
        return bboxes, category_ids
    try:
        transform = get_augmentation_pipeline(aug_params)
        augmented = transform(image=image_array, bboxes=bboxes, category_ids=category_ids)
        return augmented['image'], augmented['bboxes'], augmented['category_ids']
    except:
        logger.debug(f"error on {img_path}")
        return None

    # return augmented['image'], augmented['bboxes'], augmented['category_ids']

def check_image_variants(image_path):
    """
    Get the base path and file name without extension (1070L.png ,1070U.png ....)
    """
    logger.info(f"Checking image variants for {image_path}")
    base_path, file_name = os.path.split(image_path)
    file_name_no_ext = os.path.splitext(file_name)[0]
    
    # Define the variants
    variants = ['L', 'R', 'U', 'D']
    
    # Initialize a dictionary to store the paths of existing variants
    # Check for each variant
    for variant in variants:
        variant_path = os.path.join(base_path, f"{file_name_no_ext}{variant}.png")
        if os.path.exists(variant_path):
            return variant_path
        

    
def db_to_metadata(db_file, metadata_file, masked_image_output=None):
    """
    Convert SQLite database to metadata JSON file.
    """
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Query to retrieve all rows from the results_table
    cursor.execute('SELECT image_path, results_json FROM results_table')
    rows = cursor.fetchall()

    # Create output directory if it doesn't exist
    if masked_image_output:
        output_dir = masked_image_output
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    # Initialize a list to store metadata
    metadata_list = []

    for row in rows:
        img_path, results_json = row
        split_img_path = img_path.split(', ')
        data_dir = split_img_path[0]
        lot_id = split_img_path[1]
        image_id = split_img_path[2]
        results = json.loads(results_json)
        # logger.debug(f"img_path: {img_path}")
        # Check if 'areas' key exists in the JSON
        if 'areas' in results:
            t_boxes = []
            rt_boxes = []
            areas = results['areas']
            for area_key, area_value in areas.items():
                if area_value['object'] == 'image3' or area_value['object'] == 'image6':
                    label = area_value['results'][0]['value']['rectanglelabels'][0]
                    x = area_value['x']
                    y = area_value['y']
                    width = area_value['width']
                    height = area_value['height']
                    if area_value['object'] == 'image3':
                    # Append metadata to the list
                        t_boxes.append(
                            {"x": x, "y": y, "width": width, "height": height, "label": label} 
                        )
                    else:
                        rt_boxes.append(
                            {"x": x, "y": y, "width": width, "height": height, "label": label} 
                        )

            metadata_list.append({
                "image_path": f"{data_dir}/{lot_id}/{lot_id}/Images/InstantReviewT/{image_id}.png",
                "bounding_box": t_boxes,
            })
            metadata_list.append({
                "image_path": f"{data_dir}/{lot_id}/{lot_id}/Images/InstantReviewRt/{image_id}.png",
                "bounding_box": rt_boxes,
            })

    # Write metadata to a JSON file
    with open(metadata_file, 'w') as json_file:
        json.dump(metadata_list, json_file, indent=4)

    # Close the database connection
    conn.close()
    # json_to_text('metadata.json')

def create_dataset(db_file ,json_file, dataset_dir, image_size=256, diff_map=False, aug_params = None, use_background = False, max_augment_factor = 8):

    print(f"diff_map: {diff_map}")
    # Create dataset directories
    train_images_dir = os.path.join(dataset_dir, 'train/images')
    train_labels_dir = os.path.join(dataset_dir, 'train/labels')
    val_images_dir = os.path.join(dataset_dir, 'val/images')
    val_labels_dir = os.path.join(dataset_dir, 'val/labels')
    test_images_dir = os.path.join(dataset_dir, 'test/images')
    test_labels_dir = os.path.join(dataset_dir, 'test/labels')
    
    os.makedirs(train_images_dir, exist_ok=True)
    os.makedirs(train_labels_dir, exist_ok=True)
    os.makedirs(val_images_dir, exist_ok=True)
    os.makedirs(val_labels_dir, exist_ok=True)
    os.makedirs(test_images_dir, exist_ok=True)
    os.makedirs(test_labels_dir, exist_ok=True)

    # Load JSON data
    db_to_metadata(db_file, json_file)
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    # Split data into train and validation sets
    # First, split the data into train+val and test sets
    train_val_data, test_data = train_test_split(data, test_size=0.1, random_state=42)

    # Then, split the train+val data into train and val sets
    train_data, val_data = train_test_split(train_val_data, test_size=0.2222, random_state=42)  # 0.2222 is approximately 2/9

    # Now you have train_data (70%), val_data (20%), and test_data (10%)
    
    def process_data(data, images_dir, labels_dir, data_type):
        for item in tqdm(data, desc=f"Processing {data_type} data"):
            image_path = item['image_path']
            if diff_map:
                max_features = 1000
                max_shift = 10
                ransac_reproj_threshold = 0.20
                image_dir = os.path.dirname(image_path)
                # the lot_id doesn't matter here, it just affects the metadata created below, which won't be used in this case.
                selected_lot_id = "DummyVariable"
                vmin_level = -0.60
                conv_kernel_size = 2
                
                # Extract the filename
                filename = os.path.basename(image_path)
                if 'InstantReviewT' in image_path:
                    image_type = "T"
                elif 'InstantReviewRt' in image_path:
                    image_type = "Rt"

                # Extract the number part of the filename using regular expressions
                image_id = re.search(r'\d+', filename).group()

                ref, test, is_median_ref = load_defect_pair(image_dir, image_id)

                """
                I think there's a bottleneck here. When the image isn't able to be aligned with correlate, the correlation alignment function outputs an error which
                takes a makes the progress bar stuck for a while. This is worth checking out when optimizing the dataset pipeline. 
                """

                # Here we use Correlate for all images because it can handle broader cases than SIFT.
                try:
                    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
                        get_diff_map(ref, test, image_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method="Correlate")
                except:
                    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
                        get_diff_map(ref, test, image_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method="SIFT")                  
                #save proc_diff as png image
                diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)
                conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))
                diff_image_png = Image.fromarray(conv_diff_image)

                # Save as PNG
                
                label_file = f"{image_type}_{image_id}_diff.txt"
                label_path = os.path.join(labels_dir, label_file)             
                # remove last filename from image_path

                
                #Start augmentation
                bounding_boxes = item['bounding_box']

                #before augmentation, add original image and label into dataset
                
                if len(bounding_boxes) == 0:
                    if use_background == True:
                        continue
                    with open(label_path, "w") as f:
                        pass
                        
                else:
                    try:
                        aug_bboxes, aug_labels = process_and_augment_images(conv_diff_image, bounding_boxes, aug_params, image_path, original=True)
                    except:
                        continue
                    for box, label in zip(aug_bboxes, aug_labels):
                        with open(label_path, "a") as f:
                            f.write(f"{label} {box[0]} {box[1]} {box[2]} {box[3]}\n")
                diff_image_png.save(os.path.join(images_dir, f"{image_type}_{image_id}_diff.png"))

                num_of_augments = np.random.randint(1, max_augment_factor)
                
                for i in range(1, num_of_augments+1):
                    #random vmin level [-1.70, -0.30]
                    vmin_level = torch.FloatTensor(1).uniform_(-1.70, -0.30).item()
                    diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)

                    #apply random convolution kernel
                    conv_kernel_size = torch.randint(1, 4, (1,)).item()
                    conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))

                    #apply albumentation's augmentations.
                    try:
                        aug_image, aug_bboxes, aug_labels = process_and_augment_images(conv_diff_image, bounding_boxes, aug_params, image_path)
                    except:
                        continue
                    png_path = os.path.join(images_dir, f"{image_type}_{image_id}_diff_{str(i)}.png")
                    label_path = os.path.join(labels_dir, f"{image_type}_{image_id}_diff_{str(i)}.txt")

                    np_image = aug_image.permute(1, 2, 0).cpu().numpy()
                    
                    # Convert the NumPy array to a PIL image
                    pil_image = Image.fromarray(np_image)
                    
                    # Save the PIL image
                    pil_image.save(png_path)
                    if len(aug_bboxes) == 0:
                        if use_background == False:
                            continue
                        label = 'None'
                        with open(label_path, 'w') as f:
                            pass
                    else:
                        for bbox, label in zip(aug_bboxes, aug_labels):
                            with open(label_path, 'a') as f:
                                f.write(f"{label} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\n")
            else:
                # Copy image to the dataset directory
                shutil.copy(image_path, images_dir)
                
                # Create label file
                image_name = os.path.basename(image_path)
                label_file = os.path.splitext(image_name)[0] + '.txt'
                label_path = os.path.join(labels_dir, label_file)

    
    # Process train and validation data


    process_data(train_data, train_images_dir, train_labels_dir, "train")
    process_data(val_data, val_images_dir, val_labels_dir, "val")
    process_data(test_data, test_images_dir, test_labels_dir, "test")


def dataset_checker(dataset_path):
    # Directories to check
    directories = ['train', 'val', 'test']
    
    missing_labels = []
    empty_labels = []

    for directory in directories:
        image_dir = os.path.join(dataset_path, directory, 'images')
        label_dir = os.path.join(dataset_path, directory, 'labels')
        
        # Get list of image files
        image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]
        
        for image_file in image_files:
            label_file = os.path.splitext(image_file)[0] + '.txt'
            label_path = os.path.join(label_dir, label_file)
            
            if not os.path.exists(label_path):
                print(f"no label at {image_file}")
                missing_labels.append(os.path.join(directory, 'images', image_file))
            else:
                with open(label_path, 'r') as f:
                    content = f.read().strip()
                    if not content:
                        print(f"empty label at {label_file}")
                        empty_labels.append(os.path.join(directory, 'labels', label_file))
    
    # return missing_labels, empty_labels



def main():
    parser = argparse.ArgumentParser(description="Create YOLO dataset from .db file")
    parser.add_argument("--db_file" , type=str, help="Path to the .db file")
    parser.add_argument("--json_metadata", type=str, help="Path to the JSON metadata file")
    parser.add_argument("--dataset_path", type=str, help="Path to the dataset directory")
    parser.add_argument("--use_diff_map", type=bool, default=True, help="Use diff map as image for the dataset")
    parser.add_argument("--max_augment_factor", type=int, default=8, help="Max number of augmentation data on each image")
    parser.add_argument("--use_background", action='store_true', default=False, help="Use background images for the dataset")
    args = parser.parse_args()

    aug_params = {
        'rotation_prob': 0.5,
        'flip_prob': 0.5,
        'scale': (1.0, 1.2),
        'brightness': 0.2,
        'contrast': 0.2
    }
    create_dataset(args.db_file, args.json_metadata, args.dataset_path, diff_map=args.use_diff_map, aug_params=aug_params, use_background=args.use_background, max_augment_factor=args.max_augment_factor)


if __name__ == "__main__":
    main()
    # dataset_checker("/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/model_based/YOLO/datasets/m_layer_aug_fix")



######yolo prelabel training
from prelabel.model_based.YOLO.inference import inference


def YOLO_prelabel(t_np_image, rt_np_image, model_path, annotations):
        t_bbox = inference(t_np_image, model_path)
        if t_bbox != None:
            for box in t_bbox:
                x = box[0]
                y = box[1]
                width = box[2]
                height = box[3]
                label = box[4]
                for i in range(1, 4):
                    annotation = {
                        'from_name': f'label_image{i}',
                        'to_name': f'image{i}',
                        'type': 'rectanglelabels',
                        'value': {
                            'rectanglelabels': ['Defect_right'],
                            'x': x.item()*100/256,
                            'y': y.item()*100/256,
                            'width': width.item()*100/256,
                            'height': height.item()*100/256,
                        }
                    }       
                    annotations.append(annotation)       
        rt_bbox = inference(rt_np_image, model_path)
        if rt_bbox != None:
            for box in rt_bbox:
                x = box[0]
                y = box[1]
                width = box[2]
                height = box[3]
                label = box[4]
                # The main diference is here: it is using a different range for the loop (image 4,5,6)
                for i in range(4, 7):
                    annotation = {
                        'from_name': f'label_image{i}',
                        'to_name': f'image{i}',
                        'type': 'rectanglelabels',
                        'value': {
                            'rectanglelabels': ['Defect_right'],
                            'x': x.item()*100/256,
                            'y': y.item()*100/256,
                            'width': width.item()*100/256,
                            'height': height.item()*100/256,
                        }
                    }       
                    annotations.append(annotation)      
                return annotations


# YOLO prelabeler training


## Prepare dataset
**Run commands under /toggle_labelstudio**

After finishing labeling, you will get .db file under /toggle_labelstudio.
Convert that .db file into YOLO training format by running the following command:
```bash
python -m prelabel.model_based.YOLO.dataset --db_file example.db --json_metadata example.json --dataset_path dataset_path
```
* `db_file`: .db file path.
* `json_metadata`: desired metadata json file path (will create for you).
* `dataset_path`: output dataset directory (will create for you).

## Train model

### run finetune

```bash
python -m prelabel.model_based.YOLO.finetune --epochs 200 --imgsz 256 --exp_name "experiment name" --dataset_path "dataset_path"
```
Other arguments will be added later(TODO)

###################################################prelabel.py

import cv2
import numpy as np
import base64
from PIL import Image
from io import BytesIO
from prelabel.model_based.YOLO.api import YOLO_prelabel
from prelabel.rule_based.minmax.api import minmax_prelabel
from loguru import logger

def base64_to_numpy(base64_str):
    img_data = base64.b64decode(base64_str)
    img = Image.open(BytesIO(img_data))
    return np.array(img)

def extract_annotations(existing_labels):
    """
    This function extracts the bounding boxes from "existing_labels", which is the dictionary that "st_labelstudio" returns after submit.
    It is quite important to study how "existing_labels" is structured if you want to add new annotation types.
    "polygonlabels" is deprecated currently since the database doesn't handle that format, but as long as you 
    understand "existing_labels" format, you can add it back.
    """
    annotations = []
    if 'areas' in existing_labels:
        # areas describe all the bounded areas (bounding box, polygon..)
        for area_id, area_data in existing_labels['areas'].items():
            if 'results' in area_data:
                for result in area_data['results']:
                    annotation = {
                        "id": result["id"],
                        "from_name": result['from_name'],
                        "to_name": result['to_name'],
                        "type": result['type']
                    }
                    if annotation["type"] == "polygonlabels":
                        annotation["value"] = {
                            "polygonlabels": result['value']['polygonlabels'],
                            "points": [[point['relativeX'], point['relativeY']] for point in area_data['points']],
                        }
                    elif annotation["type"] == "rectanglelabels":
                        annotation["value"] = {
                            "rectanglelabels": result['value']['rectanglelabels'],
                            # converting absolute pixel values into percentage
                            "x": area_data['x']*100/256,
                            "y": area_data['y']*100/256,
                            "width": area_data['width']*100/256,
                            "height": area_data['height']*100/256,
                            "rotation": area_data['rotation'],
                        }
                    annotations.append(annotation)
    return annotations

def generate_prelabels(metadatas, crop_size, method="minmax", model_path=None, t_image=None, rt_image=None, label_type="rectangle"):
    """
    Call your prelabel logics here.
    You can call/define any prelabel method as long as the list "annotations" contains 6 elements each with the following format:
    {
        'from_name': f'label_image{i}',
        'to_name': f'image{i}',
        'type': 'rectanglelabels',
        'value': {
            'rectanglelabels': ['Defect_right'],
            'x': x coord of bounding box's TOP LEFT in PERCENTAGE [0, 100],
            'y': y coord of bounding box's TOP LEFT in PERCENTAGE [0, 100],
            'width': bounding box width in PERCENTAGE [0, 100],
            'height': bounding box height in PERCENTAGE [0, 100],
        }
    } 

    Basically you have to return six bounding boxes for i is from 1~6, representing 6 images (t_ref, t_test, t_diff, rt_ref, rt_test, rt_diff).
    Then these annotations will be rendered onto UI.
    """
    
    annotations = []
    if method == "YOLO":
        # why does YOLO only take in 2 images? Because we only predict on the diff map. t_image is the t_diffmap vice versa.
        t_np_image = base64_to_numpy(t_image)
        rt_np_image = base64_to_numpy(rt_image)
        YOLO_prelabel(t_np_image, rt_np_image, model_path, annotations)
   
    elif method == "minmax":
        # metadatas contains the min/max point, which is acquired from SIFT's alignment functions.
        # crop_size is the size of the bounding box to bound the min/max point. It can be toggled on sidebar.
        minmax_prelabel(metadatas, crop_size, annotations)
    return annotations

def task_generator(images, crop_size, metadatas=None, method="minmax", model_path=None, label_type="rectangle", existing_labels=None):
    """
    This is the MAIN function that handles our customize logic and pass it in to labelstudio as "predictions". 
    Be aware that both "existing labels" and "prelabel predictions" are passed in as "predictions".
    I didn't use "completions", you can ignore that, also you can disable its UI in the UI settings (gear icon).
    """
    task = {
        'completions': [],
        'predictions': [],
        'id': 1,
        'data': {
            'image1': f"data:image/jpeg;base64,{images[0]}",
            'image2': f"data:image/jpeg;base64,{images[1]}",
            'image3': f"data:image/jpeg;base64,{images[2]}",
            'image4': f"data:image/jpeg;base64,{images[3]}",
            'image5': f"data:image/jpeg;base64,{images[4]}",
            'image6': f"data:image/jpeg;base64,{images[5]}"
        }
    }
    if existing_labels:
        annotations = extract_annotations(existing_labels)
        task['predictions'].append({
            'model_version': 'existing_labels',
            'result': annotations
        })
    else:
        print("no labels found, generating prelabels...")
        annotations = generate_prelabels(metadatas, crop_size, method=method, model_path=model_path, t_image=images[2], rt_image=images[5], label_type=label_type)
        task['predictions'].append({
            'model_version': 'prelabeling',
            'result': annotations
        })

    return task


######export_json_and_mask

import sqlite3
import json
from PIL import Image, ImageDraw
import os
from tqdm import tqdm
import argparse

def check_image_variants(image_path):
    # Get the base path and file name without extension
    base_path, file_name = os.path.split(image_path)
    file_name_no_ext = os.path.splitext(file_name)[0]
    
    # Define the variants
    variants = ['L', 'R', 'U', 'D']
    
    # Initialize a dictionary to store the paths of existing variants
    # Check for each variant
    for variant in variants:
        variant_path = os.path.join(base_path, f"{file_name_no_ext}{variant}.png")
        if os.path.exists(variant_path):
            return variant_path
    
def process_db_and_create_masked_images(db_file, output_dir):
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Query to retrieve all rows from the results_table
    cursor.execute('SELECT image_path, results_json FROM results_table')
    rows = cursor.fetchall()

    # Create output directory if it doesn't exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Initialize a list to store metadata
    metadata_list = []
    for row in tqdm(rows, desc=f"Processing images"):
        img_info, results_json = row
        dataset_dir, lot_id, image_id = img_info.replace(" ","").split(',')
        results = json.loads(results_json)
        img_path = os.path.join(dataset_dir, f"{lot_id}/{lot_id}/Images")
        # Check if 'areas' key exists in the JSON
        if 'areas' in results:
            areas = results['areas']
            for area_key, area_value in areas.items():
                if area_value['object'] == 'image3' or area_value['object'] == 'image6':
                    if area_value['object'] == 'image3':
                        full_img_path = os.path.join(img_path, f"InstantReviewT/{image_id}.png")
                    else:
                        full_img_path = os.path.join(img_path, f"InstantReviewRt/{image_id}.png")
                    """
                    If it comes from the right, the defect image is "fn_id.png", if left, it would be "fn_idL.png" or "fn_idU.png" ... etc.
                    """
                    if area_value['results'][0]['value']['rectanglelabels'][0] == "Defect_left":
                        full_img_path = check_image_variants(full_img_path)
                    x = area_value['x']
                    y = area_value['y']
                    width = area_value['width']
                    height = area_value['height']

                    # Open the image
                    img = Image.open(full_img_path).convert("RGBA")
                    masked_img = Image.new("RGBA", img.size, (0, 0, 0, 255))
                    draw = ImageDraw.Draw(masked_img)

                    # Draw the bounding box on the masked image (make it transparent)
                    draw.rectangle([x, y, x + width, y + height], fill=(0, 0, 0, 0))

                    # Composite the original image with the masked image
                    final_img = Image.alpha_composite(img, masked_img)

                    # Save the final masked image
                    output_path = os.path.join(output_dir, f"{lot_id}_{os.path.basename(full_img_path)}")
                    final_img.save(output_path)
                    
                    # Append metadata to the list
                    metadata_list.append({
                        "image_path": full_img_path,
                        "bounding_box": {
                            "x": x,
                            "y": y,
                            "width": width,
                            "height": height
                        }
                    })

    # Write metadata to a JSON file
    with open(os.path.join(output_dir, 'metadata.json'), 'w') as json_file:
        json.dump(metadata_list, json_file, indent=4)

    # Close the database connection
    conn.close()

def main():
    parser = argparse.ArgumentParser(description="Process a SQLite database and create masked images.")
    parser.add_argument('--db_path', type=str, help='Path to the SQLite database file')
    parser.add_argument('--output_dir', type=str, help='Output directory for the masked images')
    args = parser.parse_args()
    process_db_and_create_masked_images(args.db_path, args.output_dir)


if __name__ == "__main__":
    main()



########################################metadata.json
[
    {
        "image_path": "/mnt/fs0/x9u_detection_result/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/771L.png",
        "bounding_box": {
            "x": 137,
            "y": 111,
            "width": 54,
            "height": 23
        }
    },
    {
        "image_path": "/mnt/fs0/x9u_detection_result/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/822L.png",
        "bounding_box": {
            "x": 124,
            "y": 118,
            "width": 12,
            "height": 15
        }
    },]
#################################alightment.py

"""
This file is modified from inspection_sift.py by Carl.
I've removed some functions since the app doesn't use it. 
The most important functions are "load_defect_pair" and "get_diff_map"
"""




import numpy as np
import cv2
import matplotlib.pyplot as plt
import math
import json
import glob
import os
from loguru import logger
from alignment.SIFT.sift import align_images_sift
from alignment.correlation.correlation import align_images_corr



def normalize_images(ref, test):
    """Normalize images using OpenCV functions."""
    # Convert to float32 for processing
    ref_f = ref.astype(np.float32)
    test_f = test.astype(np.float32)

    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    ref_eq = clahe.apply((ref_f * 255).astype(np.uint8)) / 255.0
    test_eq = clahe.apply((test_f * 255).astype(np.uint8)) / 255.0

    # Normalize to zero mean and unit variance
    ref_norm = cv2.normalize(ref_eq, None, 0, 1, cv2.NORM_MINMAX)
    test_norm = cv2.normalize(test_eq, None, 0, 1, cv2.NORM_MINMAX)

    return ref_norm, test_norm

def load_defect_pair(img_path, defect_no):
    """
    This function finds the test and reference image from the dataset directory. (currently from "/mnt/fs0/dataset/Layer_M")
    It handles the suffixes(L, U, Up, Lp ...)
    """
    test = None
    ref_U = None
    ref_L = None
    ref_Up = None
    ref_Lp = None

    images = glob.glob(f'{img_path}/{str(defect_no)}*')
    for image in images:
        if image == f'{img_path}/{str(defect_no)}.png':
            test = image
        elif image == f'{img_path}/{str(defect_no)}U.png':
            ref_U = image
        elif image == f'{img_path}/{str(defect_no)}L.png':
            ref_L = image
        elif image == f'{img_path}/{str(defect_no)}L_p.png':
            ref_Lp = image
        elif image == f'{img_path}/{str(defect_no)}U_p.png':
            ref_Up = image

    if test is None:
        raise ValueError(f'{defect_no}: No Test Image')

    test = cv2.imread(test, cv2.IMREAD_GRAYSCALE)/255

    # Create list to store all available images for median filtering
    all_images = [test]

    ref_u_img = None
    ref_l_img = None
    ref_up_img = None
    ref_lp_img = None
    if ref_U is not None:
        ref_u_img = cv2.imread(ref_U, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_u_img)

    if ref_L is not None:
        ref_l_img = cv2.imread(ref_L, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_l_img)

    if ref_Up is not None:
        ref_up_img = cv2.imread(ref_Up, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_up_img)

    if ref_Lp is not None:
        ref_lp_img = cv2.imread(ref_Lp, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_lp_img)

    # Only use median if we have more than 3 images, otherwise use ref_U as reference
    if len(all_images) > 3:
        reference = np.median(np.stack(all_images), axis=0)
        is_median_ref = True
    else:
        is_median_ref = False
        if ref_u_img is not None:
            reference = ref_u_img
        elif ref_l_img is not None:
            reference = ref_l_img
        elif ref_lp_img is not None:
            reference = ref_lp_img
        elif ref_up_img is not None:
            reference = ref_up_img
        else:
            raise ValueError(f'{defect_no}: No reference image available')

    return reference, test, is_median_ref

def get_diff_map(ref, test, defect_no, lot_id, is_median_ref, max_features=1000, max_shift=10, ransacReprojThreshold=0.0, method="SIFT"):
    """
    input:
    ref : numpy.ndarray, shape(256, 256), grayscale [0,1]
    test : numpy.ndarray, shape(256, 256), grayscale [0,1]

    This function gets you the difference map and its metadata
    You can add your own alignment methods here. As long as you return the following:
    1.aligned_ref : numpy.ndarray, shape(256, 256), grayscale [0,1]
    2.aligned_test : numpy.ndarray, shape(256, 256), grayscale [0,1]
    3.translation : [translation_x, translation_y]  negative means left/up, positive means right/down
    """
    assert(ref.shape == test.shape)

    # logger.debug(f"ref type:{type(ref)}")
    # logger.debug(f"ref shape:{ref.shape}")
    # logger.debug(f"ref:{ref}")
    # logger.debug(f"test type:{type(test)}")
    # logger.debug(f"test shape:{test.shape}")
    # logger.debug(F"test:{test}")
    # Align images using SIFT
    if method == "SIFT":
        if ransacReprojThreshold > 0.0:
            #use RANSAC to filter feature point outliers
            aligned_ref, aligned_test, translation = align_images_sift(ref, test, max_features=max_features, max_shift=max_shift, RANSAC=True, ransacReprojThreshold=ransacReprojThreshold)
        else:
            aligned_ref, aligned_test, translation = align_images_sift(ref, test, max_features=max_features, max_shift=max_shift, RANSAC=False)

        # Convert translation to regular Python float
        translation = [float(translation[0]), float(translation[1])]

    elif method == "Correlate":
        aligned_ref, aligned_test, translation = align_images_corr(ref, test)


    # Normalize images
    processed_ref, processed_test = normalize_images(aligned_ref, aligned_test)
    processed_diff = processed_ref - processed_test
    # Calculate RMSE and max absolute difference
    rmse = float(np.sqrt(np.mean((processed_ref - processed_test) ** 2)))
    max_diff = float(max(abs(np.max(processed_diff)), abs(np.min(processed_diff))))

    def get_ceiling_floor(translation):
        if translation < 0:
            return -max_shift-2+int(math.floor(translation))
        else:
            return max_shift+2+int(math.ceil(translation))
    int_translation = [get_ceiling_floor(t) for t in translation]
    # Define the region of interest excluding the translation area
    roi_start = [max(0, int_translation[0], -int_translation[0]), max(0, int_translation[1], -int_translation[1])]
    roi_end = [min(processed_diff.shape[0], processed_diff.shape[0] + int_translation[0], processed_diff.shape[0] - int_translation[0]),
            min(processed_diff.shape[1], processed_diff.shape[1] + int_translation[1], processed_diff.shape[1] - int_translation[1])]

    # Extract the region of interest

    roi_diff = processed_diff[roi_start[0]:roi_end[0], roi_start[1]:roi_end[1]]

    # Find max/min positions within the region of interest
    max_pos = [int(x) for x in np.unravel_index(roi_diff.argmax(), roi_diff.shape)]
    min_pos = [int(x) for x in np.unravel_index(roi_diff.argmin(), roi_diff.shape)]
    # x,y coord will be swapped because of the way numpy unravels indices
    max_pos = [max_pos[1], max_pos[0]]
    min_pos = [min_pos[1], min_pos[0]]
    # Adjust positions to the original image coordinates
    max_pos = [max_pos[0] + roi_start[0], max_pos[1] + roi_start[1]]
    min_pos = [min_pos[0] + roi_start[0], min_pos[1] + roi_start[1]]

    # Create metadata dictionary with Python native types
    metadata = {
        'lot_id': lot_id,
        'defect_no': int(defect_no),
        'translation': translation,
        'max_difference': float(np.max(processed_diff)),
        'min_difference': float(np.min(processed_diff)),
        'abs_max_difference': max_diff,
        'rmse': rmse,
        'max_pos': max_pos,
        'min_pos': min_pos,
        'is_median_ref': is_median_ref  # Include the median reference flag
    }

    return metadata, processed_ref, processed_test, processed_diff, max_pos, min_pos




#####backend###########################################combine_db.py

import sqlite3
import json

def save_json_to_sqlite(img_path, results_raw, db_path):
    json_string = json.dumps(results_raw)
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT results_json FROM results_table WHERE image_path = ?', (img_path,))
    existing_entry = cursor.fetchone()
    if existing_entry:
        existing_json_string = existing_entry[0]
        if existing_json_string == json_string:
            print("Same img_path and same results_raw. No update needed.")
        else:
            cursor.execute('UPDATE results_table SET results_json = ? WHERE image_path = ?', (json_string, img_path))
            print("Same img_path but different results_raw. Updated the entry.")
    else:
        cursor.execute('INSERT INTO results_table (image_path, results_json) VALUES (?, ?)', (img_path, json_string))
        print("New img_path. Inserted a new entry.")
    conn.commit()
    conn.close()
    print("Connection closed")

def fetch_results(image_path, db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT results_json FROM results_table WHERE image_path = ?', (image_path,))
    result = cursor.fetchone()
    conn.close()
    if result:
        return json.loads(result[0])
    return None

def combine_databases(db1_path, db2_path, combined_db_path):
    conn1 = sqlite3.connect(db1_path)
    conn2 = sqlite3.connect(db2_path)
    cursor1 = conn1.cursor()
    cursor2 = conn2.cursor()
    combined_conn = sqlite3.connect(combined_db_path)
    combined_cursor = combined_conn.cursor()
    combined_cursor.execute('''
        CREATE TABLE IF NOT EXISTS results_table (
            image_path TEXT PRIMARY KEY,
            results_json TEXT
        )
    ''')
    cursor1.execute('SELECT image_path, results_json FROM results_table')
    entries1 = cursor1.fetchall()
    cursor2.execute('SELECT image_path, results_json FROM results_table')
    entries2 = cursor2.fetchall()
    combined_entries = {}
    for img_path, results_json in entries1:
        combined_entries[img_path] = results_json
    for img_path, results_json in entries2:
        if img_path not in combined_entries:
            combined_entries[img_path] = results_json
    for img_path, results_json in combined_entries.items():
        combined_cursor.execute('INSERT INTO results_table (image_path, results_json) VALUES (?, ?)', (img_path, results_json))
    combined_conn.commit()
    conn1.close()
    conn2.close()
    combined_conn.close()

# Example usage
db1_path = '/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/new_m_layer.db'
db2_path = '/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/combined_YOLO.db'
combined_db_path = '/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/megamind.db'

combine_databases(db1_path, db2_path, combined_db_path)

print(f"Combined database created at {combined_db_path}")



########dbview.py
import sqlite3
import pandas as pd

# Connect to the database
conn = sqlite3.connect('backend/db_files/the_regional_dataset.db')

# Read the data into a DataFrame
df = pd.read_sql_query("SELECT image_path, results_json FROM results_table", conn)

# Adjust display settings
pd.set_option('display.max_colwidth', 80)
# Display the DataFrame
print(df)


############sqlite_functions.py
import json
import sqlite3


def save_json_to_sqlite(img_path, results_raw, db_path):
    # Convert results_raw to JSON string
    json_string = json.dumps(results_raw)

    # Connect to SQLite database
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Check if an entry with the same img_path already exists
    cursor.execute('SELECT results_json FROM results_table WHERE image_path = ?', (img_path,))
    existing_entry = cursor.fetchone()

    if existing_entry:
        # Compare existing results_raw with the new one
        existing_json_string = existing_entry[0]
        if existing_json_string == json_string:
            print("Same img_path and same results_raw. No update needed.")
        else:
            # Update the existing entry with the new results_raw
            cursor.execute('UPDATE results_table SET results_json = ? WHERE image_path = ?', (json_string, img_path))
            print("Same img_path but different results_raw. Updated the entry.")
    else:
        # Insert a new entry
        cursor.execute('INSERT INTO results_table (image_path, results_json) VALUES (?, ?)', (img_path, json_string))
        print("New img_path. Inserted a new entry.")

    # Commit changes and close the connection
    conn.commit()
    conn.close()
    print("Connection closed")


def fetch_results(image_path, db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT results_json FROM results_table WHERE image_path = ?', (image_path,))
    result = cursor.fetchone()
    conn.close()
    if result:
        return json.loads(result[0])
    return None


def round_to_closest_integer(db_file):
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Query to retrieve all rows from the results_table
    cursor.execute('SELECT image_path, results_json FROM results_table')
    rows = cursor.fetchall()

    # Iterate over each row
    for row in rows:
        img_path, results_json = row
        results = json.loads(results_json)

        # Check if 'areas' key exists in the JSON
        if 'areas' in results:
            areas = results['areas']
            for area_key, area_value in areas.items():
                if area_value['object'] == 'image3':
                    # Round x, y, width, and height to the closest integer
                    area_value['x'] = round(area_value['x'])
                    area_value['y'] = round(area_value['y'])
                    area_value['width'] = round(area_value['width'])
                    area_value['height'] = round(area_value['height'])

        # Update the row with the modified JSON
        cursor.execute('UPDATE results_table SET results_json = ? WHERE image_path = ?', (json.dumps(results), img_path))

    # Commit the changes and close the database connection
    conn.commit()
    conn.close()

if __name__ == "__main__":
    db_file = "/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/example.db"
    round_to_closest_integer(db_file)




#####frontend####################### image process.py
from PIL import Image
import base64
from io import BytesIO
import numpy as np
import json
import cv2
from alignment.alignment import load_defect_pair, get_diff_map
import matplotlib.pyplot as plt
# Function to convert a NumPy array to a Base64 encoded string

def numpy_to_base64(img_array):
    pil_img = Image.fromarray(img_array)
    buffer = BytesIO()
    pil_img.save(buffer, format="JPEG")
    return base64.b64encode(buffer.getvalue()).decode("utf-8")


# Function to load images from a JSON file line by line and save into a dictionary
def load_images_from_json(json_file):
    images = {}
    with open(json_file, 'r') as f:
        for line in f:
            data = json.loads(line)
            lot_id = data['lot_id']
            images[lot_id] = data
    return images

def apply_colormap(np_array, colormap, vmin=None, vmax=None):
    # norm_array = (np_array - np_array.min()) / (np_array.max() - np_array.min())
    if vmin and vmax:
        norm_array = (np_array - vmin) / (vmax - vmin)
    else:
        norm_array = np_array
    colormap_func = plt.get_cmap(colormap)
    colored_array = colormap_func(norm_array)
    return (colored_array[:, :, :3] * 255).astype(np.uint8)

# Define a 2x2 convolution kernel
def conv_kernel(kernel_size):
    return np.ones((kernel_size, kernel_size), dtype=np.float32) / (kernel_size ** 2)

def get_image_pair_for_studio_input(data_dir, selected_lot_id, selected_image_id, selected_image_type, vmin_level, max_features, max_shift, ransac_reproj_threshold, selected_alignment_method, conv_kernel_size):
    img_dir = f"{data_dir}/{selected_lot_id}/{selected_lot_id}/Images/InstantReview{selected_image_type}"
    fn_id = selected_image_id
    ref, test, is_median_ref = load_defect_pair(img_dir, fn_id)
    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
        get_diff_map(ref, test, fn_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method=selected_alignment_method)
    print(f"max_pos:{metadata['max_pos']}")
    print(f"min_pos:{metadata['min_pos']}")

    ref_colored = apply_colormap(proc_ref, 'gray')
    test_colored = apply_colormap(proc_test, 'gray')
    diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)
    conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))
    img1_base64 = numpy_to_base64(ref_colored)
    img2_base64 = numpy_to_base64(test_colored)
    img3_base64 = numpy_to_base64(diff_colored)
    img4_base64 = numpy_to_base64(conv_diff_image)
    return [img1_base64, img2_base64, img4_base64], metadata




#########json_functions.py
import json
import copy

def save_results_to_json(results_raw):
    with open('results.json', 'w') as f:
        json.dump(results_raw, f, indent=4)

def sync_labels_across_3images(results_raw):
    """
    Syncs labels across three images by replicating 'image3' entries to 'image1' and 'image2'.

    Args:
        results_raw (dict): Dictionary containing areas information.

    Returns:
        dict: Updated dictionary with synchronized labels.
    """

    # Filter image3 entries
    image3_entries = [area for area in results_raw['areas'].values() if area['object'] == 'image3']

    # Create new list of areas with only the replicated image3 entries as image1, image2 and image3
    areas_list = []
    

    for entry in image3_entries:
        # Create image3 copy
        image3_copy = copy.deepcopy(entry)
        # image3_copy['results'][0]['value']['rectangles'][0] = image3_copy['results']['width']*image3_copy['results']['height']
        areas_list.append(image3_copy)

        # Create image2 copy
        image2_copy = copy.deepcopy(entry)
        image2_copy['object'] = 'image2'
        image2_copy['results'][0]['from_name'] = 'label_image2'
        image2_copy['results'][0]['to_name'] = 'image2'
        image2_copy['results'][0]['id'] = f"image2{entry['results'][0]['id']}"
        areas_list.append(image2_copy)

        # Create image1 copy
        image1_copy = copy.deepcopy(entry)
        image1_copy['object'] = 'image1'
        image1_copy['results'][0]['from_name'] = 'label_image1'
        image1_copy['results'][0]['to_name'] = 'image1'
        image1_copy['results'][0]['id'] = f"image1{entry['results'][0]['id']}"
        areas_list.append(image1_copy)


    # Filter image3 entries
    image6_entries = [area for area in results_raw['areas'].values() if area['object'] == 'image6']


    

    for entry in image6_entries:
        # Create image3 copy
        image6_copy = copy.deepcopy(entry)
        # image3_copy['results'][0]['value']['rectangles'][0] = image3_copy['results']['width']*image3_copy['results']['height']
        areas_list.append(image6_copy)

        # Create image5 copy
        image5_copy = copy.deepcopy(entry)
        image5_copy['object'] = 'image5'
        image5_copy['results'][0]['from_name'] = 'label_image5'
        image5_copy['results'][0]['to_name'] = 'image5'
        image5_copy['results'][0]['id'] = f"image5{entry['results'][0]['id']}"
        areas_list.append(image5_copy)

        # Create image4 copy
        image4_copy = copy.deepcopy(entry)
        image4_copy['object'] = 'image4'
        image4_copy['results'][0]['from_name'] = 'label_image4'
        image4_copy['results'][0]['to_name'] = 'image4'
        image4_copy['results'][0]['id'] = f"image4{entry['results'][0]['id']}"
        areas_list.append(image4_copy)

    # Update results_raw dictionary
    results_raw['areas'] = dict(enumerate(areas_list))

    return results_raw

def get_area_size(results_raw):
    image3_entries = [area for area in results_raw['areas'].values() if area['object'] == 'image3']
    size_list = []
    for entry in image3_entries:
        size_list.append(float(entry['width'])*float(entry['height'])*10000/(256**2))
    return size_list



#########label_config.py
# poly_rec_config = """
# <View>
#   <Header value="Select label and click the image to start"/>
#   <View>
#     <View style="display: flex;">
#       <View style="width: 33%; margin-right: 1%;">
#         <Image name="image1" value="$image1"/>
#         <PolygonLabels name="label1_image1" toName="image1">
#           <Label value="Poly_Defect" background="green"/>
#         </PolygonLabels>
#         <RectangleLabels name="label2_image1" toName="image1">
#           <Label value="Rec_Defect" background="blue"/>
#         </RectangleLabels>
#       </View>
#       <View style="width: 33%; margin-right: 1%;">
#         <Image name="image2" value="$image2"/>
#         <PolygonLabels name="label1_image2" toName="image2">
#           <Label value="Poly_Defect" background="green"/>
#         </PolygonLabels>
#         <RectangleLabels name="label2_image2" toName="image2">
#           <Label value="Rec_Defect" background="blue"/>
#         </RectangleLabels>
#       </View>
#       <View style="width: 33%;">
#         <Image name="image3" value="$image3"/>
#         <PolygonLabels name="label1_image3" toName="image3">
#           <Label value="Poly_Defect" background="green"/>
#         </PolygonLabels>
#         <RectangleLabels name="label2_image3" toName="image3">
#           <Label value="Rec_Defect" background="blue"/>
#         </RectangleLabels>
#       </View>
#     </View>
#   </View>

# </View>
# """

config = """
<View>
  <Header value="Select label and click the image to start"/>
  <View>
    <View style="display: flex;">
      <View style="width: 33%; margin-right: 1%;">
        <Image name="image1" value="$image1" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image1" toName="image1">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
      <View style="width: 33%; margin-right: 1%;">
        <Image name="image2" value="$image2" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image2" toName="image2">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
      <View style="width: 33%;">
        <Image name="image3" value="$image3" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image3" toName="image3">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
    </View>
  </View>

</View>
"""


two_row_config="""
<View>
  <Header value="Select label and click the image to start"/>
  <View>
    <Header value="Category: T"/>
    <View style="display: flex;">
      <View style="width: 33%; margin-right: 1%;">
        <Image name="image1" value="$image1" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image1" toName="image1">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
      <View style="width: 33%; margin-right: 1%;">
        <Image name="image2" value="$image2" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image2" toName="image2">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
      <View style="width: 33%;">
        <Image name="image3" value="$image3" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image3" toName="image3">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
    </View>
    <Header value="Category: Rt"/>
    <View style="display: flex; margin-top: 1%;">
      <View style="width: 33%; margin-right: 1%;">
        <Image name="image4" value="$image4" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image4" toName="image4">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
      <View style="width: 33%; margin-right: 1%;">
        <Image name="image5" value="$image5" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image5" toName="image5">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
      <View style="width: 33%;">
        <Image name="image6" value="$image6" brightnessControl="true" contrastControl="true" zoomControl="true"/>
        <RectangleLabels name="label_image6" toName="image6">
          <Label value="Defect_left" background="blue"/>
          <Label value="Defect_right" background="red"/>
          <Label value="4D" background="green"/>
        </RectangleLabels>
      </View>
    </View>
  </View>
</View>
"""


interfaces = [
  "panel",
  "update",
  "controls",
  "side-column",
  "completions:menu",
  "completions:add-new",
  "completions:delete",
  "predictions:menu",
],

user = {
  'pk': 1,
  'firstName': "Labeler",
  'lastName': "",
},

######app.py
import streamlit as st
from streamlit_labelstudio import st_labelstudio
import cv2
from alignment.alignment import load_defect_pair, get_diff_map
from frontend.image_processor import numpy_to_base64, load_images_from_json, get_image_pair_for_studio_input, apply_colormap, conv_kernel
from frontend.label_config import config, interfaces, user, two_row_config
from prelabel.prelabel import task_generator
from frontend.read_lrf import get_defect_list, get_lrf_file
from backend.sqlite_functions import save_json_to_sqlite, fetch_results
from frontend.json_functions import save_results_to_json, sync_labels_across_3images, get_area_size
import sqlite3
import os
import traceback
import argparse
from loguru import logger

# modify the .db filename if you want a fresh start (will create file for you)
db_path = "backend/db_files/the_regional_dataset.db"
# pass in trained model checkpoint (currently only supports YOLO)
model_path = "/home/hubert007/Code/label_tool/labeling/runs/detect/megamind_nojit_vmin_fix/weights/best.pt"

st.set_page_config(layout='wide')

# Initialize session state for results_raw and image index
if 'previous_results_raw' not in st.session_state:
    st.session_state.previous_results_raw = None
if 'image_index' not in st.session_state:
    st.session_state.image_index = 0
if 'lot_index' not in st.session_state:
    st.session_state.lot_index = 0

# Function to check if results_raw has changed
def has_results_raw_changed(current_results_raw):
    previous_results_raw = st.session_state.previous_results_raw
    if previous_results_raw != current_results_raw:
        st.session_state.previous_results_raw = current_results_raw
        return True
    return False


conn = sqlite3.connect(db_path)
cursor = conn.cursor()
cursor.execute('''
CREATE TABLE IF NOT EXISTS results_table (
    id INTEGER PRIMARY KEY,
    image_path TEXT,
    results_json TEXT
)
''')
conn.commit()
conn.close()


# Sidebar for directory-like structure
st.sidebar.title('Image Directory')

data_dir = "/mnt/fs0/dataset/Layer_M"  
items = os.listdir(data_dir)

# Filter out only directories
lot_ids = [item for item in items if os.path.isdir(os.path.join(data_dir, item)) and item != 'log']
lot_ids.sort()
try:
    selected_lot_id = st.sidebar.selectbox('Select Lot ID', lot_ids, index=st.session_state.lot_index)
except:
    selected_lot_id = st.sidebar.selectbox('Select Lot ID', lot_ids, index=0)
# selected_image_type = st.sidebar.selectbox('Select Image Type', ["T", "Rt"], index=0)

lrf_file = get_lrf_file(data_dir, selected_lot_id)


# logger.debug(f"lrf_file: {lrf_file}")
defect_images_id_list, defect_type = get_defect_list(lrf_file)



# Skip to the next image_id when the button is clicked
if st.sidebar.button('Previous Image'):
    st.session_state.image_index -= 1
    if st.session_state.image_index < 0:
        st.session_state.lot_index = (st.session_state.lot_index - 1) % len(lot_ids)
        selected_lot_id = lot_ids[st.session_state.lot_index]
        lrf_file = get_lrf_file(data_dir, selected_lot_id)
        defect_images_id_list, defect_type = get_defect_list(lrf_file)
        st.session_state.image_index = len(defect_images_id_list) - 1

# Go to the previous image_id when the button is clicked
if st.sidebar.button('Next Image'):
    st.session_state.image_index += 1
    if st.session_state.image_index >= len(defect_images_id_list):
        st.session_state.image_index = 0
        st.session_state.lot_index = (st.session_state.lot_index + 1) % len(lot_ids)
        selected_lot_id = lot_ids[st.session_state.lot_index]
        lrf_file = get_lrf_file(data_dir, selected_lot_id)
        defect_images_id_list, defect_type = get_defect_list(lrf_file)

# logger.debug(f"image_id_list:{defect_images_id_list}")
try:
    selected_image_id = st.sidebar.selectbox('Select Image ID', defect_images_id_list, index=st.session_state.image_index)
except:
    selected_image_id = '1'
# try:
st.session_state.image_index = defect_images_id_list.index(selected_image_id)
# except:
#     st.session_state.image_index = 1
st.write(f"defect type : {defect_type[st.session_state.image_index]}")

prelabel_methods = ['minmax', 'YOLO']
selected_prelabel_method = st.sidebar.selectbox('Select prelabel method', prelabel_methods, index=0)

alignment_methods = ['SIFT', 'Correlate']
selected_alignment_method = st.sidebar.selectbox('Select alignment method', alignment_methods, index=0)

# Slider for saturation adjustment
vmin_level = st.sidebar.slider('vmin_level', -2.0, 0.0, -0.5)
max_features = st.sidebar.slider('max_features', 20.0, 1000.0, 1000.0, 20.0)
max_shift = st.sidebar.slider('max_shift', 0.0, 20.0, 10.0)
ransac_reproj_threshold = st.sidebar.slider('ransac_reproj_threshold', 0.0, 0.3, 0.10, 0.01)
crop_size = st.sidebar.slider('crop_size', 1.0, 30.0, 15.0, 1.0)
conv_kernel_size = st.sidebar.slider('conv_kernel_size', 1.0, 10.0, 2.0, 1.0)


T_images, T_metadata = get_image_pair_for_studio_input(data_dir, selected_lot_id, selected_image_id, "T", vmin_level, max_features, max_shift, ransac_reproj_threshold, selected_alignment_method, conv_kernel_size)
Rt_images, Rt_metadata = get_image_pair_for_studio_input(data_dir, selected_lot_id, selected_image_id, "Rt", vmin_level, max_features, max_shift, ransac_reproj_threshold, selected_alignment_method, conv_kernel_size)

images_base64 = [T_images[0], T_images[1], T_images[2], Rt_images[0], Rt_images[1], Rt_images[2]]
metadatas = [T_metadata, Rt_metadata]

config = two_row_config

img_path = f"{data_dir}, {selected_lot_id}, {selected_image_id}"
existing_labels = fetch_results(img_path, db_path)
if not existing_labels:
    results_raw = st_labelstudio(config, interfaces, user, task_generator(images_base64, crop_size, metadatas=metadatas, method=selected_prelabel_method, model_path=model_path))
else:
    print("Labels already exist. Using existing Labels.")
    st.write("Using existing labels.")
    results_raw = st_labelstudio(config, interfaces, user, task_generator(images_base64, crop_size, method=selected_prelabel_method, existing_labels=existing_labels))
if results_raw is not None and has_results_raw_changed(results_raw):
    results_raw = sync_labels_across_3images(results_raw)
    # save_results_to_json(results_raw)
    # st.write(f"label areas:{get_area_size(results_raw)}")
    save_json_to_sqlite(img_path, results_raw, db_path)
    st.session_state.image_index += 1
    if st.session_state.image_index >= len(defect_images_id_list):
        st.session_state.image_index = 0
        st.session_state.lot_index = (st.session_state.lot_index + 1) % len(lot_ids)
        selected_lot_id = lot_ids[st.session_state.lot_index]
        lrf_file = get_lrf_file(data_dir, selected_lot_id)
        defect_images_id_list, defect_type = get_defect_list(lrf_file)    
    st.rerun()






####YOLO prelabeler training

Prepare dataset
Run commands under /toggle_labelstudio
After finishing labeling, you will get .db file under /toggle_labelstudio.
Convert that .db file into YOLO training format by running the following command:

python -m model_based.YOLO.dataset --db_file example.db --json_metadata example.json --dataset_path dataset_path --use_diff_map

Train model

run finetune

python -m model_based.YOLO.finetune --epochs 200 --imgsz 256 --exp_name "experiment name" --dataset_path "dataset_path"
Other arguments will be added later(TODO)




######Toggle Label Studio

Quickstart

install dependencies

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

run the script

streamlit run app.py
This will load the False Negatives images classified by FF (just for demo).

You can toggle around visualization paramters on the left.
Click on Predictions to see the rule-based predictions(using min/max diff on diff map).
Edit these predictions if necessary by clicking again on Predictions.
Only edit on Diff Map (3rd image) for now. Edits on the image1,2 won't be saved.
Click on Submit to save annotations to backend.




# Toggle Label Studio

## Overview
This is a app that allows labeling data on the difference map and exporting them as json.

## Quickstart
### install dependencies
Use **python 3.10.0** to reproduce.
```bash
cd toggle_labelstudio
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### run the script
```bash
streamlit run app.py
```

* Click on top-right settings. Disable completion panel (it has no functionality)
* You can toggle around visualization paramters on the left.
* Click on **Predictions** to see the rule-based predictions(using min/max diff on diff map).
* Edit these predictions if necessary by clicking on the small icon next Predictions.
* Only edit on **Diff Map** for now. Edits on the grayscale images won't be saved.
* Click on **Submit** to save annotations to backend (will create a .db file for you locally under `toggle_labelstudio/`).  


bugs:<br>
* If you edit a label on the diffmap and save it, the changes on test and ref will only be displayed after you leave and return to the page (next page, then prev page).<br>

* The update button is a button to edit an label **right after** submit. The button doesn't work right now. Use the trick in mentioned in 1. to edit existing labels.<br> 

### Export data
in `app.py`, you can see your .db file path.<br>
```
# modify the .db filename if you want a fresh start (will create file for you)
db_path = "backend/db_files/this_bug_weird.db"
# pass in trained model checkpoint (currently only supports YOLO)
model_path = "/home/hubert007/Code/label_tool/labeling/runs/detect/megamind_nojit_vmin_fix/weights/best.pt"
```
Your just labeled data will be saved locally at `backend/db_files`

go to `export/` and run 
```
python export_json_and_mask.py --db_path "your_db_path.db" --output_dir "desired_output_path"
```

### Train YOLO
go read `prelabel/model_based/YOLO/README.md`

The current best model is under `/home/hubert007/Code/label_tool/labeling/runs/detect/megamind_nojit_vmin_fix`.

You can see all the metrics and checkpoints in the above folder.

It's traing config is here : `/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/prelabel/model_based/YOLO/train_settings/megamind_nojit_vmin_fix.yaml` with 200 epochs.

I currently have a more high-qulatiy labeled dataset of 225 images in `/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/backend/db_files/the_regional_dataset.db`

## Develop guide

### `Frontend`

#### `app.py`
The frontend is written with **Streamlit**, and basically all packed in `app.py`. 

Change .db path and YOLO's .ckpt path in `app.py` to point to your own .db file and YOLO's .ckpt file.

##### `Labelstudio Component`
The main labelstudio functionality comes from the repo [deneland/streamlit-labelstudio](https://github.com/deneland/streamlit-labelstudio). It creates a Streamlit component `st_labelstudio` that can be used to interact with Label Studio.

The only usage of this is in `app.py` where we use it to create a label studio instance and pass it the data.

This component takes in the following 4 parameters:
* `config`: The UI configuration of our labeling task. The Rt, T display is defined there. The annotation type is also defined there.
* `interfaces`: Which built-in interfaces to use for the labeling task.
* `user`: Who's labeling? (I think this isn't very important, but it's required by Label Studio.)
* `task`: **Important.** This is where we pass in the data that we want to label. In app.py we pass in a task_generator function. The diff-map logic is defined there. 

The component returns a dictionary after **submit** is pushed. The format of that dictionary is crucial with **extracting labels**. Please print out this dictionary and understand what it contains.

Currently, all code assumes the images are 256x256 pixels, if dynamic image size is needed, search for all 256 in the code base and replace it with variables.

##### `frontend/`<br>
Just contains some helper functions to convert data format.

### `backend/`
The current backend just stores the whole "annotation dictionary" mentioned above into sqlite3.
The schema looks like this:
```
index                       image_path                                              results_json
0    /mnt/fs0/dataset/Layer_M, F12_TMPD98_7M0A-R_20240503_170022, 1    {"id": "k8kst", "pk": null, "selecte...
1    /mnt/fs0/dataset/Layer_M, F12_TMPD98_7M0A-R_20240503_170022, 2    {"id": "xmbtL", "pk": null, "selecte...
2    /mnt/fs0/dataset/Layer_M, F12_TMPD98_7M0A-R_20240503_170022, 3    {"id": "ApHt7", "pk": null, "selecte...
3    /mnt/fs0/dataset/Layer_M, F12_TMPD98_7M0A-R_20240503_170022, 4    {"id": "aI2sc", "pk": null, "selecte...
```
`image_path` isn't the exact absolute path, it is a string composing 3 elements:<br>
1. dataset root directory
2. lot id
3. image id

`results_json` is the dictionary mentioned in `frontend`. I just store the whole dictionary into a json string.

You can run `backend/dbview.py` to print out a certain .db's contents.


### `alignment/`
The main functionalities of alignment is defined in `alignment/alignment.py`. The file itself is quite self-explanatory. <br>
You can add your custom alignment methods as long as you return with the spec described in the comments.

### `prelabel/`
The main functionalities of prelabeling is defined in `prelabel/prelabel.py`, also self-explanatory. <br>
You can easily add in other prelabeling methods by defining and swapping out the api.

### `export/`
Currently, it only contains one python file `mask_image.py`, whose <br>
inputs:<br>
1. .db file path
2. output directory

outputs:
1. metadata.json
2. cropped out mask of each defect

#### Model training pipeline.
go read `toggle_labelstudio/prelabel/model_based/YOLO/README.md`

### TODO bug fix
#### `app.py` 

1. Sometimes when choosing new id, you have to click twice. I think this is related to the session_state : image_index.

2. The **update** button is not working. I think if you go to the [streamlit_labelstudio/frontend/src/index.tsx](https://github.com/deneland/streamlit-labelstudio/blob/master/streamlit_labelstudio/frontend/src/index.tsx) you'll see the following code:
```
function onRender(event: Event): void {
  const data = (event as CustomEvent<RenderData>).detail
  
  var ls = new LabelStudio('label-studio', {
    config: data.args["config"],
    interfaces: data.args["interfaces"][0],
    user: data.args["user"][0],
    task: data.args["task"],
    
    onLabelStudioLoad: function(ls) {
      var c = ls.completionStore.addCompletion({
        userGenerate: true
      });
      ls.completionStore.selectCompletion(c.id);
    },
    
    onSubmitCompletion: function(ls, completion) {
      console.log(ls)
      completion = JSON.parse(JSON.stringify(completion));
      Streamlit.setComponentValue(completion)
    },
    
    
  });
  
  // We tell Streamlit to update our frameHeight after each render event, in
  // case it has changed. (This isn't strictly necessary for the example
  // because our height stays fixed, but this is a low-cost function, so
  // there's no harm in doing it redundantly.)
  Streamlit.setFrameHeight()
}
```
Maybe if we add in something like `onUpdateCompletion` to handle when a user updates their completion, we can activate the update button.

3. The app currently cannot handle cases where both test and ref contain the same defect. That defect won't be shown on the diff map. 
Here are some cases that cannot be handled.
```
            lot_id                           image_id 
F12_TMPD98_7M0A-R_20240503_170022               75
F12_TMPD98_7M0A-R_20240503_170022               76  
F12_TMPD98_7M0A-R_20240503_170022               78  
```

4. Here are just some cases where I cannot find the defect.
```
F12_TMPD98_7M0A-R_20240503_170022               91
F12_TMPD98_7M0A-R_20240503_170022               166
F12_TMPD98_7M0A-R_20240503_170022               186
F12_TMPD98_7M0A-R_20240503_170022               197
F12_TMPD98_7M0A-R_20240503_170022               198
F12_TMPD98_7M0A-R_20240503_170022               199

Currently "the_regional_dataset.db" only labeled to id 224.

```

5. Some cases can't be aligned with either SIFT or Correlation.
```
F12_TMPD98_7M0A-R_20240503_170022               207
F12_TMPD98_7M0A-R_20240503_170022               209
F12_TMPD98_7M0A-R_20240503_170022               211
F12_TMPD98_7M0A-R_20240503_170022               212
F12_TMPD98_7M0A-R_20240503_170022               222
F12_TMPD98_7M0A-R_20240503_170022               223
```

6. The current metadata only saves the image_path, bounding_boxes. However, we didn't save the toggle settings that were used during labeling. We need to add this information to the metadata so that we can reproduce the labeling states exactly.
<br>
<br>


#### `toggle_labelstudio/prelabel/model_based/YOLO/dataset.py`
1. The function `process_and_augment_images` seems to turn my red diffmaps into blue. It still trains well, but I think it's a bug in these lines:
```
    # Ensure the image is in RGB format
    if image_array.shape[2] == 4:  # If the image has an alpha channel
        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGRA2RGB)
    else:
        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
```
I think the R and B channel are swapped. I'm not sure if this is a bug or just a misunderstanding of how OpenCV handles color channels.











1. 整体概览
这份代码主要用于缺陷检测与标注，包含以下几个大模块：

缺陷文件读取与解析
读取存储缺陷信息的文件（.lrf 格式），提取出每个缺陷的编号与分类信息。

图像加载、预处理与差分计算
读取图像（测试图与参考图），对图像进行配准、对齐以及差分（diff map）计算，并将处理结果转换为便于前端展示的格式（例如 Base64 编码）。

缺陷配准与对齐（Alignment 模块）
利用 SIFT 或 Correlation 方法对图像进行配准，计算出参考图与测试图之间的平移、差异信息，以及最大、最小差分点（用来辅助预标注）。

预标注（Prelabeling）与推断
通过已训练好的 YOLO 模型对图像进行目标检测，得到缺陷候选框；或使用基于差分图的最小/最大值位置生成预设边界框（minmax 方法）。

数据集构建与图像增强
将标注数据从数据库或 JSON 中提取出来，按照比例划分成训练、验证、测试集，同时利用 albumentations 进行图像增强（旋转、翻转、缩放等）。

前端界面与标注交互
利用 Streamlit 结合 Label Studio 的组件构建前端标注界面，加载图像、预标注信息，并将用户的修改结果保存到数据库中。

数据库操作与结果同步
对标注结果进行存储、更新以及合并操作，保证数据的一致性，同时将预标注结果同步到各个图像视图中（例如 image1、image2、image3）。




2. 各模块详细解析
2.1 缺陷文件读取与解析
主要函数：

get_lrf_file(data_dir, selected_lot_id)
根据数据目录与选择的批次（lot）ID，查找对应的 .lrf 文件。由于文件命名可能有多种格式（如 _ADD.lrf、_Classified.lrf、或直接 .lrf），函数依次判断各个可能的文件路径，返回存在的那个路径。

read_lrf_file(file_path)
简单地以文本模式读取指定文件的内容。

detect_defect_list(content)
使用正则表达式查找文本中 “[DefectList]” 标签开始的部分，并提取出缺陷数据列表所在的段落。

extract_no_and_classtype(defect_data)
将提取出的缺陷数据按行拆分，每一行根据空格分隔后，提取第一列（缺陷编号）和第九列（缺陷分类类型）。返回一个编号与分类类型的元组列表。

get_defect_list(lrf_path)
综合调用上面几个函数：读取文件内容 → 检测缺陷数据段 → 提取编号和类型，并最终返回一个包含图像编号列表和对应缺陷类型的元组。

【总结】
这部分代码主要负责解析存放缺陷信息的文件，提取出每个缺陷在图像中的标识（例如缺陷编号）以及它的分类，用于后续图像加载和标注展示。




2.2 图像处理与预处理
关键功能与函数：

图像转换与编码

numpy_to_base64(img_array)：将 NumPy 数组形式的图像转换为 PIL 图片，然后保存为 JPEG 格式，通过 BytesIO 转换为 Base64 编码字符串，方便在前端（如 HTML 中）展示图像。
加载与颜色映射

load_images_from_json(json_file)：按行读取 JSON 文件，将图像信息存储在字典中（一般用于加载预处理好的图像元数据）。
apply_colormap(np_array, colormap, vmin, vmax)：对传入的灰度或差分图像应用颜色映射。例如，将灰度图用“gray”映射，将差分图用“seismic”映射，并根据 vmin 与 vmax 参数对数值范围做归一化，返回 RGB 格式的图像数组。
卷积核定义

conv_kernel(kernel_size)：生成一个大小为 kernel_size 的均值卷积核，用于图像平滑操作，常见于对差分图进行卷积处理以增强局部特征。
生成前端输入图像对

get_image_pair_for_studio_input(...)：
根据传入的数据目录、选中的 lot ID 和图像编号，定位存放测试图和参考图的文件夹。
调用 load_defect_pair（在 alignment 模块中）获得参考图和测试图，并确定是否使用中位图作为参考。
使用 get_diff_map 计算出两幅图像之间的差分图，同时得到一些元数据（例如最大差异点和最小差异点的位置）。
对配准后的参考图、测试图和差分图应用颜色映射，再对差分图做卷积滤波处理。
将这些图像转换成 Base64 字符串，返回给前端显示，同时返回包含对齐与差分信息的元数据。
【总结】
这一模块主要实现图像的读取、预处理和转换，以便后续缺陷检测和前端标注界面的展示。关键步骤在于计算“差分图”，帮助突出显示缺陷区域。



2.3 缺陷配准与差分计算（Alignment 模块）
核心函数：

load_defect_pair(img_path, defect_no)
根据给定的图像目录与缺陷编号，从多个可能的后缀（例如 “L”, “U”, “L_p”, “U_p”）中寻找对应的参考图和测试图。

如果测试图只有一张，则将其作为主要图像；如果有多张参考图（例如左右两个方向），则会根据图像数量决定是否使用中位图作为参考图。
get_diff_map(ref, test, defect_no, lot_id, is_median_ref, max_features, max_shift, ransacReprojThreshold, method)

根据选择的方法（SIFT 或 Correlate）对参考图和测试图进行对齐（图像配准）。
对对齐后的图像进行归一化处理，计算出两幅图像的差分图。
在图像的 ROI 区域内找到差分最大的点与最小的点，这通常对应缺陷区域。
同时计算 RMSE（均方根误差）、最大绝对差分等指标，生成一个包含所有这些信息的元数据字典。
【总结】
这一部分是整个缺陷分析的核心，通过图像对齐与差分计算，可以自动定位出图像中的异常区域，从而辅助后续的标注与检测工作。



2.4 推断与预标注
主要流程：

YOLO 模型推断

inference(img_np, ckpt)：
加载预训练或 fine-tune 后的 YOLO 模型。
对输入图像（可能经过预处理转换为 RGB 格式）进行目标检测。
遍历检测到的边界框，提取出边界框的坐标（x1, y1, x2, y2）、宽度、高度以及类别标签，构成预标注数据。
预标注逻辑

在 YOLO_prelabel 中，将通过 YOLO 推断得到的边界框转换成符合 Label Studio 标注格式的数据（以百分比表示位置与尺寸），并为不同视图（例如 image1~image6）生成相应的预标注信息。
同时，代码中还支持另一种预标注方法（minmax），即利用差分图中的最大/最小点位置生成固定大小的缺陷区域。
【总结】
这一模块利用深度学习模型和规则方法为图像自动生成缺陷候选框，减少人工标注的工作量，也为后续模型的训练提供初始数据。



2.5 数据集构建与图像增强
主要功能：

数据库与元数据转换

db_to_metadata：从 SQLite 数据库中提取原始标注信息，转换成 JSON 格式的元数据文件。
数据集创建

create_dataset：
根据提供的数据库与 JSON 文件，将数据划分为训练集、验证集和测试集（例如 70%/20%/10%）。
针对每个图像，调用 get_diff_map 及图像增强函数对图像进行预处理，并保存为 PNG 文件；同时生成对应的标注文本文件。
图像增强部分采用 albumentations 库进行旋转、翻转、仿射变换等操作，以扩充数据集，增加模型的鲁棒性。
数据集检查

dataset_checker：扫描数据集目录，检查是否有图像对应的标注文件缺失或为空，帮助排查数据准备过程中的问题。
【总结】
这一模块主要负责将标注数据转换为 YOLO 模型训练所需要的格式，并通过数据增强来丰富训练样本，确保模型训练时具有足够的多样性。




2.6 前端标注与交互（使用 Label Studio 与 Streamlit）
主要部分：

前端展示

利用 Streamlit 构建 Web 应用（app.py），在侧边栏中提供目录选择（例如 lot ID、图像编号），动态加载对应的图像及预标注信息。
图像以 Base64 编码的形式传给前端组件，方便在网页中直接显示。
Label Studio 组件集成

通过 st_labelstudio 组件（基于 deneland/streamlit-labelstudio），构建交互式标注界面。
标注界面配置（如 config、interfaces、user 等）定义了如何展示图像（如分为左右或两行显示）、标注类型（矩形标注、标签颜色等）。
用户在界面上可以修改预标注信息，最终点击提交后，将标注数据以 JSON 格式返回并保存到 SQLite 数据库中。
预标注结果同步

sync_labels_across_3images：将一个视图（例如 image3）的标注信息复制同步到其他视图（例如 image1、image2），确保前端展示时各个图像的标注保持一致。
【总结】
前端部分主要是与用户交互，加载图像和预标注、显示标注界面、捕获用户修改后返回的数据，并将这些数据保存到后端数据库中。



2.7 数据库操作与其他辅助模块
主要功能：

数据库读写与合并

提供了 save_json_to_sqlite、fetch_results、combine_databases 等函数，用于存储、查询和合并标注结果。
这些函数保证同一图像多次标注时不会重复写入，同时支持更新已有数据。
导出与后处理

export_json_and_mask.py：从数据库中提取数据，生成最终的 JSON 元数据文件，并对图像进行遮罩处理（将缺陷区域透明化或突出显示），方便后续使用。
其他辅助函数

如 check_image_variants 用于处理不同后缀图像（例如左、右、上、下）的查找，确保在存在多种版本的图像时能正确选取。
【总结】
这一部分确保整个系统的数据能够稳定存储、更新和导出，同时为后续模型训练和结果展示提供必要的支撑。




3. 流程图示与工作原理
数据读取

系统从数据目录中读取 .lrf 文件，通过正则解析提取缺陷编号和类型。
图像配准与差分计算

根据缺陷编号加载测试图和参考图，利用 SIFT 或 Correlation 对图像进行配准，对齐后计算差分图，并得到最大/最小差分点位置等元数据。
预标注生成

利用预训练 YOLO 模型（或 minmax 规则）对图像进行缺陷检测，生成初步的边界框预标注，转换成 Label Studio 所需的格式。
前端展示与人工标注

前端使用 Streamlit 与 Label Studio 组件展示图像和预标注，用户可以进行修改或确认，然后提交标注结果。
数据存储与数据集构建

提交后的标注结果存入 SQLite 数据库，同时可以利用这些数据构建用于训练 YOLO 模型的数据集（包括图像增强和格式转换）。
模型训练与迭代改进

利用构建好的数据集进行 YOLO 模型的 fine-tuning，训练后得到的模型可用于下一轮的预标注，从而不断迭代改进缺陷检测效果。


4. 总结
总体来说，这套系统采用了模块化设计，将缺陷文件解析、图像处理、配准差分计算、预标注生成、前端交互标注以及数据存储和模型训练等功能分别实现。对初学者来说，可以按以下思路来理解和逐步学习：

先理解数据输入部分：如何从 .lrf 文件中提取缺陷信息，并根据编号加载对应图像。
再了解图像对齐与差分计算：重点理解 load_defect_pair 与 get_diff_map 的处理流程，这部分直接影响后续缺陷定位。
掌握预标注生成逻辑：学习如何利用深度学习模型（YOLO）和规则方法生成初始的标注结果。
最后关注前端交互与数据存储：理解如何通过 Streamlit 与 Label Studio 组件构建用户标注界面，以及如何将标注结果存储到数据库中以便后续使用。





1. 前端展示与人工标注
主要文件与组件
app.py
这是整个前端交互的入口，利用 Streamlit 构建了一个 Web 应用，同时集成了 Label Studio 的标注组件（通过 st_labelstudio）来展示图像和预标注信息。

Label Studio 组件
前端通过调用 st_labelstudio(config, interfaces, user, task) 来启动标注界面。这里的各个参数说明如下：

config：定义了标注界面的布局和功能，比如显示哪些图像（如 image1、image2、image3 或更多），以及允许用户使用哪种标注方式（如矩形标注）。
interfaces：配置了侧边栏、更新、添加、删除等内置功能。
user：标明当前标注用户的信息。
task：由 task_generator 函数生成的任务数据，包含了前端需要展示的图像数据（通常是 Base64 编码的字符串）以及预标注的边界框信息。
在 task_generator 中，可以看到它会判断是否存在“已有标注”（existing_labels），如果没有，则调用预标注逻辑（例如 YOLO 或 minmax 方法）来生成初步的缺陷候选框，然后将结果格式化为符合 Label Studio 要求的数据结构。
当用户在界面上修改或确认后，提交的数据将以 JSON 格式返回给 Streamlit。


前端工作流程
图像加载与预处理

在 app.py 中，首先通过读取存储图像信息的文件夹（例如数据目录下的 lot 文件夹），根据当前选中的 lot ID 和图像编号调用函数 get_image_pair_for_studio_input 来获取处理好的图像（参考图、测试图、差分图）。
这些图像经过颜色映射、卷积滤波等处理后会被转换为 Base64 字符串，方便直接在 HTML 中嵌入显示。
任务构建

调用 task_generator 函数，构造一个任务（task）。任务中包含了六个图像视图（如 image1 ~ image6），每个图像对应的内容（例如灰度图、差分图等）会以 data:image/jpeg;base64,…… 的格式传递到前端。
同时，根据预设的预标注方法（YOLO 或 minmax），生成初步的标注信息（例如缺陷候选框），这些预标注结果也会传入 task 中。
用户交互

用户在 Label Studio 的界面上可以查看预标注结果，并根据需要进行修改（例如调整边界框的位置或尺寸）。
提交标注后，st_labelstudio 组件会返回更新后的标注结果（JSON 格式），代码中通过 has_results_raw_changed 判断是否有更新，并最终将结果进行保存。



2. 数据存储与数据集构建
数据存储部分
数据库操作
在代码中，SQLite 被用来存储每次标注的结果。主要使用了如下函数：

save_json_to_sqlite
该函数负责将标注结果（一个 JSON 字符串）与对应的 image_path 存入数据库。逻辑包括：
首先检查当前图片是否已经存在标注记录。
如果存在且内容发生变化，则更新记录；否则直接插入新的记录。
fetch_results
根据给定的 image_path 从数据库中获取之前保存的标注结果，用于在重新加载任务时（例如用户重新进入页面）显示已有标注。
前端与数据库的衔接
在 app.py 中，当用户提交标注结果后，程序会调用 save_json_to_sqlite 将新的标注数据保存到 SQLite 数据库中，然后通过更新 session 状态（例如 image_index、lot_index）实现切换到下一张图像，并调用 st.rerun() 刷新页面。

数据集构建部分
创建数据集的主要函数：create_dataset
这一部分的代码通常位于用于模型训练的模块中（例如 prelabel/model_based/YOLO/dataset.py），主要流程如下：

提取标注信息
通过调用 db_to_metadata 函数，从 SQLite 数据库中提取所有标注记录，将它们转换成一个 JSON 格式的元数据文件。每条记录包含了图像路径和对应的缺陷边界框信息。
数据集划分
利用 train_test_split 将数据分为训练集、验证集和测试集。划分比例通常是 70% 训练、20% 验证、10% 测试。
图像增强与预处理
对于每个数据项，读取图像文件，调用 get_diff_map（如果使用差分图作为输入）和图像增强函数（如 process_and_augment_images）。
图像增强主要使用了 albumentations 库，实现随机旋转、翻转、仿射变换等，从而增加数据多样性，扩充训练样本数量。
增强后的图像会保存为 PNG 文件，同时生成对应的标注文本文件，文本中记录了每个标注框的坐标（转换为相对于图像尺寸的百分比）。
数据集检查
最后，调用 dataset_checker 对数据集目录进行扫描，检查是否有图像文件缺失或标注文件为空，确保数据集完整无误。
数据集构建目的
整个数据集构建流程的目标是将从前端获得的标注数据（保存在 SQLite 中）转换成符合 YOLO 模型训练要求的格式，包括图像文件和对应的标注（通常为文本格式），同时通过图像增强来提高模型的泛化能力。



总结
前端展示与人工标注
前端部分主要通过 Streamlit 和 Label Studio 组件实现。代码先加载图像（经过预处理与差分计算），构造任务并展示给用户，用户可以修改预标注结果，最终将标注结果以 JSON 格式返回。

数据存储与数据集构建
用户提交的标注数据被保存到 SQLite 数据库中，通过函数 save_json_to_sqlite 实现；同时，通过从数据库中提取标注信息构建元数据，利用数据集构建代码对图像进行划分和增强，生成用于 YOLO 模型训练的完整数据集。



在 save_json_to_sqlite 中，当你传入一个 img_path（实际上就是 image_path）时，函数会先检查数据库中是否已有该 image_path 对应的记录：

如果存在且内容一致，则不做更新；
如果存在但内容不同，则更新这条记录；
如果不存在，则插入新的记录。
在 fetch_results 中，同样使用传入的 image_path 去数据库中查找对应的标注结果，并返回它。

因此，为了能够正确查找和更新相应的记录，这两个函数中使用的 image_path 应该是同样的——也就是代表同一张图片的唯一标识符。简单来说，是的，它们指代的应该是同样的东西。


1. 页面初始化与状态管理
页面设置与 Session State
在 app.py 开头，会调用

python
複製
st.set_page_config(layout='wide')
设置页面布局为宽屏模式。接着，通过检查 st.session_state 中是否存在 'previous_results_raw'、'image_index' 和 'lot_index'，来初始化这些状态变量。这些变量用于记录当前展示的图像索引、批次（lot）索引以及上一次提交的标注结果，确保页面刷新时状态能够保持同步。

数据库初始化
代码在启动时会检查并创建 SQLite 数据库和对应的表：

python
複製
conn = sqlite3.connect(db_path)
cursor = conn.cursor()
cursor.execute('''
CREATE TABLE IF NOT EXISTS results_table (
    id INTEGER PRIMARY KEY,
    image_path TEXT,
    results_json TEXT
)
''')
conn.commit()
conn.close()
这里 results_table 表中使用 image_path 作为唯一标识，用来保存每张图像对应的标注数据（JSON 格式）。



2. 侧边栏选择与图像索引更新
数据目录与批次（lot）选择
代码通过 os.listdir(data_dir) 获取数据目录下的所有文件夹，并过滤出批次文件夹（排除如 log 目录）。接着使用

python
複製
st.sidebar.selectbox('Select Lot ID', lot_ids, index=st.session_state.lot_index)
让用户选择一个批次。此时根据选中的批次 ID，利用 get_lrf_file 函数获取对应的缺陷文件（.lrf），再调用 get_defect_list 得到当前批次中所有缺陷图像的 ID 列表和缺陷类型信息。

图像索引的前进与后退
在侧边栏中还设置了“Previous Image”和“Next Image”按钮，点击后会更新 st.session_state.image_index（以及在到达边界时更新 lot_index）。这样可以方便用户浏览当前批次中的所有图像，系统会根据更新后的索引重新加载对应图像。

图像编号选择
根据更新后的索引，通过 st.sidebar.selectbox('Select Image ID', defect_images_id_list, index=st.session_state.image_index) 来确定当前要展示的图像编号。


3. 图像预处理与任务构建
参数设置
用户可以在侧边栏通过滑块调整各项参数，如 vmin_level（用于颜色映射）、max_features、max_shift、ransac_reproj_threshold、crop_size（用于后续生成预标注的框大小）以及 conv_kernel_size（用于卷积滤波）。这些参数将影响后续图像配准与差分图生成的效果。

获取处理后的图像
调用函数 get_image_pair_for_studio_input 分别为两类图像（标记为 "T" 和 "Rt"）进行处理：

内部流程：

根据选中的数据目录、批次 ID 与图像编号，定位存放图像的目录。
调用 load_defect_pair 获取参考图和测试图（同时判断是否需要使用中位图作为参考）。
通过 get_diff_map 对两幅图像进行配准和差分计算，获得对齐后的图像、差分图以及相关元数据（例如最大、最小差分点的位置）。
使用 apply_colormap 对图像进行颜色映射，差分图经过卷积滤波处理后转换成 Base64 字符串。
最终返回一个包含 Base64 编码图像（通常为参考图、测试图和处理后的差分图）的列表，以及差分图的元数据。

任务数据构建
将 “T” 类图像和 “Rt” 类图像的 Base64 字符串分别组合成一个 images_base64 列表（总共6个图像视图），同时构造一个 metadatas 列表保存相应的元数据。接着，调用 task_generator 函数生成任务：

如果在数据库中没有该图像的已有标注（通过调用 fetch_results 检查），则调用 generate_prelabels 自动生成预标注（例如基于 YOLO 或 minmax 方法）。
如果已有标注，则将其直接作为任务的一部分展示，方便用户修改。
最终，任务数据格式为一个字典，内容大致如下：


task = {
    'completions': [],
    'predictions': [{
        'model_version': 'prelabeling' 或 'existing_labels',
        'result': annotations  # 标注信息，包含每个图像的矩形框及标签
    }],
    'id': 1,
    'data': {
        'image1': "data:image/jpeg;base64,...",
        'image2': "data:image/jpeg;base64,...",
        ...  # 其他图像视图
    }
}


4. 前端标注与提交
Label Studio 组件调用
调用

python
複製
results_raw = st_labelstudio(config, interfaces, user, task_generator(...))
将任务传给 Label Studio 组件。在这个标注界面中，用户可以查看所有预处理好的图像和预标注结果，并对预标注的缺陷边界框进行微调或修改。

标注提交与结果更新
当用户点击“Submit”后，组件会返回标注结果（JSON 格式），存放在 results_raw 中。接下来：

通过函数 has_results_raw_changed 检查此次提交的标注数据与之前是否发生了变化。
如果变化存在，则调用 sync_labels_across_3images 将部分标注（例如 image3 的标注）同步到其它视图，确保前端显示的一致性。
最后，调用 save_json_to_sqlite 将最终的标注结果存入数据库中，并更新 image_index（自动切换到下一张图像），最后通过 st.rerun() 刷新页面。


5. 整个前端工作流程总结
初始化与状态管理：页面加载时设置好显示格式和初始状态（当前图像、批次以及数据库初始化）。
侧边栏选择：用户在侧边栏选择批次和图像编号，并通过按钮控制前进/后退，更新 Session 状态。
图像加载与预处理：根据选定的图像编号，调用图像预处理函数获取参考图、测试图及差分图，处理后的图像以 Base64 形式保存。
任务构建与预标注生成：利用 task_generator 构建任务数据，任务中包含图像和预标注信息，若已有标注则直接加载，否则生成预标注。
交互标注与结果提交：用户在 Label Studio 组件中修改标注后，提交结果；系统检查数据变化后同步、保存，并自动切换到下一图像，整个页面刷新更新显示。
通过这一系列步骤，前端实现了从图像加载、预标注生成，到用户交互和标注结果存储的完整工作流程，使得缺陷标注过程既自动化又便于人工校正。


Lot ID
表示一个批次或一组图像的标识符。它通常对应数据目录中的一个文件夹，里面包含了同一批次的所有图像和相关缺陷信息。通过选择不同的 Lot ID，你就可以切换到不同的数据批次。

Image ID
表示在某个特定批次（Lot）中的单张图像的标识符。它用来确定具体哪张图像需要处理、展示或者标注。比如在一个批次内，可能有数百张图像，每张图像都有唯一的 Image ID，用来区分这些图像。


下面详细说明“图像预处理与任务构建”这部分的代码逻辑和流程，帮助你理解系统如何从原始图像生成适合前端标注任务的数据。

1. 图像预处理
a. 参数设置
在侧边栏中，用户可以通过滑块设置多个关键参数，例如：

vmin_level：用于设置差分图的最小阈值，这会影响颜色映射的对比度。
max_features、max_shift、ransac_reproj_threshold：这些参数控制图像配准时（比如使用 SIFT 或 Correlation 方法）的特征提取和对齐过程。
conv_kernel_size：设置卷积核的大小，用于对差分图进行平滑处理。
b. 获取图像对（参考图、测试图和差分图）
调用函数 get_image_pair_for_studio_input 来实现这一过程。其主要逻辑如下：

确定图像目录
根据传入的参数（如数据目录、Lot ID、Image ID 和图像类型“T”或“Rt”），构建图像所在的目录路径。

加载图像

调用 load_defect_pair：在指定目录下，根据缺陷编号加载相应的测试图和参考图。该函数会根据文件名后缀（例如直接编号、带 U、L 等后缀）判断哪张图片为测试图，哪张为参考图。
判断是否存在多个参考图：如果有多个，则通常采用中位数图像作为参考，以减少噪声的影响。
图像对齐与差分计算

调用 get_diff_map：对齐参考图和测试图，计算出配准后的差分图。此函数会返回对齐后的参考图、测试图、差分图及一些元数据（例如最大、最小差分点的位置、平移量、RMSE 等）。
图像后处理

分别调用 apply_colormap 将参考图和测试图以灰度（gray）方式映射，同时对差分图使用“seismic”颜色映射（并根据 vmin_level 调整对比度）。
对差分图再进行卷积滤波（调用 conv_kernel 生成卷积核，利用 OpenCV 的 cv2.filter2D 进行平滑），增强局部特征或消除噪声。
转换为 Base64 编码

最后，调用 numpy_to_base64 将处理后的图像（参考图、测试图以及经过卷积处理的差分图）转换为 Base64 字符串，方便在前端直接以 HTML 的 <img> 标签嵌入显示。
函数返回的结果通常为一个列表（包含经过处理的图像的 Base64 字符串）和一个元数据字典（包含配准和平移等信息）。

2. 任务构建
任务构建的主要目标是将预处理好的图像数据以及自动生成的预标注信息整合到一个任务（task）数据结构中，供 Label Studio 组件调用展示和修改。

a. 构造图像数据
在前端中，会分别调用 get_image_pair_for_studio_input 处理两类图像（例如“T”和“Rt”），并将两部分的 Base64 字符串组合成一个列表。
如：
python
複製
images_base64 = [T_images[0], T_images[1], T_images[2],
                 Rt_images[0], Rt_images[1], Rt_images[2]]
这6个图像分别对应不同视图（例如：image1～image6），用于前端展示参考图、测试图和差分图。
b. 生成任务字典（Task）
调用 task_generator 函数，该函数负责构造传递给 Label Studio 的任务数据。其主要逻辑包括：

定义数据结构
构造一个字典 task，包含：

data 字段：键为 image1 至 image6，对应的值为 Base64 编码的图像字符串，形如 "data:image/jpeg;base64,..."。
completions 和 predictions：这些字段用来存储用户的标注结果和预标注信息。
判断已有标注与生成预标注

先通过 fetch_results 函数检查数据库中是否已有该图像的标注结果。如果存在，则将这些“existing_labels”传递给任务。
如果没有，则调用 generate_prelabels 生成预标注：
预标注方法可以是基于 YOLO 模型的目标检测（调用 YOLO_prelabel）或基于差分图的 minmax 策略（调用 minmax_prelabel）。
预标注生成的结果（bounding boxes 及标签）将以 Label Studio 规定的格式保存，并放入 predictions 字段中。
返回任务数据
最终，task_generator 返回一个完整的任务字典，该字典既包含图像数据，也包含预标注或已有标注信息。

c. 前端交互
在 app.py 中，最终调用：
python
複製
results_raw = st_labelstudio(config, interfaces, user, task_generator(...))
将构造好的任务传递给 Label Studio 组件。用户在该界面上可以查看所有图像及预标注，并对标注结果进行修改，提交后会得到更新后的标注数据。
总结
图像预处理：

根据选定的批次（Lot ID）和图像编号（Image ID），系统通过一系列函数完成图像加载、对齐、差分计算、颜色映射、卷积滤波等处理，并将结果转换成 Base64 格式，便于在网页上显示。
任务构建：

将预处理后的图像和自动生成的预标注（或已有标注）整合到一个任务字典中，格式符合 Label Studio 的要求。该任务数据包括6个视图（例如 image1 至 image6），使得用户能够在前端交互式地查看、修改和提交标注结果。
整个流程使得原始图像经过预处理后自动生成初步缺陷标注，并以易于交互的形式呈现在前端，为后续人工确认和修改提供了便利。希望这个详细的解释能帮助你更好地理解这部分代码的逻辑和实现细节！



图片在标注前需要经过预处理，主要原因有以下几点：

突出缺陷特征
预处理（如图像对齐、差分计算和颜色映射）可以将图像中的缺陷区域更加明显地突出出来。这样标注人员在查看时更容易发现异常区域，从而提高标注的准确性。

降低噪音与干扰
原始图像可能存在噪音、亮度不均或颜色偏差，经过预处理后可以统一图像的对比度、亮度等属性，减少干扰因素，使缺陷信息更清晰。

统一图像格式
通过预处理，所有图像都会按照相同的标准（如尺寸、色彩空间）处理后再进行标注，确保不同来源的图像在标注平台上显示一致，便于比较和分析。

辅助自动预标注
预处理后的图像更适合用自动检测算法（例如 YOLO）进行初步缺陷检测，从而生成预标注结果。标注人员只需要在此基础上进行微调，而无需从零开始标注，大大提高了工作效率。

综上所述，图片预处理在标注过程中扮演了提升图像质量、突出关键信息和辅助自动预标注的重要角色，从而帮助标注人员更高效、准确地完成缺陷标注任务。



下面逐行解释 task_generator 函数的代码逻辑，帮助你理解每一部分的作用：

python
複製
def task_generator(images, crop_size, metadatas=None, method="minmax", model_path=None, label_type="rectangle", existing_labels=None):
函数声明与参数：
images：一个包含预处理后图像的列表，这里要求至少包含 6 张图像（通常是 image1~image6，分别对应参考图、测试图、差分图等）。
crop_size：用于生成预标注时边界框的尺寸参数（例如，基于 min/max 位置生成的候选框大小）。
metadatas：包含图像配准及差分图计算后生成的元数据信息（如平移、最大/最小差分点位置等），可能对预标注有辅助作用。
method：预标注的方法选择，例如 "minmax"（基于最小最大差分点生成）或者 "YOLO"（基于深度学习检测）。
model_path：当使用 YOLO 方法时，模型权重文件的路径。
label_type：标注类型，默认是 "rectangle"（矩形标注）。
existing_labels：若已存在标注数据，则传入已有标注信息，避免重新生成预标注。
python
複製
    task = {
        'completions': [],
        'predictions': [],
        'id': 1,
        'data': {
            'image1': f"data:image/jpeg;base64,{images[0]}",
            'image2': f"data:image/jpeg;base64,{images[1]}",
            'image3': f"data:image/jpeg;base64,{images[2]}",
            'image4': f"data:image/jpeg;base64,{images[3]}",
            'image5': f"data:image/jpeg;base64,{images[4]}",
            'image6': f"data:image/jpeg;base64,{images[5]}"
        }
    }
构造任务字典 task：
'completions'：为空列表，通常用于存储用户最终的标注完成记录（这里暂未使用）。
'predictions'：用于保存预标注信息（或已有标注），稍后会填入生成的标注结果。
'id'：任务的标识符，设置为 1（在简单示例中可以固定）。
'data'：关键部分，将图像数据组织成一个字典，键名为 image1 至 image6，每个值为经过 Base64 编码的 JPEG 图片字符串，并带上数据头（"data:image/jpeg;base64,"），方便前端直接显示。
python
複製
    if existing_labels:
        annotations = extract_annotations(existing_labels)
        task['predictions'].append({
            'model_version': 'existing_labels',
            'result': annotations
        })
已有标注处理：
如果函数参数 existing_labels 不为空，则说明数据库中已经有该图像的标注数据。
调用 extract_annotations(existing_labels) 从已有数据中提取标注（例如每个区域的边界框、标签等）。
将提取出的标注结果包装成一个字典，标识模型版本为 "existing_labels"，并追加到 task['predictions'] 中。
python
複製
    else:
        print("no labels found, generating prelabels...")
        annotations = generate_prelabels(metadatas, crop_size, method=method, model_path=model_path, t_image=images[2], rt_image=images[5], label_type=label_type)
        task['predictions'].append({
            'model_version': 'prelabeling',
            'result': annotations
        })
生成预标注：
如果 existing_labels 为空，说明当前图像还没有标注数据，此时需要生成预标注结果。
打印提示信息："no labels found, generating prelabels..."。
调用 generate_prelabels 函数，传入：
metadatas：图像配准后生成的元数据，可能包含差分图中最大/最小点等信息；
crop_size：用于确定候选框大小；
method：选择使用哪种预标注策略（例如 "minmax" 或 "YOLO"）；
model_path：若方法为 YOLO，指定模型权重路径；
t_image：传入 images[2]，通常作为差分图或者用于检测的图像；
rt_image：传入 images[5]，另一类图像（比如右侧或另一角度的差分图）；
label_type：标注的类型（例如矩形）。
得到生成的预标注结果后，将它以字典形式存入 task['predictions']，并标记版本为 "prelabeling"。
python
複製
    return task
返回任务数据：
最后，将构造好的任务字典 task 返回。该字典包含所有前端所需的数据和标注信息，供 Label Studio 组件加载显示。
总结
任务构建的核心目的是：将经过预处理后的图像和自动生成（或已有）的标注结果整合为一个符合 Label Studio 接口要求的数据结构。
关键步骤：
将 6 张图像（参考图、测试图、差分图等）转换为 Base64 字符串，并组织成任务数据的 data 部分。
检查是否有现成的标注数据，若有则提取并直接使用；若无，则调用自动预标注函数生成预标注信息。
将预标注结果封装到 predictions 字段中，并返回整个任务字典。
这样设计使得前端组件既能显示预处理后的图像，也能加载或生成初始的标注结果，方便用户进行人工校正。




1. 数据库存储的设计
数据库表结构
在代码中，使用 SQLite 数据库存储标注结果，表结构定义如下（在 app.py 中）：

python
複製
conn = sqlite3.connect(db_path)
cursor = conn.cursor()
cursor.execute('''
CREATE TABLE IF NOT EXISTS results_table (
    id INTEGER PRIMARY KEY,
    image_path TEXT,
    results_json TEXT
)
''')
conn.commit()
conn.close()
id：数据库自动生成的主键。
image_path：用于唯一标识一张图片，一般由数据目录、Lot ID、Image ID 组合而成。
results_json：存储标注结果的 JSON 字符串，这个 JSON 字符串包含了前端标注的详细内容（如标注区域、标签、坐标等）。
2. 前端标注结果如何存储
关键函数介绍
2.1 fetch_results
作用：在用户加载某张图片时，通过 image_path 从数据库中查找该图片是否已有标注记录。
代码逻辑（简化版）：
python
複製
def fetch_results(image_path, db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT results_json FROM results_table WHERE image_path = ?', (image_path,))
    result = cursor.fetchone()
    conn.close()
    if result:
        return json.loads(result[0])
    return None
使用场景：在 app.py 中，加载当前图像之前，会先调用该函数，如果有记录，则将已有标注加载到 Label Studio 组件中，方便用户修改或查看已有结果。
2.2 save_json_to_sqlite
作用：当用户在前端标注页面提交标注结果后，将返回的 JSON 标注结果存入数据库中。
代码逻辑（简化版）：
python
複製
def save_json_to_sqlite(img_path, results_raw, db_path):
    json_string = json.dumps(results_raw)
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    # 检查该 image_path 是否已有记录
    cursor.execute('SELECT results_json FROM results_table WHERE image_path = ?', (img_path,))
    existing_entry = cursor.fetchone()
    if existing_entry:
        existing_json_string = existing_entry[0]
        if existing_json_string == json_string:
            print("Same img_path and same results_raw. No update needed.")
        else:
            cursor.execute('UPDATE results_table SET results_json = ? WHERE image_path = ?', (json_string, img_path))
            print("Same img_path but different results_raw. Updated the entry.")
    else:
        cursor.execute('INSERT INTO results_table (image_path, results_json) VALUES (?, ?)', (img_path, json_string))
        print("New img_path. Inserted a new entry.")
    conn.commit()
    conn.close()
    print("Connection closed")
使用场景：在前端标注提交后，app.py 会检查返回的 results_raw（标注结果），并调用该函数把标注结果与对应的 image_path 写入数据库。这样下次加载该图片时，就能通过 fetch_results 找到对应的标注数据。
3. 前端如何使用这些后端代码
在你的 app.py 中，整个流程大致如下：

加载图像及初始化任务

根据 Lot ID 与 Image ID 构造一个标识符（如 img_path），例如：
python
複製
img_path = f"{data_dir}, {selected_lot_id}, {selected_image_id}"
通过 fetch_results(img_path, db_path) 检查该图片是否已有标注记录。如果有，就直接加载已有标注；否则，使用预标注逻辑生成初始标注。
构造任务并调用 Label Studio 组件

调用 task_generator 构造任务数据，将图像数据和预标注或已有标注封装在任务字典中，然后调用：
python
複製
results_raw = st_labelstudio(config, interfaces, user, task_generator(...))
组件会展示图像及标注信息，用户可以对其进行修改。
提交标注后保存到后端

当用户点击“Submit”后，组件返回更新后的标注结果（JSON 格式）。在 app.py 中，代码会检测是否有变化（使用 has_results_raw_changed 函数）。
如果有变化，则调用 save_json_to_sqlite(img_path, results_raw, db_path) 将最新的标注结果存入数据库中。
页面刷新与数据更新

保存成功后，程序更新 st.session_state.image_index，然后调用 st.rerun() 刷新页面，这样下一次加载时，就会根据当前的 image_path 使用 fetch_results 从数据库中加载最新标注记录。
4. 总结
后端数据库主要通过 image_path 来作为唯一标识，每个标注记录以 JSON 字符串的形式保存到 results_json 字段。
当用户加载图片时，调用 fetch_results 检查是否有现成的标注，若有则加载到前端。
当用户完成标注并提交后，调用 save_json_to_sqlite 将返回的标注结果存储或更新到数据库中。
这套流程保证了前端标注结果能够持久化存储，方便后续查询、更新或导出数据用于训练模型。



1. 初始化与设置
页面配置与状态初始化
页面设置
在文件最开始调用了：

python
複製
st.set_page_config(layout='wide')
这将页面布局设置为宽屏模式，保证在显示多张图像（例如 6 张图像视图）时能够有足够空间。

Session State 的初始化
为了在页面刷新时保持一些状态信息（例如当前图像索引、批次索引以及上一次提交的标注结果），代码检查并初始化了以下变量：

python
複製
if 'previous_results_raw' not in st.session_state:
    st.session_state.previous_results_raw = None
if 'image_index' not in st.session_state:
    st.session_state.image_index = 0
if 'lot_index' not in st.session_state:
    st.session_state.lot_index = 0
这些变量后续用于控制用户浏览图像、保持已有标注数据以及管理批次切换。

2. 数据库初始化
在前端启动时，代码会连接 SQLite 数据库并创建存储标注结果的表：

python
複製
conn = sqlite3.connect(db_path)
cursor = conn.cursor()
cursor.execute('''
CREATE TABLE IF NOT EXISTS results_table (
    id INTEGER PRIMARY KEY,
    image_path TEXT,
    results_json TEXT
)
''')
conn.commit()
conn.close()
结果表结构：
image_path 用作唯一标识，每条记录对应一张图片。
results_json 存储标注数据（JSON 格式），这部分数据会在标注提交后写入数据库中。
3. 侧边栏选择与图像导航
数据目录与批次（Lot ID）选择
通过读取数据目录 data_dir 下的文件夹，筛选出所有批次（Lot ID），排除无关文件夹（如 log）。
使用 st.sidebar.selectbox 展示批次列表，用户可以选择不同批次；同时代码利用 st.session_state.lot_index 来保持当前选中批次。
图像索引与前进后退按钮
图像编号列表
调用函数 get_lrf_file 和 get_defect_list 得到当前批次中的缺陷文件及对应的图像编号列表（以及缺陷类型信息）。
按钮操作
在侧边栏中设置了“Previous Image”和“Next Image”按钮，点击后会更新 st.session_state.image_index。当到达当前批次的头或尾时，还会自动更新 lot_index（即切换批次），从而实现连续浏览整个数据集。
图像编号选择
利用 st.sidebar.selectbox 展示当前批次中的图像编号，允许用户直接选择某张图像进行标注。
4. 图像预处理与任务数据构建
获取处理后的图像数据
调用 get_image_pair_for_studio_input
这部分代码分别为两种图像类型（例如标记为 "T" 和 "Rt"）调用该函数，完成以下操作：

加载图像对：通过 Lot ID、Image ID 定位到对应的图像文件夹，从中加载测试图和参考图。
图像对齐与差分计算：调用内部的 load_defect_pair 和 get_diff_map 进行图像配准，计算两图差分（用来突出缺陷）。
颜色映射与卷积滤波：对参考图、测试图使用灰度映射，对差分图使用“seismic”映射，再经过卷积滤波平滑处理。
Base64 编码：将处理后的图像转换为 Base64 编码字符串，方便直接嵌入 HTML 显示。
组合图像数据
得到的图像数据被组合成一个列表 images_base64，通常包含 6 个视图（例如 image1~image6，对应不同角度或处理方式的图像）。

构造任务数据（Task）
调用 task_generator
使用预处理后的图像和部分元数据（metadatas），调用 task_generator 构造任务字典：
任务字典的 data 字段中存放图像数据（以 "data:image/jpeg;base64,..." 格式），供 Label Studio 组件加载。
如果数据库中已存在标注（通过 fetch_results 判断），则将已有标注传入；否则，根据预设方法（例如 YOLO 或 minmax）生成预标注信息，并将其存入 predictions 字段。
5. 前端标注交互与结果提交
Label Studio 组件调用
调用：
python
複製
results_raw = st_labelstudio(config, interfaces, user, task_generator(...))
这一步启动了 Label Studio 组件，展示任务数据（图像及标注）给用户。
config：定义了前端界面布局（如两行显示图像、每个图像对应的标注工具）。
interfaces 与 user：配置了用户界面中显示的工具和当前标注者信息。
task_generator 返回的任务数据包含了图像和初始标注。
用户标注与提交
用户在前端交互界面中可以调整预标注的边界框或直接添加新的标注。
当用户点击“Submit”后，Label Studio 组件会将标注结果（JSON 格式）返回给应用，并存储在变量 results_raw 中。
标注结果处理与存储
检测标注变化
使用 has_results_raw_changed 函数比较当前提交的标注数据与上次提交的数据是否有变化，以避免重复保存。
保存到数据库
调用 save_json_to_sqlite(img_path, results_raw, db_path)，将当前 image_path 对应的标注结果写入数据库（如有重复则更新记录）。
更新状态与刷新页面
保存完成后，更新 st.session_state.image_index（自动切换到下一张图像），并调用 st.rerun() 刷新页面，使得下次加载时能显示最新的标注记录。
6. 整体流程总结
初始化：设置页面布局、初始化 Session State 与数据库表。
侧边栏选择：用户通过侧边栏选择批次（Lot ID）和图像编号，同时通过前后按钮导航浏览图像。
图像预处理：调用相关函数加载图像对，进行图像配准、差分计算、颜色映射及卷积滤波，最终转换成 Base64 格式。
任务构建：将图像数据和预标注或已有标注整合到任务字典中，传递给 Label Studio 组件。
用户标注：用户在交互界面中修改或确认标注，提交后返回标注结果。
结果存储与更新：检测结果变化后，通过 save_json_to_sqlite 将标注结果存入 SQLite 数据库，同时更新 Session State 并刷新页面。
这种设计使得前端和后端之间形成一个闭环：用户每次标注都会写入数据库，下次加载时直接从数据库读取已有标注，保证数据的一致性和持久化存储，同时支持用户在每次标注后自动切换到下一张图像进行连续作业。





