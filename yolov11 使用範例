æ•°æ®é›†é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼ˆdataset.yamlï¼‰ï¼š
train: path/to/your/train/images  # è®­ç»ƒå›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
val: path/to/your/val/images      # éªŒè¯å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„

names:
  0: defect_type_1
  1: defect_type_2
  # æ·»åŠ æ›´å¤šç±»åˆ«



ç›®å½•ç»“æ„ç¤ºä¾‹ï¼š
datasets/
    my_dataset/
        images/
            train/  # è®­ç»ƒå›¾ç‰‡
            val/    # éªŒè¯å›¾ç‰‡
        labels/
            train/  # è®­ç»ƒæ ‡ç­¾ (YOLOæ ¼å¼)
            val/    # éªŒè¯æ ‡ç­¾ (YOLOæ ¼å¼)







########################yolov11 ä½¿ç”¨é æ¸¬å¤šç¨®é¡/æ•´é«”   ç¼ºé™·æƒ…æ³ sample code###########################

import torch
from yolov11 import YOLOv11  # å‡è®¾æœ‰ä¸€ä¸ª YOLOv11 çš„ Python å®ç°

# åŠ è½½é¢„è®­ç»ƒçš„ YOLOv11 æ¨¡å‹
model = YOLOv11(pretrained=True)
model.eval()

# å®šä¹‰éªŒè¯æ•°æ®é›†çš„è·¯å¾„
val_data_path = 'path/to/your/validation/dataset'  # æ›¿æ¢ä¸ºæ‚¨çš„éªŒè¯æ•°æ®é›†è·¯å¾„

# æ‰§è¡ŒéªŒè¯
results = model.val(data=val_data_path)

# è¾“å‡ºæ•´ä½“è¯„ä¼°æŒ‡æ ‡
print(f"Overall mAP@0.5: {results['mAP_0.5']:.4f}")
print(f"Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
print(f"Overall Precision: {results['precision']:.4f}")
print(f"Overall Recall: {results['recall']:.4f}")
print(f"Overall F1 Score: {results['f1']:.4f}")

# è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯„ä¼°æŒ‡æ ‡
for idx, class_name in enumerate(results['names']):
    class_precision = results['per_class_precision'][idx]
    class_recall = results['per_class_recall'][idx]
    class_f1 = results['per_class_f1'][idx]
    class_ap = results['per_class_ap'][idx]
    print(f"Class: {class_name}")
    print(f"  Precision: {class_precision:.4f}")
    print(f"  Recall: {class_recall:.4f}")
    print(f"  F1 Score: {class_f1:.4f}")
    print(f"  AP@0.5: {class_ap:.4f}")



æ³¨æ„äº‹é¡¹ï¼š

ç»“æœå­—å…¸ç»“æ„ï¼šä¸Šè¿°ä»£ç å‡è®¾ results å­—å…¸åŒ…å«ä»¥ä¸‹é”®ï¼š

'mAP_0.5'ï¼šæ•´ä½“ mAP@0.5 å€¼ã€‚
'mAP_0.5:0.95'ï¼šæ•´ä½“ mAP@0.5:0.95 å€¼ã€‚
'precision'ï¼šæ•´ä½“ç²¾åº¦ã€‚
'recall'ï¼šæ•´ä½“å¬å›ç‡ã€‚
'f1'ï¼šæ•´ä½“ F1 åˆ†æ•°ã€‚
'names'ï¼šç±»åˆ«åç§°åˆ—è¡¨ã€‚
'per_class_precision'ï¼šæ¯ä¸ªç±»åˆ«çš„ç²¾åº¦åˆ—è¡¨ã€‚
'per_class_recall'ï¼šæ¯ä¸ªç±»åˆ«çš„å¬å›ç‡åˆ—è¡¨ã€‚
'per_class_f1'ï¼šæ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°åˆ—è¡¨ã€‚
'per_class_ap'ï¼šæ¯ä¸ªç±»åˆ«çš„ AP@0.5 å€¼åˆ—è¡¨ã€‚
æ¨¡å‹å®ç°ï¼šç¡®ä¿æ‚¨ä½¿ç”¨çš„ YOLOv11 å®ç°çš„ val æ–¹æ³•è¿”å›çš„ results å­—å…¸åŒ…å«ä¸Šè¿°é”®ã€‚å¦‚æœæ²¡æœ‰ï¼Œæ‚¨å¯èƒ½éœ€è¦æŸ¥çœ‹æ¨¡å‹çš„æºç ï¼Œäº†è§£å¦‚ä½•æå–æ¯ä¸ªç±»åˆ«çš„è¯„ä¼°æŒ‡æ ‡ã€‚

è¯„ä¼°æŒ‡æ ‡è®¡ç®—ï¼šå¦‚æœæ¨¡å‹æœªæä¾›æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼Œæ‚¨å¯èƒ½éœ€è¦æ‰‹åŠ¨è®¡ç®—ã€‚è¿™æ¶‰åŠå°†é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ«çš„ TPã€FPã€FNï¼Œç„¶åæ ¹æ®è¿™äº›å€¼è®¡ç®—ç²¾åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚è¿™å¯èƒ½éœ€è¦æ·±å…¥äº†è§£æ¨¡å‹çš„è¾“å‡ºæ ¼å¼å’Œè¯„ä¼°æµç¨‹ã€‚

é€šè¿‡ä¸Šè¿°ä¿®æ”¹ï¼Œæ‚¨çš„ä»£ç å°†èƒ½å¤Ÿè¾“å‡ºæ¯ä¸ªç¼ºé™·ç±»åˆ«çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¸®åŠ©æ‚¨æ›´ç»†è‡´åœ°è¯„ä¼°æ¨¡å‹åœ¨å„ä¸ªç±»åˆ«ä¸Šçš„æ€§èƒ½ã€‚




åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ä¸»è¦åŒ…æ‹¬ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ã€å¬å›ç‡ï¼ˆRecallï¼‰ã€F1 åˆ†æ•°ï¼ˆF1 Scoreï¼‰ã€å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰å’Œå¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰ã€‚ç†è§£è¿™äº›æŒ‡æ ‡çš„è®¡ç®—å…¬å¼å’Œç›®çš„ï¼Œæœ‰åŠ©äºåˆ¤æ–­æ¨¡å‹çš„ä¼˜åŠ£ï¼Œå¹¶å‘ä»–äººæ¸…æ™°åœ°è§£é‡Šæ¨¡å‹æ€§èƒ½ã€‚

1. ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰

è®¡ç®—å…¬å¼ï¼š

Precision
=
ğ‘‡
ğ‘ƒ
ğ‘‡
ğ‘ƒ
+
ğ¹
ğ‘ƒ
Precision= 
TP+FP
TP
â€‹
 

å…¶ä¸­ï¼ŒTPï¼ˆTrue Positiveï¼‰è¡¨ç¤ºè¢«æ­£ç¡®æ£€æµ‹ä¸ºç›®æ ‡çš„æ•°é‡ï¼ŒFPï¼ˆFalse Positiveï¼‰è¡¨ç¤ºè¢«é”™è¯¯æ£€æµ‹ä¸ºç›®æ ‡çš„æ•°é‡ã€‚

ç›®çš„ï¼š

ç²¾ç¡®ç‡è¡¡é‡æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå³æ¨¡å‹é¢„æµ‹ä¸ºç›®æ ‡çš„å®ä¾‹ä¸­ï¼Œæœ‰å¤šå°‘æ˜¯çœŸæ­£çš„ç›®æ ‡ã€‚é«˜ç²¾ç¡®ç‡è¡¨ç¤ºæ¨¡å‹çš„è¯¯æŠ¥ç‡ä½ã€‚

2. å¬å›ç‡ï¼ˆRecallï¼‰

è®¡ç®—å…¬å¼ï¼š

Recall
=
ğ‘‡
ğ‘ƒ
ğ‘‡
ğ‘ƒ
+
ğ¹
ğ‘
Recall= 
TP+FN
TP
â€‹
 

å…¶ä¸­ï¼ŒFNï¼ˆFalse Negativeï¼‰è¡¨ç¤ºè¢«æ¼æ£€çš„ç›®æ ‡æ•°é‡ã€‚

ç›®çš„ï¼š

å¬å›ç‡è¡¡é‡æ¨¡å‹çš„æ£€æµ‹èƒ½åŠ›ï¼Œå³å®é™…å­˜åœ¨çš„ç›®æ ‡ä¸­ï¼Œæœ‰å¤šå°‘è¢«æ¨¡å‹æˆåŠŸæ£€æµ‹å‡ºæ¥ã€‚é«˜å¬å›ç‡è¡¨ç¤ºæ¨¡å‹çš„æ¼æ£€ç‡ä½ã€‚

3. F1 åˆ†æ•°ï¼ˆF1 Scoreï¼‰

è®¡ç®—å…¬å¼ï¼š

ğ¹
1
=
2
Ã—
Precision
Ã—
Recall
Precision
+
Recall
F1=2Ã— 
Precision+Recall
PrecisionÃ—Recall
â€‹
 

ç›®çš„ï¼š

F1 åˆ†æ•°æ˜¯ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼Œæä¾›äº†ä¸¤è€…ä¹‹é—´çš„å¹³è¡¡è¯„ä¼°ã€‚å½“éœ€è¦åœ¨ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´å–å¾—å¹³è¡¡æ—¶ï¼ŒF1 åˆ†æ•°æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æŒ‡æ ‡ã€‚

4. å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰

è®¡ç®—æ–¹æ³•ï¼š

AP æ˜¯ Precision-Recall æ›²çº¿ä¸‹çš„é¢ç§¯ã€‚é€šè¿‡æ”¹å˜æ£€æµ‹é˜ˆå€¼ï¼Œç»˜åˆ¶å‡ºä¸åŒçš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ç»„åˆï¼Œå½¢æˆæ›²çº¿ï¼Œç„¶åè®¡ç®—è¯¥æ›²çº¿ä¸‹çš„é¢ç§¯å³ä¸º APã€‚

ç›®çš„ï¼š

AP è¡¡é‡æ¨¡å‹åœ¨ç‰¹å®šç±»åˆ«ä¸Šçš„æ£€æµ‹æ€§èƒ½ï¼Œç»¼åˆè€ƒè™‘äº†ä¸åŒé˜ˆå€¼ä¸‹çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ã€‚

5. å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰

è®¡ç®—æ–¹æ³•ï¼š

mAP æ˜¯å¯¹æ‰€æœ‰ç±»åˆ«çš„ AP å–å¹³å‡å€¼ã€‚

ç›®çš„ï¼š

mAP æä¾›äº†æ¨¡å‹åœ¨æ‰€æœ‰ç±»åˆ«ä¸Šçš„æ€»ä½“æ£€æµ‹æ€§èƒ½ï¼Œæ˜¯è¯„ä¼°å¤šç±»åˆ«ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„å…³é”®æŒ‡æ ‡ã€‚

å¦‚ä½•åˆ¤æ–­æ¨¡å‹çš„å¥½å

åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½æ—¶ï¼Œåº”ç»¼åˆè€ƒè™‘ä¸Šè¿°æŒ‡æ ‡ï¼š

é«˜ç²¾ç¡®ç‡å’Œé«˜å¬å›ç‡ï¼šç†æƒ³æƒ…å†µä¸‹ï¼Œæ¨¡å‹åº”å…·æœ‰é«˜ç²¾ç¡®ç‡å’Œé«˜å¬å›ç‡ï¼Œå³è¯¯æŠ¥å’Œæ¼æ£€éƒ½å°‘ã€‚

F1 åˆ†æ•°ï¼šå½“éœ€è¦åœ¨ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´å–å¾—å¹³è¡¡æ—¶ï¼Œå…³æ³¨ F1 åˆ†æ•°ã€‚

AP å’Œ mAPï¼šå¯¹äºå¤šç±»åˆ«æ£€æµ‹ä»»åŠ¡ï¼Œå…³æ³¨æ¯ä¸ªç±»åˆ«çš„ AP å’Œæ•´ä½“çš„ mAPï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å„ç±»åˆ«ä¸Šçš„æ€§èƒ½å’Œæ€»ä½“è¡¨ç°ã€‚

é€šè¿‡å…¨é¢åˆ†æè¿™äº›æŒ‡æ ‡ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°åˆ¤æ–­æ¨¡å‹çš„ä¼˜åŠ£ï¼Œå¹¶ä¸ºæ¨¡å‹çš„æ”¹è¿›æä¾›æŒ‡å¯¼ã€‚
############################################################################################################################








##########################################
import torch
from yolov11 import YOLOv11  # å‡è®¾æœ‰ä¸€ä¸ª YOLOv11 çš„ Python å®ç°

# åŠ è½½é¢„è®­ç»ƒçš„ YOLOv11 æ¨¡å‹
model = YOLOv11(pretrained=True)
model.eval()

# å®šä¹‰éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†çš„è·¯å¾„
val_data_path = 'path/to/your/validation/dataset'  # æ›¿æ¢ä¸ºæ‚¨çš„éªŒè¯æ•°æ®é›†è·¯å¾„
test_data_path = 'path/to/your/test/dataset'       # æ›¿æ¢ä¸ºæ‚¨çš„æµ‹è¯•æ•°æ®é›†è·¯å¾„

def evaluate_model(data_path, dataset_type='Validation'):
    """
    ä½¿ç”¨æŒ‡å®šçš„æ•°æ®é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

    å‚æ•°ï¼š
    - data_path: æ•°æ®é›†çš„è·¯å¾„
    - dataset_type: æ•°æ®é›†ç±»å‹ï¼ˆ'Validation' æˆ– 'Test'ï¼‰
    """
    print(f"Evaluating on {dataset_type} Dataset...")
    results = model.val(data=data_path)

    # è¾“å‡ºæ•´ä½“è¯„ä¼°æŒ‡æ ‡
    print(f"{dataset_type} Dataset - Overall mAP@0.5: {results['mAP_0.5']:.4f}")
    print(f"{dataset_type} Dataset - Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
    print(f"{dataset_type} Dataset - Overall Precision: {results['precision']:.4f}")
    print(f"{dataset_type} Dataset - Overall Recall: {results['recall']:.4f}")
    print(f"{dataset_type} Dataset - Overall F1 Score: {results['f1']:.4f}")

    # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯„ä¼°æŒ‡æ ‡
    for idx, class_name in enumerate(results['names']):
        class_precision = results['per_class_precision'][idx]
        class_recall = results['per_class_recall'][idx]
        class_f1 = results['per_class_f1'][idx]
        class_ap = results['per_class_ap'][idx]
        print(f"Class: {class_name}")
        print(f"  Precision: {class_precision:.4f}")
        print(f"  Recall: {class_recall:.4f}")
        print(f"  F1 Score: {class_f1:.4f}")
        print(f"  AP@0.5: {class_ap:.4f}")

# å¯¹éªŒè¯æ•°æ®é›†è¿›è¡Œè¯„ä¼°
evaluate_model(val_data_path, 'Validation')

# å¯¹æµ‹è¯•æ•°æ®é›†è¿›è¡Œè¯„ä¼°
evaluate_model(test_data_path, 'Test')


æ³¨æ„äº‹é¡¹ï¼š

æ•°æ®é›†é…ç½®ï¼šç¡®ä¿ val_data_path å’Œ test_data_path åˆ†åˆ«æŒ‡å‘éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†çš„é…ç½®æ–‡ä»¶ï¼ˆé€šå¸¸ä¸º .yaml æ ¼å¼ï¼‰ã€‚è¿™äº›é…ç½®æ–‡ä»¶åº”åŒ…å«æ•°æ®é›†çš„ç›¸å…³ä¿¡æ¯ï¼Œå¦‚å›¾åƒè·¯å¾„ã€æ ‡ç­¾è·¯å¾„å’Œç±»åˆ«åç§°ç­‰ã€‚

è¯„ä¼°æ–¹æ³•ï¼šä¸Šè¿°ä»£ç å‡è®¾ YOLOv11 ç±»æä¾›äº† val æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ¥å—æ•°æ®é›†é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼Œå¹¶è¿”å›åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„ç»“æœå­—å…¸ã€‚å¦‚æœæ‚¨çš„å®ç°æœ‰æ‰€ä¸åŒï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚

æ€§èƒ½æŒ‡æ ‡ï¼šresults å­—å…¸åº”åŒ…å«æ•´ä½“å’Œæ¯ä¸ªç±»åˆ«çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚ mAPã€Precisionã€Recallã€F1 Score ç­‰ã€‚å¦‚æœæŸäº›æŒ‡æ ‡ä¸å¯ç”¨ï¼Œè¯·æ ¹æ®æ‚¨çš„éœ€æ±‚å’Œæ¨¡å‹å®ç°è¿›è¡Œç›¸åº”çš„ä¿®æ”¹ã€‚

é€šè¿‡ä¸Šè¿°ä»£ç ï¼Œæ‚¨å¯ä»¥å¯¹éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è¾“å‡ºæ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡ã€‚è¿™æœ‰åŠ©äºå…¨é¢äº†è§£æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„ä¼˜åŒ–æä¾›å‚è€ƒã€‚
##########################################################################################################







#####################################################################################################

1. æ•°æ®é›†ç›®å½•ç»“æ„ç¤ºä¾‹
å‡è®¾æ‚¨çš„æ•°æ®é›†å­˜æ”¾åœ¨ä¸€ä¸ªæ ¹ç›®å½•ä¸‹ï¼Œç›®å½•ç»“æ„å¦‚ä¸‹ï¼š


dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/     # è®­ç»ƒå›¾åƒ
â”‚   â”œâ”€â”€ val/       # éªŒè¯å›¾åƒ
â”‚   â””â”€â”€ test/      # æµ‹è¯•å›¾åƒ
â””â”€â”€ labels/
    â”œâ”€â”€ train/     # è®­ç»ƒæ ‡ç­¾
    â”œâ”€â”€ val/       # éªŒè¯æ ‡ç­¾
    â””â”€â”€ test/      # æµ‹è¯•æ ‡ç­¾
images/ æ–‡ä»¶å¤¹ä¸­å­˜æ”¾æ‰€æœ‰å›¾åƒæ–‡ä»¶ã€‚

train/ ç”¨äºè®­ç»ƒçš„å›¾åƒã€‚
val/ ç”¨äºéªŒè¯çš„å›¾åƒã€‚
test/ ç”¨äºæµ‹è¯•çš„å›¾åƒã€‚
labels/ æ–‡ä»¶å¤¹ä¸­å­˜æ”¾å›¾åƒå¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶ã€‚

æ¯ä¸ªæ ‡ç­¾æ–‡ä»¶çš„æ–‡ä»¶åä¸å¯¹åº”å›¾åƒç›¸åŒï¼ˆä½†æ‰©å±•åä¸º .txtï¼‰ï¼Œæ–‡ä»¶å†…å®¹é‡‡ç”¨ YOLO æ ¼å¼ã€‚
2. æ ‡ç­¾æ–‡ä»¶æ ¼å¼è¯´æ˜
æ¯ä¸ªæ ‡ç­¾æ–‡ä»¶é‡‡ç”¨ YOLO æ ¼å¼ï¼Œä¸€è¡Œå¯¹åº”å›¾åƒä¸­çš„ä¸€ä¸ªç›®æ ‡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š


<class_id> <x_center> <y_center> <width> <height>
å…¶ä¸­ï¼š
<class_id>ï¼šç›®æ ‡ç±»åˆ«çš„ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰ï¼Œå¯¹åº”é…ç½®æ–‡ä»¶ä¸­ names åˆ—è¡¨çš„ç´¢å¼•ï¼ˆä» 0 å¼€å§‹ï¼‰ã€‚
<x_center> å’Œ <y_center>ï¼šè¾¹ç•Œæ¡†ä¸­å¿ƒç‚¹çš„å½’ä¸€åŒ–åæ ‡ï¼ˆç›¸å¯¹äºå›¾åƒå®½åº¦å’Œé«˜åº¦ï¼Œå–å€¼èŒƒå›´ 0ï½1ï¼‰ã€‚
<width> å’Œ <height>ï¼šè¾¹ç•Œæ¡†çš„å½’ä¸€åŒ–å®½åº¦å’Œé«˜åº¦ï¼ˆç›¸å¯¹äºå›¾åƒå°ºå¯¸ï¼Œå–å€¼èŒƒå›´ 0ï½1ï¼‰ã€‚
ç¤ºä¾‹ï¼š
å‡è®¾æœ‰ä¸€å¼ éªŒè¯å›¾åƒ image1.jpgï¼Œå¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶ image1.txt å†…å®¹å¦‚ä¸‹ï¼š

0 0.5 0.5 0.2 0.3
1 0.7 0.8 0.1 0.1
è¡¨ç¤ºå›¾åƒä¸­æœ‰ä¸¤ä¸ªç›®æ ‡ï¼Œç¬¬ä¸€ä¸ªç›®æ ‡ç±»åˆ«ä¸º 0ï¼Œä¸­å¿ƒåœ¨å›¾åƒæ­£ä¸­å¤®ï¼Œå®½åº¦ä¸ºå›¾åƒå®½åº¦çš„ 20%ã€é«˜åº¦ä¸º 30%ï¼›ç¬¬äºŒä¸ªç›®æ ‡ç±»åˆ«ä¸º 1ï¼Œä¸­å¿ƒä½äºå›¾åƒå®½åº¦ 70% å’Œé«˜åº¦ 80% çš„ä½ç½®ï¼Œè¾¹ç•Œæ¡†å°ºå¯¸ä¸ºå›¾åƒå°ºå¯¸çš„ 10%ã€‚

3. æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼ˆdataset.yamlï¼‰
åˆ›å»ºä¸€ä¸ªåä¸º dataset.yaml çš„æ–‡ä»¶ï¼Œç”¨äºæŒ‡å®šæ•°æ®é›†æ ¹ç›®å½•ã€å„å­ç›®å½•åŠç±»åˆ«ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼š

# æ•°æ®é›†é…ç½®æ–‡ä»¶

# å›¾åƒå’Œæ ‡ç­¾çš„æ ¹ç›®å½•ï¼ˆè¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…è·¯å¾„ï¼‰
path: path/to/your/dataset

# è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•å›¾åƒçš„å­ç›®å½•ï¼ˆç›¸å¯¹äº pathï¼‰
train: images/train
val: images/val
test: images/test

# ç±»åˆ«æ•°ï¼ˆè¯·æ›¿æ¢ä¸ºæ‚¨çš„ç±»åˆ«æ•°é‡ï¼‰
nc: 3

# ç±»åˆ«åç§°åˆ—è¡¨ï¼ˆæŒ‰ç´¢å¼•é¡ºåºå¯¹åº”æ ‡ç­¾ä¸­çš„ class_idï¼‰
names:
  0: defect_type_1
  1: defect_type_2
  2: defect_type_3
è¯·ç¡®ä¿å°† path/to/your/dataset æ›¿æ¢ä¸ºæ‚¨çš„æ•°æ®é›†æ ¹ç›®å½•å®é™…è·¯å¾„ã€‚

4. è¯„ä¼°ä»£ç ç¤ºä¾‹
ä¸‹é¢çš„ä»£ç ä½¿ç”¨æ‚¨æä¾›çš„è¯„ä¼°ä»£ç ï¼Œå¹¶æ¼”ç¤ºå¦‚ä½•åˆ©ç”¨æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚ dataset.yamlï¼‰å¯¹éªŒè¯é›†å’Œæµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ï¼š




import torch
from yolov11 import YOLOv11  # å‡è®¾æœ‰ä¸€ä¸ª YOLOv11 çš„ Python å®ç°

# åŠ è½½é¢„è®­ç»ƒçš„ YOLOv11 æ¨¡å‹
model = YOLOv11(pretrained=True)
model.eval()

# å®šä¹‰æ•°æ®é›†é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼ˆåŒ…å«éªŒè¯å’Œæµ‹è¯•é›†çš„é…ç½®ä¿¡æ¯ï¼‰
data_config_path = 'path/to/your/dataset.yaml'  # æ›¿æ¢ä¸ºæ‚¨çš„æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„

def evaluate_model(data_path, dataset_type='Validation'):
    """
    ä½¿ç”¨æŒ‡å®šçš„æ•°æ®é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

    å‚æ•°ï¼š
    - data_path: æ•°æ®é›†é…ç½®æ–‡ä»¶çš„è·¯å¾„
    - dataset_type: æ•°æ®é›†ç±»å‹ï¼ˆ'Validation' æˆ– 'Test'ï¼‰
    """
    print(f"Evaluating on {dataset_type} Dataset...")
    results = model.val(data=data_path)

    # è¾“å‡ºæ•´ä½“è¯„ä¼°æŒ‡æ ‡
    print(f"{dataset_type} Dataset - Overall mAP@0.5: {results['mAP_0.5']:.4f}")
    print(f"{dataset_type} Dataset - Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
    print(f"{dataset_type} Dataset - Overall Precision: {results['precision']:.4f}")
    print(f"{dataset_type} Dataset - Overall Recall: {results['recall']:.4f}")
    print(f"{dataset_type} Dataset - Overall F1 Score: {results['f1']:.4f}")

    # è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„è¯„ä¼°æŒ‡æ ‡
    for idx, class_name in enumerate(results['names']):
        class_precision = results['per_class_precision'][idx]
        class_recall = results['per_class_recall'][idx]
        class_f1 = results['per_class_f1'][idx]
        class_ap = results['per_class_ap'][idx]
        print(f"Class: {class_name}")
        print(f"  Precision: {class_precision:.4f}")
        print(f"  Recall: {class_recall:.4f}")
        print(f"  F1 Score: {class_f1:.4f}")
        print(f"  AP@0.5: {class_ap:.4f}")

# ç¤ºä¾‹ç”¨æ³•ï¼šåˆ†åˆ«å¯¹éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†è¿›è¡Œè¯„ä¼°
# å¦‚æœéªŒè¯é›†å’Œæµ‹è¯•é›†éƒ½åœ¨ dataset.yaml ä¸­å®šä¹‰ï¼Œå¯ä»¥ç›´æ¥ç”¨åŒä¸€ä¸ªé…ç½®æ–‡ä»¶è¿›è¡Œè¯„ä¼°ï¼Œ
# æ¨¡å‹å†…éƒ¨ä¼šæ ¹æ®é…ç½®æ–‡ä»¶ä¸­ç›¸åº”çš„ 'val' æˆ– 'test' å­—æ®µåŠ è½½å¯¹åº”æ•°æ®ã€‚

print("----- éªŒè¯é›†è¯„ä¼° -----")
evaluate_model(data_config_path, 'Validation')

print("\n----- æµ‹è¯•é›†è¯„ä¼° -----")
evaluate_model(data_config_path, 'Test')
è¯´æ˜ï¼š

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œdata_config_path æŒ‡å‘çš„æ˜¯æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼ˆdataset.yamlï¼‰ï¼Œè¯¥æ–‡ä»¶ä¸­å·²ç»åŒ…å«äº†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†çš„å›¾åƒç›®å½•ä¿¡æ¯ä»¥åŠç±»åˆ«è®¾ç½®ã€‚
evaluate_model å‡½æ•°è°ƒç”¨ model.val(data=data_path)ï¼Œè¿™å‡è®¾æ‚¨çš„ YOLOv11 å®ç°æ”¯æŒç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ ¹æ®é…ç½®æ–‡ä»¶åŠ è½½ç›¸åº”çš„æ•°æ®é›†ã€‚
æ ¹æ®å®é™…å®ç°ï¼Œå¦‚æœéœ€è¦åˆ†åˆ«æŒ‡å®šéªŒè¯é›†å’Œæµ‹è¯•é›†çš„è·¯å¾„ï¼Œå¯ä»¥è°ƒæ•´é…ç½®æ–‡ä»¶æˆ–åˆ†åˆ«ä¼ å…¥ä¸åŒçš„è·¯å¾„ã€‚


#########################################################




from ultralytics import YOLO  # ä½¿ç”¨å®˜æ–¹API
import torch
from tabulate import tabulate

# åŠ è½½å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹ (æ³¨æ„æ­£ç¡®æ¨¡å‹åç§°)
model = YOLO('yolov11n.pt')  # å®˜æ–¹æ¨¡å‹åä¸ºyolov11nè€Œéyolo11n
model.eval()

def evaluate_model(data_config, dataset_type='val'):
    """
    ä¼˜åŒ–åçš„å¤šåˆ†ç±»è¯„ä¼°å‡½æ•°ï¼ˆç¬¦åˆUltralyticsæ ‡å‡†ï¼‰
    
    å‚æ•°ï¼š
    - data_config: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ (.yaml)
    - dataset_type: æ•°æ®é›†ç±»å‹ ('val' æˆ– 'test')
    """
    print(f"\n{'='*30} {dataset_type.upper()} è¯„ä¼° {'='*30}")
    
    # æ‰§è¡ŒéªŒè¯ï¼ˆæ·»åŠ å…³é”®å‚æ•°ï¼‰
    results = model.val(
        data=data_config,
        split=dataset_type,  # å®˜æ–¹ä½¿ç”¨splitå‚æ•°
        plots=True,          # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        save_json=True,      # ä¿å­˜JSONæ ¼å¼ç»“æœ
        conf=0.01,           # ç½®ä¿¡åº¦é˜ˆå€¼
        iou=0.6              # IoUé˜ˆå€¼
    )
    
    # å…¨å±€æŒ‡æ ‡è¾“å‡º
    print(f"\n[å…¨å±€æŒ‡æ ‡]")
    print(f"mAP@0.5:0.95 | {results.box.map:.4f} (COCOæ ‡å‡†)")
    print(f"mAP@0.5      | {results.box.map50:.4f}")
    print(f"Precision    | {results.box.precision.mean():.4f}")
    print(f"Recall       | {results.box.recall.mean():.4f}")
    print(f"æ¨ç†é€Ÿåº¦     | {results.speed['inference']:.2f} ms/å¼ ")

    # å¤šåˆ†ç±»è¯¦ç»†æŒ‡æ ‡ï¼ˆè¡¨æ ¼åŒ–è¾“å‡ºï¼‰
    print("\n[åˆ†ç±»åˆ«æŒ‡æ ‡]")
    table_data = []
    for idx, name in enumerate(results.names):
        row = [
            name,
            f"{results.box.ap50[idx]:.4f}",
            f"{results.box.ap75[idx]:.4f}",
            f"{results.box.ap[idx]:.4f}",
            f"{results.box.precision[idx]:.4f}",
            f"{results.box.recall[idx]:.4f}"
        ]
        table_data.append(row)
    
    print(tabulate(table_data,
                 headers=['ç±»åˆ«', 'AP50', 'AP75', 'AP50-95', 'Precision', 'Recall'],
                 tablefmt='github'))
    
    # å¯è§†åŒ–ä¿å­˜
    print("\nç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶ï¼š")
    print(f"- æ··æ·†çŸ©é˜µ: runs/detect/val/confusion_matrix.png")
    print(f"- é¢„æµ‹ç¤ºä¾‹: runs/detect/val/val_batch_pred.jpg")
    print(f"- JSONç»“æœ: runs/detect/val/results.json")

# é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç¤ºä¾‹ï¼‰
data_config_path = 'path/to/your/dataset.yaml'

# æ‰§è¡ŒéªŒè¯
if __name__ == "__main__":
    # éªŒè¯é›†è¯„ä¼°
    evaluate_model(data_config_path, 'val')
    
    # æµ‹è¯•é›†è¯„ä¼°ï¼ˆéœ€è¦é…ç½®æ–‡ä»¶ä¸­å®šä¹‰testè·¯å¾„ï¼‰
    evaluate_model(data_config_path, 'test')



ä¸»è¦æ”¹è¿›ç‚¹è¯´æ˜ï¼š
APIæ ‡å‡†åŒ–ï¼š
ä½¿ç”¨from ultralytics import YOLOæ›¿ä»£è‡ªå®šä¹‰å¯¼å…¥
ä¿®æ­£æ¨¡å‹åç§°yolov11n.ptï¼ˆå®˜æ–¹å‘½åè§„èŒƒï¼‰
ä½¿ç”¨splitå‚æ•°æ›¿ä»£è‡ªå®šä¹‰ç±»å‹åˆ¤æ–­
æŒ‡æ ‡è¾“å‡ºä¼˜åŒ–ï¼š
å¢åŠ AP75æŒ‡æ ‡è¾“å‡º
æ·»åŠ æ¨ç†é€Ÿåº¦æ˜¾ç¤º
ä½¿ç”¨è¡¨æ ¼åŒ–è¾“å‡ºåˆ†ç±»åˆ«æŒ‡æ ‡
ç¬¦åˆCOCOæ ‡å‡†æŒ‡æ ‡å‘½åï¼ˆbox.map / box.map50ï¼‰
å‚æ•°ä¼˜åŒ–ï¼š
æ·»åŠ save_json=Trueä¿å­˜å¯è§£æç»“æœ
è®¾ç½®åˆç†é»˜è®¤å€¼conf=0.01å’Œiou=0.6
è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶è·¯å¾„æç¤º
å¤šåˆ†ç±»æ”¯æŒå¼ºåŒ–ï¼š
æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„AP50/AP75/AP50-95
åˆ†ç±»åˆ«ç²¾ç¡®ç‡å’Œå¬å›ç‡å•ç‹¬æ˜¾ç¤º
å…¼å®¹ä¸åŒæ•°æ®åˆ†å‰²ï¼ˆval/testï¼‰
ä½¿ç”¨å‰éœ€ç¡®ä¿ï¼š
bash
pip install ultralytics tabulate
æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶è¯·ç¡®ä¿ï¼š
dataset.yamlä¸­æ­£ç¡®å®šä¹‰testè·¯å¾„
æµ‹è¯•é›†æ ‡ç­¾æ–‡ä»¶ç¬¦åˆYOLOæ ¼å¼
æ¨¡å‹æ–‡ä»¶ä¸ä»»åŠ¡åŒ¹é…ï¼ˆåˆ†ç±»æ•°ä¸€è‡´ï¼‰
############################################






YOLOv11 å¤šåˆ†ç±»ç‰©ä½“æ£€æµ‹è¯„ä¼°å®Œæ•´æŒ‡å—
ä¸€ã€ç¯å¢ƒå‡†å¤‡
bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install ultralytics tabulate matplotlib torch
äºŒã€æ•°æ®é›†é…ç½®æ ‡å‡†
1. ç›®å½•ç»“æ„è§„èŒƒ
bash
datasets/
â””â”€â”€ defect_detection/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ images/  # è®­ç»ƒé›†å›¾åƒ
    â”‚   â””â”€â”€ labels/  # YOLOæ ¼å¼æ ‡ç­¾
    â”œâ”€â”€ val/
    â”‚   â”œâ”€â”€ images/  # éªŒè¯é›†å›¾åƒ
    â”‚   â””â”€â”€ labels/
    â””â”€â”€ test/
        â”œâ”€â”€ images/  # æµ‹è¯•é›†å›¾åƒ
        â””â”€â”€ labels/
2. æ ‡ç­¾æ–‡ä»¶ç¤ºä¾‹
labels/train/image001.txt:
text
0 0.347656 0.489583 0.128906 0.239583
2 0.712891 0.581250 0.089844 0.166667
3. æ•°æ®é›†é…ç½®æ–‡ä»¶ (defect_config.yaml)
text
path: /projects/datasets/defect_detection
train: train/images
val: val/images
test: test/images

nc: 3
names:
  0: scratch
  1: dent
  2: crack
ä¸‰ã€è¯„ä¼°ä»£ç å®ç°
python
from ultralytics import YOLO
from tabulate import tabulate

def main():
    # åˆå§‹åŒ–æ¨¡å‹
    model = YOLO('yolov11n.pt')
    
    # æ‰§è¡Œè¯„ä¼°æµç¨‹
    run_evaluation(model, 'defect_config.yaml')

def run_evaluation(model, config_path):
    """å…¨æµç¨‹è¯„ä¼°å‡½æ•°"""
    # éªŒè¯é›†è¯„ä¼°
    print("\n" + "="*40)
    print("Starting Validation Evaluation")
    results_val = evaluate_model(model, config_path, 'val')
    
    # æµ‹è¯•é›†è¯„ä¼°
    print("\n" + "="*40)
    print("Starting Test Evaluation")
    results_test = evaluate_model(model, config_path, 'test')
    
    return results_val, results_test

def evaluate_model(model, data_config, dataset_type='val'):
    """æ¨¡å‹è¯„ä¼°æ ¸å¿ƒå‡½æ•°"""
    print(f"\n{'='*30} {dataset_type.upper()} EVALUATION {'='*30}")
    
    # æ‰§è¡ŒéªŒè¯æµç¨‹
    results = model.val(
        data=data_config,
        split=dataset_type,
        plots=True,
        save_json=True,
        conf=0.01,
        iou=0.6,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # è¾“å‡ºè¯„ä¼°æŒ‡æ ‡
    print_metrics(results)
    
    return results

def print_metrics(results):
    """ç»“æ„åŒ–è¾“å‡ºè¯„ä¼°æŒ‡æ ‡"""
    # å…¨å±€æŒ‡æ ‡
    print("\n[Global Metrics]")
    print(f"mAP@0.5:0.95 | {results.box.map:.4f} (COCO Primary)")
    print(f"mAP@0.5      | {results.box.map50:.4f}")
    print(f"Precision    | {results.box.precision.mean():.4f} Â±{results.box.precision.std():.4f}")
    print(f"Recall       | {results.box.recall.mean():.4f} Â±{results.box.recall.std():.4f}")
    print(f"Inference Speed | {results.speed['inference']:.2f} ms/img")
    
    # åˆ†ç±»åˆ«æŒ‡æ ‡è¡¨æ ¼
    print("\n[Per-Class Metrics]")
    table_data = [
        [
            name,
            f"{results.box.ap50[idx]:.4f}",
            f"{results.box.ap75[idx]:.4f}",
            f"{results.box.ap[idx]:.4f}",
            f"{results.box.precision[idx]:.4f}",
            f"{results.box.recall[idx]:.4f}"
        ]
        for idx, name in enumerate(results.names)
    ]
    
    print(tabulate(table_data,
                 headers=['Class', 'AP50', 'AP75', 'AP50-95', 'Precision', 'Recall'],
                 tablefmt='github'))
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    print("\nGenerated Files:")
    print(f"- æ··æ·†çŸ©é˜µ: runs/detect/{results.save_dir}/confusion_matrix.png")
    print(f"- é¢„æµ‹å¯è§†åŒ–: runs/detect/{results.save_dir}/val_batch_pred.jpg")
    print(f"- è¯¦ç»†ç»“æœ: runs/detect/{results.save_dir}/results.json")

if __name__ == "__main__":
    main()
å››ã€å…³é”®å‚æ•°è¯´æ˜
å‚æ•°	ç±»å‹	é»˜è®¤å€¼	è¯´æ˜
split	str	'val'	æ•°æ®é›†åˆ†å‰²ç±»å‹ (val/test)
plots	bool	True	ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
save_json	bool	True	ä¿å­˜JSONæ ¼å¼ç»“æœ
conf	float	0.01	æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
iou	float	0.6	IoUé˜ˆå€¼
device	str	auto	è®¡ç®—è®¾å¤‡è‡ªåŠ¨é€‰æ‹©
äº”ã€æ‰§è¡Œä¸è¾“å‡ºè§£è¯»
1. è¿è¡Œå‘½ä»¤
bash
python evaluate.py
2. å…¸å‹è¾“å‡ºç¤ºä¾‹
text
============================== VAL EVALUATION =============================

[Global Metrics]
mAP@0.5:0.95 | 0.6723 (COCO Primary)
mAP@0.5      | 0.8521
Precision    | 0.7812 Â±0.032
Recall       | 0.6934 Â±0.041
Inference Speed | 4.23 ms/img

[Per-Class Metrics]
| Class    |   AP50 |   AP75 |   AP50-95 |   Precision |   Recall |
|----------|--------|--------|-----------|-------------|----------|
| scratch  | 0.8723 | 0.7021 |    0.6423 |      0.8023 |   0.7123 |
| dent     | 0.8345 | 0.6532 |    0.5934 |      0.7623 |   0.6834 |
| crack    | 0.8012 | 0.5921 |    0.5432 |      0.7321 |   0.6532 |
3. è¾“å‡ºæ–‡ä»¶è¯´æ˜
confusion_matrix.png: ç±»åˆ«æ··æ·†çŸ©é˜µ
val_batch_pred.jpg: å…¸å‹æ£€æµ‹ç»“æœå¯è§†åŒ–
results.json: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„JSONæ–‡ä»¶
å…­ã€é«˜çº§é…ç½®å»ºè®®
å¤šGPUè¯„ä¼°åŠ é€Ÿï¼š
python
model.val(..., device=[0,1,2,3])  # ä½¿ç”¨4å—GPU
æ‰¹é‡å¤§å°ä¼˜åŒ–ï¼š
python
model.val(..., batch=64)  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
ç‰¹å®šç±»åˆ«åˆ†æï¼š
python
# åœ¨print_metricså‡½æ•°ä¸­æ·»åŠ 
print(f"\nClass 'crack'è¯¦ç»†æŒ‡æ ‡:")
print(f"- æŸ¥å‡†ç‡: {results.box.precision[2]:.4f}")
print(f"- æ¼æ£€ç‡: {1 - results.box.recall[2]:.4f}")
æœ¬æŒ‡å—å®Œæ•´å®ç°äº†YOLOv11çš„è¯„ä¼°æµç¨‹ï¼ŒåŒ…å«å¤šåˆ†ç±»æ£€æµ‹çš„å…³é”®æŒ‡æ ‡åˆ†æï¼Œå¯æ»¡è¶³å·¥ä¸šçº§ç¼ºé™·æ£€æµ‹ã€å•†å“è¯†åˆ«ç­‰åœºæ™¯çš„è¯„ä¼°éœ€æ±‚ã€‚






ç¬¬äºŒéƒ¨åˆ†ï¼šå·®å¼‚å›¾è®¡ç®—

metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = get_diff_map(ref, test, fn_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_thres
###############################




###############################lrf å›å®¶ä½œæ¥­#############################################
def get_lrf_file(data_dir, selected_lot_id):
    if os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_ADD.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_ADD.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_Classified.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_Classified.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_classified.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_classified.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}.lrf")
    else:
        lrf_file = None

    return lrf_file

def read_lrf_file(file_path):
    with open(file_path, 'r') as file:
        content = file.read()
    return content

def detect_defect_list(content):
    pattern = re.compile(r'\[DefectList\]\s+DefectDataColumn.*?DefectDataList\s+\d+\s+(.*?)\s+(?=\[|$)', re.DOTALL)
    match = pattern.search(content)
    if match:
        # logger.debug("DefectList found in the LRF file.")
        return match.group(1)
    # logger.debug("No DefectList found in the LRF file.")
    return None

def extract_no_and_classtype(defect_data):
    # logger.debug(f"defect_data: {defect_data}")

    lines = defect_data.strip().split('\n')
    results = []
    for line in lines:
        parts = line.split()
        if len(parts) >= 9:
            no = parts[0]
            classtype = parts[8]
            results.append((no, classtype))
    # logger.debug(f"results: {results}")
    return results

def get_defect_list(lrf_path):
    # if data_type == "old_data":
    file_path = lrf_path
    content = read_lrf_file(file_path)
    defect_data = detect_defect_list(content)
    image_list = []
    defect_type = []
    if defect_data:
        results = extract_no_and_classtype(defect_data)
        for no, classtype in results:
            # if classtype == '0' or classtype == '1':
            image_list.append(no)
            defect_type.append(classtype)
    # logger.info(f"image_list:{image_list}")
    return image_list, defect_type 




[DefectList]
DefectDataColumn No X Y W H Kind Ch PixelCount ClassType U/L ShfitDir KindBits MaskToMask Block SubBlock StripeNo Label ChgDiff Comment;
DefectDataList 2518
         1 -65575.3190  48572.2706      2.5000      0.8996 0000000000008c8c 00600000 254   0   2   0 0d000000000d00000008011881100f00000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 
         2 -65509.8190  13637.9665      1.2000      0.8496 0000000000000b08 00000060 185   0   2   0 00000000000000c40008011881100f00000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 
         3 -65505.8690  45174.8051      0.5000      0.1999 0000000000004b00 00000030  17   0   2   0 0080000000c004c40008011000000000000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 

        2514  63532.3185  -7693.8569      0.3250      0.1499 0000000000000b00 02000000   6   0   2   0 0000000000800c040008000000000000000000000000000000000000000000000000000000000000   1   0  -1 1076       -1     -999 
        2515  63758.7310   9989.8003      0.7500      0.5997 000000000000000b 10000000  11  10   2   0 00000000000000330000000081100100000000000000000000000000000000000000000000000000   1   0  -1 1078       -1     -999 
        2516  64450.4435  14399.3323      1.0750      0.7496 0000000000000b0b 00000020  60   0   2   0 00000000000000f70008211081100f00000000000000000000000000000000000000000000000000   1   0  -1 1083       -1     -999 
        2517  64592.7935  38925.8217      2.6250      0.9495 0000000000004b4b 60000000 403   0   2   0 00b0000000c004f70008011881100f00000000000000000000000000000000000000000000000000   1   0  -1 1085       -1     -999 
        2518  64941.1060  42573.1883      0.3500      0.4998 0000000000000100 0000000c  21  10   2   0 00000000000004040000000000000000000000000000000000000000000000000000000000000000   1   0  -1 1087       -1     -999

######################################labeling config

interfaces = [
  "panel",
  "update",
  "controls",
  "side-column",
  "completions:menu",
  "completions:add-new",
  "completions:delete",
  "predictions:menu",
],

user = {
  'pk': 1,
  'firstName': "Labeler",
  'lastName': "",
},
###############################json function

interfaces = [
  "panel",
  "update",
  "controls",
  "side-column",
  "completions:menu",
  "completions:add-new",
  "completions:delete",
  "predictions:menu",
],

user = {
  'pk': 1,
  'firstName': "Labeler",
  'lastName': "",
},

################## image processor
from PIL import Image
import base64
from io import BytesIO
import numpy as np
import json
import cv2
from alignment.alignment import load_defect_pair, get_diff_map
import matplotlib.pyplot as plt
# Function to convert a NumPy array to a Base64 encoded string

def numpy_to_base64(img_array):
    pil_img = Image.fromarray(img_array)
    buffer = BytesIO()
    pil_img.save(buffer, format="JPEG")
    return base64.b64encode(buffer.getvalue()).decode("utf-8")


# Function to load images from a JSON file line by line and save into a dictionary
def load_images_from_json(json_file):
    images = {}
    with open(json_file, 'r') as f:
        for line in f:
            data = json.loads(line)
            lot_id = data['lot_id']
            images[lot_id] = data
    return images

def apply_colormap(np_array, colormap, vmin=None, vmax=None):
    # norm_array = (np_array - np_array.min()) / (np_array.max() - np_array.min())
    if vmin and vmax:
        norm_array = (np_array - vmin) / (vmax - vmin)
    else:
        norm_array = np_array
    colormap_func = plt.get_cmap(colormap)
    colored_array = colormap_func(norm_array)
    return (colored_array[:, :, :3] * 255).astype(np.uint8)

# Define a 2x2 convolution kernel
def conv_kernel(kernel_size):
    return np.ones((kernel_size, kernel_size), dtype=np.float32) / (kernel_size ** 2)

def get_image_pair_for_studio_input(data_dir, selected_lot_id, selected_image_id, selected_image_type, vmin_level, max_features, max_shift, ransac_reproj_threshold, selected_alignment_method, conv_kernel_size):
    img_dir = f"{data_dir}/{selected_lot_id}/{selected_lot_id}/Images/InstantReview{selected_image_type}"
    fn_id = selected_image_id
    ref, test, is_median_ref = load_defect_pair(img_dir, fn_id)
    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
        get_diff_map(ref, test, fn_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method=selected_alignment_method)
    print(f"max_pos:{metadata['max_pos']}")
    print(f"min_pos:{metadata['min_pos']}")

    ref_colored = apply_colormap(proc_ref, 'gray')
    test_colored = apply_colormap(proc_test, 'gray')
    diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)
    conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))
    img1_base64 = numpy_to_base64(ref_colored)
    img2_base64 = numpy_to_base64(test_colored)
    img3_base64 = numpy_to_base64(diff_colored)
    img4_base64 = numpy_to_base64(conv_diff_image)
    return [img1_base64, img2_base64, img4_base64], metadata

##############inference ########################################


def load_image_as_numpy(image_path):
    img = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format
    return np.array(img)

def preprocess_image(img_np):
    if img_np.ndim == 2:  # If the image is grayscale, convert to RGB
        img_np = np.stack((img_np,) * 3, axis=-1)
    elif img_np.shape[2] == 4:  # If the image has an alpha channel, remove it
        img_np = img_np[:, :, :3]
    return img_np

def inference(img_np, ckpt="/home/hubert007/Code/label_tool/labeling/runs/detect/train3/weights/best.pt"):
    # Load the finetuned model checkpoint
    model = YOLO(ckpt)
    
    # Preprocess the image
    img_np = preprocess_image(img_np)
    
    # Perform object detection
    results = model(img_np)
    # print(results['precision'])
    # print(results['recall'])

    # logger.info(f"inference results: {results}")
    # Extract bounding box information
    if results and len(results[0].boxes) > 0:
        boxs_and_labels = []
        # logger.debug(f"cls length: {len(results[0].boxes.cls)}")
        # logger.debug(f"cls shape: {results[0].boxes.cls.shape}")
        for i in range(len(results[0].boxes.cls)):

            # logger.info(f"Detected boxs: {results[0].boxes}")

            box = results[0].boxes  # Assuming we take the first detected box
            label = box.cls[i]
            x1, y1, x2, y2 = box.xyxy[i]
            width = x2 - x1
            height = y2 - y1
            boxs_and_labels.append((x1, y1, width, height, label))
        return boxs_and_labels
    else:
        return None

# Example usage
if __name__ == "__main__":
    image_path = "/mnt/fs0/dataset/Layer_M/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/1411L.png"
    np_image = load_image_as_numpy(image_path)
    bbox = inference(np_image)
    # print('test result', bbox[0][0], bbox[0][1], bbox[0][2], bbox[0][3], bbox[0][4])  #ok  (x1, y1, width, height, label)

    
    if bbox:
        print(f"Top-left: ({bbox[0][0]}, {bbox[0][1]}), Width: {bbox[0][2]}, Height: {bbox[0][3]}, label: {bbox[0][4]}")
    else:
        print("No bounding box detected.")


######################################fine tune
from ultralytics import YOLO
import os
import argparse
from loguru import logger
import yaml


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=200, help='number of epochs')
    parser.add_argument('--imgsz', type=int, default=256, help='image size')
    parser.add_argument('--exp_name', type=str, default=None, help='name of your experiment')
    parser.add_argument('--dataset_path', type=str, required=True, help='dataset path')
    args = parser.parse_args()



    # Create the directory if it doesn't exist
    os.makedirs('prelabel/model_based/YOLO/train_settings', exist_ok=True)

    # Define the content of the YAML file
    yaml_content = {
        'path': args.dataset_path,
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images',
        'nc': 1,
        'names': {
            0: 'defect'
        }
    }

    # Write the content to the YAML file
    with open(f'prelabel/model_based/YOLO/train_settings/{args.exp_name}.yaml', 'w') as yaml_file:
        yaml.dump(yaml_content, yaml_file, default_flow_style=False)


    # Load a model
    model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

    train_results = model.train(data=f"prelabel/model_based/YOLO/train_settings/{args.exp_name}.yaml", epochs=args.epochs, imgsz=args.imgsz, name=args.exp_name)




    # Export the model to ONNX format
    path = model.export(format="onnx")  # return path to exported model

if __name__ == "__main__":
    main()



#########  dataset#################################################


import shutil
from sklearn.model_selection import train_test_split
import json
import os
from PIL import Image
import numpy as np
from torchvision.utils import save_image
import torch
import cv2
import re
import sqlite3
from PIL import Image, ImageDraw
from alignment.alignment import load_defect_pair, get_diff_map
from frontend.image_processor import numpy_to_base64, load_images_from_json, apply_colormap, conv_kernel
import argparse
from loguru import logger
from tqdm import tqdm
import albumentations as A
from albumentations.pytorch import ToTensorV2

def get_augmentation_pipeline(aug_params):
    """
    Create albumentations augmentation pipeline.
    We use "albumentations" because it can handle the augmentations of bounding boxes. For example, when we rotate the image,
    the bounding boxes should also be rotated.
    :param aug_params: dict of augmentation parameters
    :return: albumentations.Compose object with augmentation pipeline
    """
    return A.Compose([
        A.RandomRotate90(p=aug_params.get('rotation_prob', 0.5)),
        A.HorizontalFlip(p=aug_params.get('flip_prob', 0.5)),
        A.VerticalFlip(p=aug_params.get('flip_prob', 0.5)),
        A.Affine(
            scale=aug_params.get('scale', (1.0, 1.2)),
            translate_percent=(0.0, 0.0),
            p=1.0
        ),
        # A.ColorJitter(
        #     brightness=aug_params.get('brightness', 0.2),
        #     contrast=aug_params.get('contrast', 0.2),
        #     p=1.0
        # ),
        ToTensorV2()
    ], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))


def process_and_augment_images(image_array, bounding_boxes, aug_params, img_path, original=False):
    """
    augment the images and bounding boxes.
    """
    # Ensure the image is in RGB format
    if image_array.shape[2] == 4:  # If the image has an alpha channel
        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGRA2RGB)
    else:
        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)

    # Prepare bounding boxes and labels
    bboxes = []
    category_ids = []
    for bbox in bounding_boxes:
        x_center = (bbox['x']+bbox['width']/2) / image_array.shape[1]
        y_center = (bbox['y']+bbox['height']/2) / image_array.shape[0]
        width = bbox['width'] / image_array.shape[1]
        height = bbox['height'] / image_array.shape[0]
        x_center = min(1.0, max(0.0, x_center))
        y_center = min(1.0, max(0.0, y_center))
        width = min(1.0, max(0.0, width))
        height = min(1.0, max(0.0, height))

        label = bbox['label']
        
        if label == "Defect_right" or label == "Defect_left":
            category_ids.append(0)
        elif label == "4D":
            return None
        
        bboxes.append([x_center, y_center, width, height])

    # Apply augmentations
    if original == True:
        return bboxes, category_ids
    try:
        transform = get_augmentation_pipeline(aug_params)
        augmented = transform(image=image_array, bboxes=bboxes, category_ids=category_ids)
        return augmented['image'], augmented['bboxes'], augmented['category_ids']
    except:
        logger.debug(f"error on {img_path}")
        return None

    # return augmented['image'], augmented['bboxes'], augmented['category_ids']

def check_image_variants(image_path):
    """
    Get the base path and file name without extension (1070L.png ,1070U.png ....)
    """
    logger.info(f"Checking image variants for {image_path}")
    base_path, file_name = os.path.split(image_path)
    file_name_no_ext = os.path.splitext(file_name)[0]
    
    # Define the variants
    variants = ['L', 'R', 'U', 'D']
    
    # Initialize a dictionary to store the paths of existing variants
    # Check for each variant
    for variant in variants:
        variant_path = os.path.join(base_path, f"{file_name_no_ext}{variant}.png")
        if os.path.exists(variant_path):
            return variant_path
        

    
def db_to_metadata(db_file, metadata_file, masked_image_output=None):
    """
    Convert SQLite database to metadata JSON file.
    """
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Query to retrieve all rows from the results_table
    cursor.execute('SELECT image_path, results_json FROM results_table')
    rows = cursor.fetchall()

    # Create output directory if it doesn't exist
    if masked_image_output:
        output_dir = masked_image_output
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    # Initialize a list to store metadata
    metadata_list = []

    for row in rows:
        img_path, results_json = row
        split_img_path = img_path.split(', ')
        data_dir = split_img_path[0]
        lot_id = split_img_path[1]
        image_id = split_img_path[2]
        results = json.loads(results_json)
        # logger.debug(f"img_path: {img_path}")
        # Check if 'areas' key exists in the JSON
        if 'areas' in results:
            t_boxes = []
            rt_boxes = []
            areas = results['areas']
            for area_key, area_value in areas.items():
                if area_value['object'] == 'image3' or area_value['object'] == 'image6':
                    label = area_value['results'][0]['value']['rectanglelabels'][0]
                    x = area_value['x']
                    y = area_value['y']
                    width = area_value['width']
                    height = area_value['height']
                    if area_value['object'] == 'image3':
                    # Append metadata to the list
                        t_boxes.append(
                            {"x": x, "y": y, "width": width, "height": height, "label": label} 
                        )
                    else:
                        rt_boxes.append(
                            {"x": x, "y": y, "width": width, "height": height, "label": label} 
                        )

            metadata_list.append({
                "image_path": f"{data_dir}/{lot_id}/{lot_id}/Images/InstantReviewT/{image_id}.png",
                "bounding_box": t_boxes,
            })
            metadata_list.append({
                "image_path": f"{data_dir}/{lot_id}/{lot_id}/Images/InstantReviewRt/{image_id}.png",
                "bounding_box": rt_boxes,
            })

    # Write metadata to a JSON file
    with open(metadata_file, 'w') as json_file:
        json.dump(metadata_list, json_file, indent=4)

    # Close the database connection
    conn.close()
    # json_to_text('metadata.json')

def create_dataset(db_file ,json_file, dataset_dir, image_size=256, diff_map=False, aug_params = None, use_background = False, max_augment_factor = 8):

    print(f"diff_map: {diff_map}")
    # Create dataset directories
    train_images_dir = os.path.join(dataset_dir, 'train/images')
    train_labels_dir = os.path.join(dataset_dir, 'train/labels')
    val_images_dir = os.path.join(dataset_dir, 'val/images')
    val_labels_dir = os.path.join(dataset_dir, 'val/labels')
    test_images_dir = os.path.join(dataset_dir, 'test/images')
    test_labels_dir = os.path.join(dataset_dir, 'test/labels')
    
    os.makedirs(train_images_dir, exist_ok=True)
    os.makedirs(train_labels_dir, exist_ok=True)
    os.makedirs(val_images_dir, exist_ok=True)
    os.makedirs(val_labels_dir, exist_ok=True)
    os.makedirs(test_images_dir, exist_ok=True)
    os.makedirs(test_labels_dir, exist_ok=True)

    # Load JSON data
    db_to_metadata(db_file, json_file)
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    # Split data into train and validation sets
    # First, split the data into train+val and test sets
    train_val_data, test_data = train_test_split(data, test_size=0.1, random_state=42)

    # Then, split the train+val data into train and val sets
    train_data, val_data = train_test_split(train_val_data, test_size=0.2222, random_state=42)  # 0.2222 is approximately 2/9

    # Now you have train_data (70%), val_data (20%), and test_data (10%)
    
    def process_data(data, images_dir, labels_dir, data_type):
        for item in tqdm(data, desc=f"Processing {data_type} data"):
            image_path = item['image_path']
            if diff_map:
                max_features = 1000
                max_shift = 10
                ransac_reproj_threshold = 0.20
                image_dir = os.path.dirname(image_path)
                # the lot_id doesn't matter here, it just affects the metadata created below, which won't be used in this case.
                selected_lot_id = "DummyVariable"
                vmin_level = -0.60
                conv_kernel_size = 2
                
                # Extract the filename
                filename = os.path.basename(image_path)
                if 'InstantReviewT' in image_path:
                    image_type = "T"
                elif 'InstantReviewRt' in image_path:
                    image_type = "Rt"

                # Extract the number part of the filename using regular expressions
                image_id = re.search(r'\d+', filename).group()

                ref, test, is_median_ref = load_defect_pair(image_dir, image_id)

                """
                I think there's a bottleneck here. When the image isn't able to be aligned with correlate, the correlation alignment function outputs an error which
                takes a makes the progress bar stuck for a while. This is worth checking out when optimizing the dataset pipeline. 
                """

                # Here we use Correlate for all images because it can handle broader cases than SIFT.
                try:
                    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
                        get_diff_map(ref, test, image_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method="Correlate")
                except:
                    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
                        get_diff_map(ref, test, image_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method="SIFT")                  
                #save proc_diff as png image
                diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)
                conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))
                diff_image_png = Image.fromarray(conv_diff_image)

                # Save as PNG
                
                label_file = f"{image_type}_{image_id}_diff.txt"
                label_path = os.path.join(labels_dir, label_file)             
                # remove last filename from image_path

                
                #Start augmentation
                bounding_boxes = item['bounding_box']

                #before augmentation, add original image and label into dataset
                
                if len(bounding_boxes) == 0:
                    if use_background == True:
                        continue
                    with open(label_path, "w") as f:
                        pass
                        
                else:
                    try:
                        aug_bboxes, aug_labels = process_and_augment_images(conv_diff_image, bounding_boxes, aug_params, image_path, original=True)
                    except:
                        continue
                    for box, label in zip(aug_bboxes, aug_labels):
                        with open(label_path, "a") as f:
                            f.write(f"{label} {box[0]} {box[1]} {box[2]} {box[3]}\n")
                diff_image_png.save(os.path.join(images_dir, f"{image_type}_{image_id}_diff.png"))

                num_of_augments = np.random.randint(1, max_augment_factor)
                
                for i in range(1, num_of_augments+1):
                    #random vmin level [-1.70, -0.30]
                    vmin_level = torch.FloatTensor(1).uniform_(-1.70, -0.30).item()
                    diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)

                    #apply random convolution kernel
                    conv_kernel_size = torch.randint(1, 4, (1,)).item()
                    conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))

                    #apply albumentation's augmentations.
                    try:
                        aug_image, aug_bboxes, aug_labels = process_and_augment_images(conv_diff_image, bounding_boxes, aug_params, image_path)
                    except:
                        continue
                    png_path = os.path.join(images_dir, f"{image_type}_{image_id}_diff_{str(i)}.png")
                    label_path = os.path.join(labels_dir, f"{image_type}_{image_id}_diff_{str(i)}.txt")

                    np_image = aug_image.permute(1, 2, 0).cpu().numpy()
                    
                    # Convert the NumPy array to a PIL image
                    pil_image = Image.fromarray(np_image)
                    
                    # Save the PIL image
                    pil_image.save(png_path)
                    if len(aug_bboxes) == 0:
                        if use_background == False:
                            continue
                        label = 'None'
                        with open(label_path, 'w') as f:
                            pass
                    else:
                        for bbox, label in zip(aug_bboxes, aug_labels):
                            with open(label_path, 'a') as f:
                                f.write(f"{label} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\n")
            else:
                # Copy image to the dataset directory
                shutil.copy(image_path, images_dir)
                
                # Create label file
                image_name = os.path.basename(image_path)
                label_file = os.path.splitext(image_name)[0] + '.txt'
                label_path = os.path.join(labels_dir, label_file)

    
    # Process train and validation data


    process_data(train_data, train_images_dir, train_labels_dir, "train")
    process_data(val_data, val_images_dir, val_labels_dir, "val")
    process_data(test_data, test_images_dir, test_labels_dir, "test")


def dataset_checker(dataset_path):
    # Directories to check
    directories = ['train', 'val', 'test']
    
    missing_labels = []
    empty_labels = []

    for directory in directories:
        image_dir = os.path.join(dataset_path, directory, 'images')
        label_dir = os.path.join(dataset_path, directory, 'labels')
        
        # Get list of image files
        image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]
        
        for image_file in image_files:
            label_file = os.path.splitext(image_file)[0] + '.txt'
            label_path = os.path.join(label_dir, label_file)
            
            if not os.path.exists(label_path):
                print(f"no label at {image_file}")
                missing_labels.append(os.path.join(directory, 'images', image_file))
            else:
                with open(label_path, 'r') as f:
                    content = f.read().strip()
                    if not content:
                        print(f"empty label at {label_file}")
                        empty_labels.append(os.path.join(directory, 'labels', label_file))
    
    # return missing_labels, empty_labels



def main():
    parser = argparse.ArgumentParser(description="Create YOLO dataset from .db file")
    parser.add_argument("--db_file" , type=str, help="Path to the .db file")
    parser.add_argument("--json_metadata", type=str, help="Path to the JSON metadata file")
    parser.add_argument("--dataset_path", type=str, help="Path to the dataset directory")
    parser.add_argument("--use_diff_map", type=bool, default=True, help="Use diff map as image for the dataset")
    parser.add_argument("--max_augment_factor", type=int, default=8, help="Max number of augmentation data on each image")
    parser.add_argument("--use_background", action='store_true', default=False, help="Use background images for the dataset")
    args = parser.parse_args()

    aug_params = {
        'rotation_prob': 0.5,
        'flip_prob': 0.5,
        'scale': (1.0, 1.2),
        'brightness': 0.2,
        'contrast': 0.2
    }
    create_dataset(args.db_file, args.json_metadata, args.dataset_path, diff_map=args.use_diff_map, aug_params=aug_params, use_background=args.use_background, max_augment_factor=args.max_augment_factor)


if __name__ == "__main__":
    main()
    # dataset_checker("/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/model_based/YOLO/datasets/m_layer_aug_fix")



######yolo prelabel training
from prelabel.model_based.YOLO.inference import inference


def YOLO_prelabel(t_np_image, rt_np_image, model_path, annotations):
        t_bbox = inference(t_np_image, model_path)
        if t_bbox != None:
            for box in t_bbox:
                x = box[0]
                y = box[1]
                width = box[2]
                height = box[3]
                label = box[4]
                for i in range(1, 4):
                    annotation = {
                        'from_name': f'label_image{i}',
                        'to_name': f'image{i}',
                        'type': 'rectanglelabels',
                        'value': {
                            'rectanglelabels': ['Defect_right'],
                            'x': x.item()*100/256,
                            'y': y.item()*100/256,
                            'width': width.item()*100/256,
                            'height': height.item()*100/256,
                        }
                    }       
                    annotations.append(annotation)       
        rt_bbox = inference(rt_np_image, model_path)
        if rt_bbox != None:
            for box in rt_bbox:
                x = box[0]
                y = box[1]
                width = box[2]
                height = box[3]
                label = box[4]
                # The main diference is here: it is using a different range for the loop (image 4,5,6)
                for i in range(4, 7):
                    annotation = {
                        'from_name': f'label_image{i}',
                        'to_name': f'image{i}',
                        'type': 'rectanglelabels',
                        'value': {
                            'rectanglelabels': ['Defect_right'],
                            'x': x.item()*100/256,
                            'y': y.item()*100/256,
                            'width': width.item()*100/256,
                            'height': height.item()*100/256,
                        }
                    }       
                    annotations.append(annotation)      
                return annotations


# YOLO prelabeler training


## Prepare dataset
**Run commands under /toggle_labelstudio**

After finishing labeling, you will get .db file under /toggle_labelstudio.
Convert that .db file into YOLO training format by running the following command:
```bash
python -m prelabel.model_based.YOLO.dataset --db_file example.db --json_metadata example.json --dataset_path dataset_path
```
* `db_file`: .db file path.
* `json_metadata`: desired metadata json file path (will create for you).
* `dataset_path`: output dataset directory (will create for you).

## Train model

### run finetune

```bash
python -m prelabel.model_based.YOLO.finetune --epochs 200 --imgsz 256 --exp_name "experiment name" --dataset_path "dataset_path"
```
Other arguments will be added later(TODO)

###################################################prelabel.py

import cv2
import numpy as np
import base64
from PIL import Image
from io import BytesIO
from prelabel.model_based.YOLO.api import YOLO_prelabel
from prelabel.rule_based.minmax.api import minmax_prelabel
from loguru import logger

def base64_to_numpy(base64_str):
    img_data = base64.b64decode(base64_str)
    img = Image.open(BytesIO(img_data))
    return np.array(img)

def extract_annotations(existing_labels):
    """
    This function extracts the bounding boxes from "existing_labels", which is the dictionary that "st_labelstudio" returns after submit.
    It is quite important to study how "existing_labels" is structured if you want to add new annotation types.
    "polygonlabels" is deprecated currently since the database doesn't handle that format, but as long as you 
    understand "existing_labels" format, you can add it back.
    """
    annotations = []
    if 'areas' in existing_labels:
        # areas describe all the bounded areas (bounding box, polygon..)
        for area_id, area_data in existing_labels['areas'].items():
            if 'results' in area_data:
                for result in area_data['results']:
                    annotation = {
                        "id": result["id"],
                        "from_name": result['from_name'],
                        "to_name": result['to_name'],
                        "type": result['type']
                    }
                    if annotation["type"] == "polygonlabels":
                        annotation["value"] = {
                            "polygonlabels": result['value']['polygonlabels'],
                            "points": [[point['relativeX'], point['relativeY']] for point in area_data['points']],
                        }
                    elif annotation["type"] == "rectanglelabels":
                        annotation["value"] = {
                            "rectanglelabels": result['value']['rectanglelabels'],
                            # converting absolute pixel values into percentage
                            "x": area_data['x']*100/256,
                            "y": area_data['y']*100/256,
                            "width": area_data['width']*100/256,
                            "height": area_data['height']*100/256,
                            "rotation": area_data['rotation'],
                        }
                    annotations.append(annotation)
    return annotations

def generate_prelabels(metadatas, crop_size, method="minmax", model_path=None, t_image=None, rt_image=None, label_type="rectangle"):
    """
    Call your prelabel logics here.
    You can call/define any prelabel method as long as the list "annotations" contains 6 elements each with the following format:
    {
        'from_name': f'label_image{i}',
        'to_name': f'image{i}',
        'type': 'rectanglelabels',
        'value': {
            'rectanglelabels': ['Defect_right'],
            'x': x coord of bounding box's TOP LEFT in PERCENTAGE [0, 100],
            'y': y coord of bounding box's TOP LEFT in PERCENTAGE [0, 100],
            'width': bounding box width in PERCENTAGE [0, 100],
            'height': bounding box height in PERCENTAGE [0, 100],
        }
    } 

    Basically you have to return six bounding boxes for i is from 1~6, representing 6 images (t_ref, t_test, t_diff, rt_ref, rt_test, rt_diff).
    Then these annotations will be rendered onto UI.
    """
    
    annotations = []
    if method == "YOLO":
        # why does YOLO only take in 2 images? Because we only predict on the diff map. t_image is the t_diffmap vice versa.
        t_np_image = base64_to_numpy(t_image)
        rt_np_image = base64_to_numpy(rt_image)
        YOLO_prelabel(t_np_image, rt_np_image, model_path, annotations)
   
    elif method == "minmax":
        # metadatas contains the min/max point, which is acquired from SIFT's alignment functions.
        # crop_size is the size of the bounding box to bound the min/max point. It can be toggled on sidebar.
        minmax_prelabel(metadatas, crop_size, annotations)
    return annotations

def task_generator(images, crop_size, metadatas=None, method="minmax", model_path=None, label_type="rectangle", existing_labels=None):
    """
    This is the MAIN function that handles our customize logic and pass it in to labelstudio as "predictions". 
    Be aware that both "existing labels" and "prelabel predictions" are passed in as "predictions".
    I didn't use "completions", you can ignore that, also you can disable its UI in the UI settings (gear icon).
    """
    task = {
        'completions': [],
        'predictions': [],
        'id': 1,
        'data': {
            'image1': f"data:image/jpeg;base64,{images[0]}",
            'image2': f"data:image/jpeg;base64,{images[1]}",
            'image3': f"data:image/jpeg;base64,{images[2]}",
            'image4': f"data:image/jpeg;base64,{images[3]}",
            'image5': f"data:image/jpeg;base64,{images[4]}",
            'image6': f"data:image/jpeg;base64,{images[5]}"
        }
    }
    if existing_labels:
        annotations = extract_annotations(existing_labels)
        task['predictions'].append({
            'model_version': 'existing_labels',
            'result': annotations
        })
    else:
        print("no labels found, generating prelabels...")
        annotations = generate_prelabels(metadatas, crop_size, method=method, model_path=model_path, t_image=images[2], rt_image=images[5], label_type=label_type)
        task['predictions'].append({
            'model_version': 'prelabeling',
            'result': annotations
        })

    return task


######export_json_and_mask

import sqlite3
import json
from PIL import Image, ImageDraw
import os
from tqdm import tqdm
import argparse

def check_image_variants(image_path):
    # Get the base path and file name without extension
    base_path, file_name = os.path.split(image_path)
    file_name_no_ext = os.path.splitext(file_name)[0]
    
    # Define the variants
    variants = ['L', 'R', 'U', 'D']
    
    # Initialize a dictionary to store the paths of existing variants
    # Check for each variant
    for variant in variants:
        variant_path = os.path.join(base_path, f"{file_name_no_ext}{variant}.png")
        if os.path.exists(variant_path):
            return variant_path
    
def process_db_and_create_masked_images(db_file, output_dir):
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Query to retrieve all rows from the results_table
    cursor.execute('SELECT image_path, results_json FROM results_table')
    rows = cursor.fetchall()

    # Create output directory if it doesn't exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Initialize a list to store metadata
    metadata_list = []
    for row in tqdm(rows, desc=f"Processing images"):
        img_info, results_json = row
        dataset_dir, lot_id, image_id = img_info.replace(" ","").split(',')
        results = json.loads(results_json)
        img_path = os.path.join(dataset_dir, f"{lot_id}/{lot_id}/Images")
        # Check if 'areas' key exists in the JSON
        if 'areas' in results:
            areas = results['areas']
            for area_key, area_value in areas.items():
                if area_value['object'] == 'image3' or area_value['object'] == 'image6':
                    if area_value['object'] == 'image3':
                        full_img_path = os.path.join(img_path, f"InstantReviewT/{image_id}.png")
                    else:
                        full_img_path = os.path.join(img_path, f"InstantReviewRt/{image_id}.png")
                    """
                    If it comes from the right, the defect image is "fn_id.png", if left, it would be "fn_idL.png" or "fn_idU.png" ... etc.
                    """
                    if area_value['results'][0]['value']['rectanglelabels'][0] == "Defect_left":
                        full_img_path = check_image_variants(full_img_path)
                    x = area_value['x']
                    y = area_value['y']
                    width = area_value['width']
                    height = area_value['height']

                    # Open the image
                    img = Image.open(full_img_path).convert("RGBA")
                    masked_img = Image.new("RGBA", img.size, (0, 0, 0, 255))
                    draw = ImageDraw.Draw(masked_img)

                    # Draw the bounding box on the masked image (make it transparent)
                    draw.rectangle([x, y, x + width, y + height], fill=(0, 0, 0, 0))

                    # Composite the original image with the masked image
                    final_img = Image.alpha_composite(img, masked_img)

                    # Save the final masked image
                    output_path = os.path.join(output_dir, f"{lot_id}_{os.path.basename(full_img_path)}")
                    final_img.save(output_path)
                    
                    # Append metadata to the list
                    metadata_list.append({
                        "image_path": full_img_path,
                        "bounding_box": {
                            "x": x,
                            "y": y,
                            "width": width,
                            "height": height
                        }
                    })

    # Write metadata to a JSON file
    with open(os.path.join(output_dir, 'metadata.json'), 'w') as json_file:
        json.dump(metadata_list, json_file, indent=4)

    # Close the database connection
    conn.close()

def main():
    parser = argparse.ArgumentParser(description="Process a SQLite database and create masked images.")
    parser.add_argument('--db_path', type=str, help='Path to the SQLite database file')
    parser.add_argument('--output_dir', type=str, help='Output directory for the masked images')
    args = parser.parse_args()
    process_db_and_create_masked_images(args.db_path, args.output_dir)


if __name__ == "__main__":
    main()



########################################metadata.json
[
    {
        "image_path": "/mnt/fs0/x9u_detection_result/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/771L.png",
        "bounding_box": {
            "x": 137,
            "y": 111,
            "width": 54,
            "height": 23
        }
    },
    {
        "image_path": "/mnt/fs0/x9u_detection_result/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/822L.png",
        "bounding_box": {
            "x": 124,
            "y": 118,
            "width": 12,
            "height": 15
        }
    },]
#################################alightment.py

"""
This file is modified from inspection_sift.py by Carl.
I've removed some functions since the app doesn't use it. 
The most important functions are "load_defect_pair" and "get_diff_map"
"""




import numpy as np
import cv2
import matplotlib.pyplot as plt
import math
import json
import glob
import os
from loguru import logger
from alignment.SIFT.sift import align_images_sift
from alignment.correlation.correlation import align_images_corr



def normalize_images(ref, test):
    """Normalize images using OpenCV functions."""
    # Convert to float32 for processing
    ref_f = ref.astype(np.float32)
    test_f = test.astype(np.float32)

    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    ref_eq = clahe.apply((ref_f * 255).astype(np.uint8)) / 255.0
    test_eq = clahe.apply((test_f * 255).astype(np.uint8)) / 255.0

    # Normalize to zero mean and unit variance
    ref_norm = cv2.normalize(ref_eq, None, 0, 1, cv2.NORM_MINMAX)
    test_norm = cv2.normalize(test_eq, None, 0, 1, cv2.NORM_MINMAX)

    return ref_norm, test_norm

def load_defect_pair(img_path, defect_no):
    """
    This function finds the test and reference image from the dataset directory. (currently from "/mnt/fs0/dataset/Layer_M")
    It handles the suffixes(L, U, Up, Lp ...)
    """
    test = None
    ref_U = None
    ref_L = None
    ref_Up = None
    ref_Lp = None

    images = glob.glob(f'{img_path}/{str(defect_no)}*')
    for image in images:
        if image == f'{img_path}/{str(defect_no)}.png':
            test = image
        elif image == f'{img_path}/{str(defect_no)}U.png':
            ref_U = image
        elif image == f'{img_path}/{str(defect_no)}L.png':
            ref_L = image
        elif image == f'{img_path}/{str(defect_no)}L_p.png':
            ref_Lp = image
        elif image == f'{img_path}/{str(defect_no)}U_p.png':
            ref_Up = image

    if test is None:
        raise ValueError(f'{defect_no}: No Test Image')

    test = cv2.imread(test, cv2.IMREAD_GRAYSCALE)/255

    # Create list to store all available images for median filtering
    all_images = [test]

    ref_u_img = None
    ref_l_img = None
    ref_up_img = None
    ref_lp_img = None
    if ref_U is not None:
        ref_u_img = cv2.imread(ref_U, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_u_img)

    if ref_L is not None:
        ref_l_img = cv2.imread(ref_L, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_l_img)

    if ref_Up is not None:
        ref_up_img = cv2.imread(ref_Up, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_up_img)

    if ref_Lp is not None:
        ref_lp_img = cv2.imread(ref_Lp, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_lp_img)

    # Only use median if we have more than 3 images, otherwise use ref_U as reference
    if len(all_images) > 3:
        reference = np.median(np.stack(all_images), axis=0)
        is_median_ref = True
    else:
        is_median_ref = False
        if ref_u_img is not None:
            reference = ref_u_img
        elif ref_l_img is not None:
            reference = ref_l_img
        elif ref_lp_img is not None:
            reference = ref_lp_img
        elif ref_up_img is not None:
            reference = ref_up_img
        else:
            raise ValueError(f'{defect_no}: No reference image available')

    return reference, test, is_median_ref

def get_diff_map(ref, test, defect_no, lot_id, is_median_ref, max_features=1000, max_shift=10, ransacReprojThreshold=0.0, method="SIFT"):
    """
    input:
    ref : numpy.ndarray, shape(256, 256), grayscale [0,1]
    test : numpy.ndarray, shape(256, 256), grayscale [0,1]

    This function gets you the difference map and its metadata
    You can add your own alignment methods here. As long as you return the following:
    1.aligned_ref : numpy.ndarray, shape(256, 256), grayscale [0,1]
    2.aligned_test : numpy.ndarray, shape(256, 256), grayscale [0,1]
    3.translation : [translation_x, translation_y]  negative means left/up, positive means right/down
    """
    assert(ref.shape == test.shape)

    # logger.debug(f"ref type:{type(ref)}")
    # logger.debug(f"ref shape:{ref.shape}")
    # logger.debug(f"ref:{ref}")
    # logger.debug(f"test type:{type(test)}")
    # logger.debug(f"test shape:{test.shape}")
    # logger.debug(F"test:{test}")
    # Align images using SIFT
    if method == "SIFT":
        if ransacReprojThreshold > 0.0:
            #use RANSAC to filter feature point outliers
            aligned_ref, aligned_test, translation = align_images_sift(ref, test, max_features=max_features, max_shift=max_shift, RANSAC=True, ransacReprojThreshold=ransacReprojThreshold)
        else:
            aligned_ref, aligned_test, translation = align_images_sift(ref, test, max_features=max_features, max_shift=max_shift, RANSAC=False)

        # Convert translation to regular Python float
        translation = [float(translation[0]), float(translation[1])]

    elif method == "Correlate":
        aligned_ref, aligned_test, translation = align_images_corr(ref, test)


    # Normalize images
    processed_ref, processed_test = normalize_images(aligned_ref, aligned_test)
    processed_diff = processed_ref - processed_test
    # Calculate RMSE and max absolute difference
    rmse = float(np.sqrt(np.mean((processed_ref - processed_test) ** 2)))
    max_diff = float(max(abs(np.max(processed_diff)), abs(np.min(processed_diff))))

    def get_ceiling_floor(translation):
        if translation < 0:
            return -max_shift-2+int(math.floor(translation))
        else:
            return max_shift+2+int(math.ceil(translation))
    int_translation = [get_ceiling_floor(t) for t in translation]
    # Define the region of interest excluding the translation area
    roi_start = [max(0, int_translation[0], -int_translation[0]), max(0, int_translation[1], -int_translation[1])]
    roi_end = [min(processed_diff.shape[0], processed_diff.shape[0] + int_translation[0], processed_diff.shape[0] - int_translation[0]),
            min(processed_diff.shape[1], processed_diff.shape[1] + int_translation[1], processed_diff.shape[1] - int_translation[1])]

    # Extract the region of interest

    roi_diff = processed_diff[roi_start[0]:roi_end[0], roi_start[1]:roi_end[1]]

    # Find max/min positions within the region of interest
    max_pos = [int(x) for x in np.unravel_index(roi_diff.argmax(), roi_diff.shape)]
    min_pos = [int(x) for x in np.unravel_index(roi_diff.argmin(), roi_diff.shape)]
    # x,y coord will be swapped because of the way numpy unravels indices
    max_pos = [max_pos[1], max_pos[0]]
    min_pos = [min_pos[1], min_pos[0]]
    # Adjust positions to the original image coordinates
    max_pos = [max_pos[0] + roi_start[0], max_pos[1] + roi_start[1]]
    min_pos = [min_pos[0] + roi_start[0], min_pos[1] + roi_start[1]]

    # Create metadata dictionary with Python native types
    metadata = {
        'lot_id': lot_id,
        'defect_no': int(defect_no),
        'translation': translation,
        'max_difference': float(np.max(processed_diff)),
        'min_difference': float(np.min(processed_diff)),
        'abs_max_difference': max_diff,
        'rmse': rmse,
        'max_pos': max_pos,
        'min_pos': min_pos,
        'is_median_ref': is_median_ref  # Include the median reference flag
    }

    return metadata, processed_ref, processed_test, processed_diff, max_pos, min_pos



