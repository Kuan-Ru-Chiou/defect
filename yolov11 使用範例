数据集配置文件示例（dataset.yaml）：
train: path/to/your/train/images  # 训练图像文件夹路径
val: path/to/your/val/images      # 验证图像文件夹路径

names:
  0: defect_type_1
  1: defect_type_2
  # 添加更多类别



目录结构示例：
datasets/
    my_dataset/
        images/
            train/  # 训练图片
            val/    # 验证图片
        labels/
            train/  # 训练标签 (YOLO格式)
            val/    # 验证标签 (YOLO格式)







########################yolov11 使用預測多種類/整體   缺陷情況 sample code###########################

import torch
from yolov11 import YOLOv11  # 假设有一个 YOLOv11 的 Python 实现

# 加载预训练的 YOLOv11 模型
model = YOLOv11(pretrained=True)
model.eval()

# 定义验证数据集的路径
val_data_path = 'path/to/your/validation/dataset'  # 替换为您的验证数据集路径

# 执行验证
results = model.val(data=val_data_path)

# 输出整体评估指标
print(f"Overall mAP@0.5: {results['mAP_0.5']:.4f}")
print(f"Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
print(f"Overall Precision: {results['precision']:.4f}")
print(f"Overall Recall: {results['recall']:.4f}")
print(f"Overall F1 Score: {results['f1']:.4f}")

# 输出每个类别的评估指标
for idx, class_name in enumerate(results['names']):
    class_precision = results['per_class_precision'][idx]
    class_recall = results['per_class_recall'][idx]
    class_f1 = results['per_class_f1'][idx]
    class_ap = results['per_class_ap'][idx]
    print(f"Class: {class_name}")
    print(f"  Precision: {class_precision:.4f}")
    print(f"  Recall: {class_recall:.4f}")
    print(f"  F1 Score: {class_f1:.4f}")
    print(f"  AP@0.5: {class_ap:.4f}")



注意事项：

结果字典结构：上述代码假设 results 字典包含以下键：

'mAP_0.5'：整体 mAP@0.5 值。
'mAP_0.5:0.95'：整体 mAP@0.5:0.95 值。
'precision'：整体精度。
'recall'：整体召回率。
'f1'：整体 F1 分数。
'names'：类别名称列表。
'per_class_precision'：每个类别的精度列表。
'per_class_recall'：每个类别的召回率列表。
'per_class_f1'：每个类别的 F1 分数列表。
'per_class_ap'：每个类别的 AP@0.5 值列表。
模型实现：确保您使用的 YOLOv11 实现的 val 方法返回的 results 字典包含上述键。如果没有，您可能需要查看模型的源码，了解如何提取每个类别的评估指标。

评估指标计算：如果模型未提供每个类别的指标，您可能需要手动计算。这涉及将预测结果与真实标签进行比较，计算每个类别的 TP、FP、FN，然后根据这些值计算精度、召回率和 F1 分数。这可能需要深入了解模型的输出格式和评估流程。

通过上述修改，您的代码将能够输出每个缺陷类别的评估指标，帮助您更细致地评估模型在各个类别上的性能。




在目标检测任务中，评估模型性能的指标主要包括精确率（Precision）、召回率（Recall）、F1 分数（F1 Score）、平均精度（AP）和平均精度均值（mAP）。理解这些指标的计算公式和目的，有助于判断模型的优劣，并向他人清晰地解释模型性能。

1. 精确率（Precision）

计算公式：

Precision
=
𝑇
𝑃
𝑇
𝑃
+
𝐹
𝑃
Precision= 
TP+FP
TP
​
 

其中，TP（True Positive）表示被正确检测为目标的数量，FP（False Positive）表示被错误检测为目标的数量。

目的：

精确率衡量模型预测的准确性，即模型预测为目标的实例中，有多少是真正的目标。高精确率表示模型的误报率低。

2. 召回率（Recall）

计算公式：

Recall
=
𝑇
𝑃
𝑇
𝑃
+
𝐹
𝑁
Recall= 
TP+FN
TP
​
 

其中，FN（False Negative）表示被漏检的目标数量。

目的：

召回率衡量模型的检测能力，即实际存在的目标中，有多少被模型成功检测出来。高召回率表示模型的漏检率低。

3. F1 分数（F1 Score）

计算公式：

𝐹
1
=
2
×
Precision
×
Recall
Precision
+
Recall
F1=2× 
Precision+Recall
Precision×Recall
​
 

目的：

F1 分数是精确率和召回率的调和平均，提供了两者之间的平衡评估。当需要在精确率和召回率之间取得平衡时，F1 分数是一个有效的指标。

4. 平均精度（AP）

计算方法：

AP 是 Precision-Recall 曲线下的面积。通过改变检测阈值，绘制出不同的精确率和召回率组合，形成曲线，然后计算该曲线下的面积即为 AP。

目的：

AP 衡量模型在特定类别上的检测性能，综合考虑了不同阈值下的精确率和召回率。

5. 平均精度均值（mAP）

计算方法：

mAP 是对所有类别的 AP 取平均值。

目的：

mAP 提供了模型在所有类别上的总体检测性能，是评估多类别目标检测模型的关键指标。

如何判断模型的好坏

在评估模型性能时，应综合考虑上述指标：

高精确率和高召回率：理想情况下，模型应具有高精确率和高召回率，即误报和漏检都少。

F1 分数：当需要在精确率和召回率之间取得平衡时，关注 F1 分数。

AP 和 mAP：对于多类别检测任务，关注每个类别的 AP 和整体的 mAP，以评估模型在各类别上的性能和总体表现。

通过全面分析这些指标，可以更准确地判断模型的优劣，并为模型的改进提供指导。
############################################################################################################################








##########################################
import torch
from yolov11 import YOLOv11  # 假设有一个 YOLOv11 的 Python 实现

# 加载预训练的 YOLOv11 模型
model = YOLOv11(pretrained=True)
model.eval()

# 定义验证和测试数据集的路径
val_data_path = 'path/to/your/validation/dataset'  # 替换为您的验证数据集路径
test_data_path = 'path/to/your/test/dataset'       # 替换为您的测试数据集路径

def evaluate_model(data_path, dataset_type='Validation'):
    """
    使用指定的数据集评估模型性能。

    参数：
    - data_path: 数据集的路径
    - dataset_type: 数据集类型（'Validation' 或 'Test'）
    """
    print(f"Evaluating on {dataset_type} Dataset...")
    results = model.val(data=data_path)

    # 输出整体评估指标
    print(f"{dataset_type} Dataset - Overall mAP@0.5: {results['mAP_0.5']:.4f}")
    print(f"{dataset_type} Dataset - Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
    print(f"{dataset_type} Dataset - Overall Precision: {results['precision']:.4f}")
    print(f"{dataset_type} Dataset - Overall Recall: {results['recall']:.4f}")
    print(f"{dataset_type} Dataset - Overall F1 Score: {results['f1']:.4f}")

    # 输出每个类别的评估指标
    for idx, class_name in enumerate(results['names']):
        class_precision = results['per_class_precision'][idx]
        class_recall = results['per_class_recall'][idx]
        class_f1 = results['per_class_f1'][idx]
        class_ap = results['per_class_ap'][idx]
        print(f"Class: {class_name}")
        print(f"  Precision: {class_precision:.4f}")
        print(f"  Recall: {class_recall:.4f}")
        print(f"  F1 Score: {class_f1:.4f}")
        print(f"  AP@0.5: {class_ap:.4f}")

# 对验证数据集进行评估
evaluate_model(val_data_path, 'Validation')

# 对测试数据集进行评估
evaluate_model(test_data_path, 'Test')


注意事项：

数据集配置：确保 val_data_path 和 test_data_path 分别指向验证和测试数据集的配置文件（通常为 .yaml 格式）。这些配置文件应包含数据集的相关信息，如图像路径、标签路径和类别名称等。

评估方法：上述代码假设 YOLOv11 类提供了 val 方法，该方法可以接受数据集配置文件的路径，并返回包含评估指标的结果字典。如果您的实现有所不同，请根据实际情况进行调整。

性能指标：results 字典应包含整体和每个类别的评估指标，如 mAP、Precision、Recall、F1 Score 等。如果某些指标不可用，请根据您的需求和模型实现进行相应的修改。

通过上述代码，您可以对验证和测试数据集进行评估，并输出模型在不同数据集上的性能指标。这有助于全面了解模型的表现，并为进一步的优化提供参考。
##########################################################################################################







#####################################################################################################

1. 数据集目录结构示例
假设您的数据集存放在一个根目录下，目录结构如下：


dataset/
├── images/
│   ├── train/     # 训练图像
│   ├── val/       # 验证图像
│   └── test/      # 测试图像
└── labels/
    ├── train/     # 训练标签
    ├── val/       # 验证标签
    └── test/      # 测试标签
images/ 文件夹中存放所有图像文件。

train/ 用于训练的图像。
val/ 用于验证的图像。
test/ 用于测试的图像。
labels/ 文件夹中存放图像对应的标签文件。

每个标签文件的文件名与对应图像相同（但扩展名为 .txt），文件内容采用 YOLO 格式。
2. 标签文件格式说明
每个标签文件采用 YOLO 格式，一行对应图像中的一个目标，格式如下：


<class_id> <x_center> <y_center> <width> <height>
其中：
<class_id>：目标类别的索引（整数），对应配置文件中 names 列表的索引（从 0 开始）。
<x_center> 和 <y_center>：边界框中心点的归一化坐标（相对于图像宽度和高度，取值范围 0～1）。
<width> 和 <height>：边界框的归一化宽度和高度（相对于图像尺寸，取值范围 0～1）。
示例：
假设有一张验证图像 image1.jpg，对应的标签文件 image1.txt 内容如下：

0 0.5 0.5 0.2 0.3
1 0.7 0.8 0.1 0.1
表示图像中有两个目标，第一个目标类别为 0，中心在图像正中央，宽度为图像宽度的 20%、高度为 30%；第二个目标类别为 1，中心位于图像宽度 70% 和高度 80% 的位置，边界框尺寸为图像尺寸的 10%。

3. 数据集配置文件（dataset.yaml）
创建一个名为 dataset.yaml 的文件，用于指定数据集根目录、各子目录及类别信息。例如：

# 数据集配置文件

# 图像和标签的根目录（请替换为您的实际路径）
path: path/to/your/dataset

# 训练、验证和测试图像的子目录（相对于 path）
train: images/train
val: images/val
test: images/test

# 类别数（请替换为您的类别数量）
nc: 3

# 类别名称列表（按索引顺序对应标签中的 class_id）
names:
  0: defect_type_1
  1: defect_type_2
  2: defect_type_3
请确保将 path/to/your/dataset 替换为您的数据集根目录实际路径。

4. 评估代码示例
下面的代码使用您提供的评估代码，并演示如何利用数据集配置文件（例如 dataset.yaml）对验证集和测试集进行评估：




import torch
from yolov11 import YOLOv11  # 假设有一个 YOLOv11 的 Python 实现

# 加载预训练的 YOLOv11 模型
model = YOLOv11(pretrained=True)
model.eval()

# 定义数据集配置文件的路径（包含验证和测试集的配置信息）
data_config_path = 'path/to/your/dataset.yaml'  # 替换为您的数据集配置文件路径

def evaluate_model(data_path, dataset_type='Validation'):
    """
    使用指定的数据集评估模型性能。

    参数：
    - data_path: 数据集配置文件的路径
    - dataset_type: 数据集类型（'Validation' 或 'Test'）
    """
    print(f"Evaluating on {dataset_type} Dataset...")
    results = model.val(data=data_path)

    # 输出整体评估指标
    print(f"{dataset_type} Dataset - Overall mAP@0.5: {results['mAP_0.5']:.4f}")
    print(f"{dataset_type} Dataset - Overall mAP@0.5:0.95: {results['mAP_0.5:0.95']:.4f}")
    print(f"{dataset_type} Dataset - Overall Precision: {results['precision']:.4f}")
    print(f"{dataset_type} Dataset - Overall Recall: {results['recall']:.4f}")
    print(f"{dataset_type} Dataset - Overall F1 Score: {results['f1']:.4f}")

    # 输出每个类别的评估指标
    for idx, class_name in enumerate(results['names']):
        class_precision = results['per_class_precision'][idx]
        class_recall = results['per_class_recall'][idx]
        class_f1 = results['per_class_f1'][idx]
        class_ap = results['per_class_ap'][idx]
        print(f"Class: {class_name}")
        print(f"  Precision: {class_precision:.4f}")
        print(f"  Recall: {class_recall:.4f}")
        print(f"  F1 Score: {class_f1:.4f}")
        print(f"  AP@0.5: {class_ap:.4f}")

# 示例用法：分别对验证和测试数据集进行评估
# 如果验证集和测试集都在 dataset.yaml 中定义，可以直接用同一个配置文件进行评估，
# 模型内部会根据配置文件中相应的 'val' 或 'test' 字段加载对应数据。

print("----- 验证集评估 -----")
evaluate_model(data_config_path, 'Validation')

print("\n----- 测试集评估 -----")
evaluate_model(data_config_path, 'Test')
说明：

在上面的代码中，data_config_path 指向的是数据集配置文件（dataset.yaml），该文件中已经包含了训练、验证和测试集的图像目录信息以及类别设置。
evaluate_model 函数调用 model.val(data=data_path)，这假设您的 YOLOv11 实现支持直接使用配置文件进行评估，并根据配置文件加载相应的数据集。
根据实际实现，如果需要分别指定验证集和测试集的路径，可以调整配置文件或分别传入不同的路径。


#########################################################




from ultralytics import YOLO  # 使用官方API
import torch
from tabulate import tabulate

# 加载官方预训练模型 (注意正确模型名称)
model = YOLO('yolov11n.pt')  # 官方模型名为yolov11n而非yolo11n
model.eval()

def evaluate_model(data_config, dataset_type='val'):
    """
    优化后的多分类评估函数（符合Ultralytics标准）
    
    参数：
    - data_config: 数据集配置文件路径 (.yaml)
    - dataset_type: 数据集类型 ('val' 或 'test')
    """
    print(f"\n{'='*30} {dataset_type.upper()} 评估 {'='*30}")
    
    # 执行验证（添加关键参数）
    results = model.val(
        data=data_config,
        split=dataset_type,  # 官方使用split参数
        plots=True,          # 生成可视化图表
        save_json=True,      # 保存JSON格式结果
        conf=0.01,           # 置信度阈值
        iou=0.6              # IoU阈值
    )
    
    # 全局指标输出
    print(f"\n[全局指标]")
    print(f"mAP@0.5:0.95 | {results.box.map:.4f} (COCO标准)")
    print(f"mAP@0.5      | {results.box.map50:.4f}")
    print(f"Precision    | {results.box.precision.mean():.4f}")
    print(f"Recall       | {results.box.recall.mean():.4f}")
    print(f"推理速度     | {results.speed['inference']:.2f} ms/张")

    # 多分类详细指标（表格化输出）
    print("\n[分类别指标]")
    table_data = []
    for idx, name in enumerate(results.names):
        row = [
            name,
            f"{results.box.ap50[idx]:.4f}",
            f"{results.box.ap75[idx]:.4f}",
            f"{results.box.ap[idx]:.4f}",
            f"{results.box.precision[idx]:.4f}",
            f"{results.box.recall[idx]:.4f}"
        ]
        table_data.append(row)
    
    print(tabulate(table_data,
                 headers=['类别', 'AP50', 'AP75', 'AP50-95', 'Precision', 'Recall'],
                 tablefmt='github'))
    
    # 可视化保存
    print("\n生成可视化文件：")
    print(f"- 混淆矩阵: runs/detect/val/confusion_matrix.png")
    print(f"- 预测示例: runs/detect/val/val_batch_pred.jpg")
    print(f"- JSON结果: runs/detect/val/results.json")

# 配置文件路径（示例）
data_config_path = 'path/to/your/dataset.yaml'

# 执行验证
if __name__ == "__main__":
    # 验证集评估
    evaluate_model(data_config_path, 'val')
    
    # 测试集评估（需要配置文件中定义test路径）
    evaluate_model(data_config_path, 'test')



主要改进点说明：
API标准化：
使用from ultralytics import YOLO替代自定义导入
修正模型名称yolov11n.pt（官方命名规范）
使用split参数替代自定义类型判断
指标输出优化：
增加AP75指标输出
添加推理速度显示
使用表格化输出分类别指标
符合COCO标准指标命名（box.map / box.map50）
参数优化：
添加save_json=True保存可解析结果
设置合理默认值conf=0.01和iou=0.6
自动生成可视化文件路径提示
多分类支持强化：
显示每个类别的AP50/AP75/AP50-95
分类别精确率和召回率单独显示
兼容不同数据分割（val/test）
使用前需确保：
bash
pip install ultralytics tabulate
注意：实际使用时请确保：
dataset.yaml中正确定义test路径
测试集标签文件符合YOLO格式
模型文件与任务匹配（分类数一致）
############################################






YOLOv11 多分类物体检测评估完整指南
一、环境准备
bash
# 安装核心依赖
pip install ultralytics tabulate matplotlib torch
二、数据集配置标准
1. 目录结构规范
bash
datasets/
└── defect_detection/
    ├── train/
    │   ├── images/  # 训练集图像
    │   └── labels/  # YOLO格式标签
    ├── val/
    │   ├── images/  # 验证集图像
    │   └── labels/
    └── test/
        ├── images/  # 测试集图像
        └── labels/
2. 标签文件示例
labels/train/image001.txt:
text
0 0.347656 0.489583 0.128906 0.239583
2 0.712891 0.581250 0.089844 0.166667
3. 数据集配置文件 (defect_config.yaml)
text
path: /projects/datasets/defect_detection
train: train/images
val: val/images
test: test/images

nc: 3
names:
  0: scratch
  1: dent
  2: crack
三、评估代码实现
python
from ultralytics import YOLO
from tabulate import tabulate

def main():
    # 初始化模型
    model = YOLO('yolov11n.pt')
    
    # 执行评估流程
    run_evaluation(model, 'defect_config.yaml')

def run_evaluation(model, config_path):
    """全流程评估函数"""
    # 验证集评估
    print("\n" + "="*40)
    print("Starting Validation Evaluation")
    results_val = evaluate_model(model, config_path, 'val')
    
    # 测试集评估
    print("\n" + "="*40)
    print("Starting Test Evaluation")
    results_test = evaluate_model(model, config_path, 'test')
    
    return results_val, results_test

def evaluate_model(model, data_config, dataset_type='val'):
    """模型评估核心函数"""
    print(f"\n{'='*30} {dataset_type.upper()} EVALUATION {'='*30}")
    
    # 执行验证流程
    results = model.val(
        data=data_config,
        split=dataset_type,
        plots=True,
        save_json=True,
        conf=0.01,
        iou=0.6,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # 输出评估指标
    print_metrics(results)
    
    return results

def print_metrics(results):
    """结构化输出评估指标"""
    # 全局指标
    print("\n[Global Metrics]")
    print(f"mAP@0.5:0.95 | {results.box.map:.4f} (COCO Primary)")
    print(f"mAP@0.5      | {results.box.map50:.4f}")
    print(f"Precision    | {results.box.precision.mean():.4f} ±{results.box.precision.std():.4f}")
    print(f"Recall       | {results.box.recall.mean():.4f} ±{results.box.recall.std():.4f}")
    print(f"Inference Speed | {results.speed['inference']:.2f} ms/img")
    
    # 分类别指标表格
    print("\n[Per-Class Metrics]")
    table_data = [
        [
            name,
            f"{results.box.ap50[idx]:.4f}",
            f"{results.box.ap75[idx]:.4f}",
            f"{results.box.ap[idx]:.4f}",
            f"{results.box.precision[idx]:.4f}",
            f"{results.box.recall[idx]:.4f}"
        ]
        for idx, name in enumerate(results.names)
    ]
    
    print(tabulate(table_data,
                 headers=['Class', 'AP50', 'AP75', 'AP50-95', 'Precision', 'Recall'],
                 tablefmt='github'))
    
    # 输出文件路径
    print("\nGenerated Files:")
    print(f"- 混淆矩阵: runs/detect/{results.save_dir}/confusion_matrix.png")
    print(f"- 预测可视化: runs/detect/{results.save_dir}/val_batch_pred.jpg")
    print(f"- 详细结果: runs/detect/{results.save_dir}/results.json")

if __name__ == "__main__":
    main()
四、关键参数说明
参数	类型	默认值	说明
split	str	'val'	数据集分割类型 (val/test)
plots	bool	True	生成可视化图表
save_json	bool	True	保存JSON格式结果
conf	float	0.01	检测置信度阈值
iou	float	0.6	IoU阈值
device	str	auto	计算设备自动选择
五、执行与输出解读
1. 运行命令
bash
python evaluate.py
2. 典型输出示例
text
============================== VAL EVALUATION =============================

[Global Metrics]
mAP@0.5:0.95 | 0.6723 (COCO Primary)
mAP@0.5      | 0.8521
Precision    | 0.7812 ±0.032
Recall       | 0.6934 ±0.041
Inference Speed | 4.23 ms/img

[Per-Class Metrics]
| Class    |   AP50 |   AP75 |   AP50-95 |   Precision |   Recall |
|----------|--------|--------|-----------|-------------|----------|
| scratch  | 0.8723 | 0.7021 |    0.6423 |      0.8023 |   0.7123 |
| dent     | 0.8345 | 0.6532 |    0.5934 |      0.7623 |   0.6834 |
| crack    | 0.8012 | 0.5921 |    0.5432 |      0.7321 |   0.6532 |
3. 输出文件说明
confusion_matrix.png: 类别混淆矩阵
val_batch_pred.jpg: 典型检测结果可视化
results.json: 包含所有指标的JSON文件
六、高级配置建议
多GPU评估加速：
python
model.val(..., device=[0,1,2,3])  # 使用4块GPU
批量大小优化：
python
model.val(..., batch=64)  # 根据显存调整
特定类别分析：
python
# 在print_metrics函数中添加
print(f"\nClass 'crack'详细指标:")
print(f"- 查准率: {results.box.precision[2]:.4f}")
print(f"- 漏检率: {1 - results.box.recall[2]:.4f}")
本指南完整实现了YOLOv11的评估流程，包含多分类检测的关键指标分析，可满足工业级缺陷检测、商品识别等场景的评估需求。






第二部分：差异图计算

metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = get_diff_map(ref, test, fn_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_thres
###############################




###############################lrf 回家作業#############################################
def get_lrf_file(data_dir, selected_lot_id):
    if os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_ADD.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_ADD.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_Classified.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_Classified.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_classified.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}_classified.lrf")
    elif os.path.exists(os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}.lrf")):
        lrf_file = os.path.join(data_dir, selected_lot_id, f"{selected_lot_id}.lrf")
    else:
        lrf_file = None

    return lrf_file

def read_lrf_file(file_path):
    with open(file_path, 'r') as file:
        content = file.read()
    return content

def detect_defect_list(content):
    pattern = re.compile(r'\[DefectList\]\s+DefectDataColumn.*?DefectDataList\s+\d+\s+(.*?)\s+(?=\[|$)', re.DOTALL)
    match = pattern.search(content)
    if match:
        # logger.debug("DefectList found in the LRF file.")
        return match.group(1)
    # logger.debug("No DefectList found in the LRF file.")
    return None

def extract_no_and_classtype(defect_data):
    # logger.debug(f"defect_data: {defect_data}")

    lines = defect_data.strip().split('\n')
    results = []
    for line in lines:
        parts = line.split()
        if len(parts) >= 9:
            no = parts[0]
            classtype = parts[8]
            results.append((no, classtype))
    # logger.debug(f"results: {results}")
    return results

def get_defect_list(lrf_path):
    # if data_type == "old_data":
    file_path = lrf_path
    content = read_lrf_file(file_path)
    defect_data = detect_defect_list(content)
    image_list = []
    defect_type = []
    if defect_data:
        results = extract_no_and_classtype(defect_data)
        for no, classtype in results:
            # if classtype == '0' or classtype == '1':
            image_list.append(no)
            defect_type.append(classtype)
    # logger.info(f"image_list:{image_list}")
    return image_list, defect_type 




[DefectList]
DefectDataColumn No X Y W H Kind Ch PixelCount ClassType U/L ShfitDir KindBits MaskToMask Block SubBlock StripeNo Label ChgDiff Comment;
DefectDataList 2518
         1 -65575.3190  48572.2706      2.5000      0.8996 0000000000008c8c 00600000 254   0   2   0 0d000000000d00000008011881100f00000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 
         2 -65509.8190  13637.9665      1.2000      0.8496 0000000000000b08 00000060 185   0   2   0 00000000000000c40008011881100f00000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 
         3 -65505.8690  45174.8051      0.5000      0.1999 0000000000004b00 00000030  17   0   2   0 0080000000c004c40008011000000000000000000000000000000000000000000000000000000000   1   0  -1   0       -1     -999 

        2514  63532.3185  -7693.8569      0.3250      0.1499 0000000000000b00 02000000   6   0   2   0 0000000000800c040008000000000000000000000000000000000000000000000000000000000000   1   0  -1 1076       -1     -999 
        2515  63758.7310   9989.8003      0.7500      0.5997 000000000000000b 10000000  11  10   2   0 00000000000000330000000081100100000000000000000000000000000000000000000000000000   1   0  -1 1078       -1     -999 
        2516  64450.4435  14399.3323      1.0750      0.7496 0000000000000b0b 00000020  60   0   2   0 00000000000000f70008211081100f00000000000000000000000000000000000000000000000000   1   0  -1 1083       -1     -999 
        2517  64592.7935  38925.8217      2.6250      0.9495 0000000000004b4b 60000000 403   0   2   0 00b0000000c004f70008011881100f00000000000000000000000000000000000000000000000000   1   0  -1 1085       -1     -999 
        2518  64941.1060  42573.1883      0.3500      0.4998 0000000000000100 0000000c  21  10   2   0 00000000000004040000000000000000000000000000000000000000000000000000000000000000   1   0  -1 1087       -1     -999

######################################labeling config

interfaces = [
  "panel",
  "update",
  "controls",
  "side-column",
  "completions:menu",
  "completions:add-new",
  "completions:delete",
  "predictions:menu",
],

user = {
  'pk': 1,
  'firstName': "Labeler",
  'lastName': "",
},
###############################json function

interfaces = [
  "panel",
  "update",
  "controls",
  "side-column",
  "completions:menu",
  "completions:add-new",
  "completions:delete",
  "predictions:menu",
],

user = {
  'pk': 1,
  'firstName': "Labeler",
  'lastName': "",
},

################## image processor
from PIL import Image
import base64
from io import BytesIO
import numpy as np
import json
import cv2
from alignment.alignment import load_defect_pair, get_diff_map
import matplotlib.pyplot as plt
# Function to convert a NumPy array to a Base64 encoded string

def numpy_to_base64(img_array):
    pil_img = Image.fromarray(img_array)
    buffer = BytesIO()
    pil_img.save(buffer, format="JPEG")
    return base64.b64encode(buffer.getvalue()).decode("utf-8")


# Function to load images from a JSON file line by line and save into a dictionary
def load_images_from_json(json_file):
    images = {}
    with open(json_file, 'r') as f:
        for line in f:
            data = json.loads(line)
            lot_id = data['lot_id']
            images[lot_id] = data
    return images

def apply_colormap(np_array, colormap, vmin=None, vmax=None):
    # norm_array = (np_array - np_array.min()) / (np_array.max() - np_array.min())
    if vmin and vmax:
        norm_array = (np_array - vmin) / (vmax - vmin)
    else:
        norm_array = np_array
    colormap_func = plt.get_cmap(colormap)
    colored_array = colormap_func(norm_array)
    return (colored_array[:, :, :3] * 255).astype(np.uint8)

# Define a 2x2 convolution kernel
def conv_kernel(kernel_size):
    return np.ones((kernel_size, kernel_size), dtype=np.float32) / (kernel_size ** 2)

def get_image_pair_for_studio_input(data_dir, selected_lot_id, selected_image_id, selected_image_type, vmin_level, max_features, max_shift, ransac_reproj_threshold, selected_alignment_method, conv_kernel_size):
    img_dir = f"{data_dir}/{selected_lot_id}/{selected_lot_id}/Images/InstantReview{selected_image_type}"
    fn_id = selected_image_id
    ref, test, is_median_ref = load_defect_pair(img_dir, fn_id)
    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
        get_diff_map(ref, test, fn_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method=selected_alignment_method)
    print(f"max_pos:{metadata['max_pos']}")
    print(f"min_pos:{metadata['min_pos']}")

    ref_colored = apply_colormap(proc_ref, 'gray')
    test_colored = apply_colormap(proc_test, 'gray')
    diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)
    conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))
    img1_base64 = numpy_to_base64(ref_colored)
    img2_base64 = numpy_to_base64(test_colored)
    img3_base64 = numpy_to_base64(diff_colored)
    img4_base64 = numpy_to_base64(conv_diff_image)
    return [img1_base64, img2_base64, img4_base64], metadata

##############inference ########################################


def load_image_as_numpy(image_path):
    img = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format
    return np.array(img)

def preprocess_image(img_np):
    if img_np.ndim == 2:  # If the image is grayscale, convert to RGB
        img_np = np.stack((img_np,) * 3, axis=-1)
    elif img_np.shape[2] == 4:  # If the image has an alpha channel, remove it
        img_np = img_np[:, :, :3]
    return img_np

def inference(img_np, ckpt="/home/hubert007/Code/label_tool/labeling/runs/detect/train3/weights/best.pt"):
    # Load the finetuned model checkpoint
    model = YOLO(ckpt)
    
    # Preprocess the image
    img_np = preprocess_image(img_np)
    
    # Perform object detection
    results = model(img_np)
    # print(results['precision'])
    # print(results['recall'])

    # logger.info(f"inference results: {results}")
    # Extract bounding box information
    if results and len(results[0].boxes) > 0:
        boxs_and_labels = []
        # logger.debug(f"cls length: {len(results[0].boxes.cls)}")
        # logger.debug(f"cls shape: {results[0].boxes.cls.shape}")
        for i in range(len(results[0].boxes.cls)):

            # logger.info(f"Detected boxs: {results[0].boxes}")

            box = results[0].boxes  # Assuming we take the first detected box
            label = box.cls[i]
            x1, y1, x2, y2 = box.xyxy[i]
            width = x2 - x1
            height = y2 - y1
            boxs_and_labels.append((x1, y1, width, height, label))
        return boxs_and_labels
    else:
        return None

# Example usage
if __name__ == "__main__":
    image_path = "/mnt/fs0/dataset/Layer_M/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/1411L.png"
    np_image = load_image_as_numpy(image_path)
    bbox = inference(np_image)
    # print('test result', bbox[0][0], bbox[0][1], bbox[0][2], bbox[0][3], bbox[0][4])  #ok  (x1, y1, width, height, label)

    
    if bbox:
        print(f"Top-left: ({bbox[0][0]}, {bbox[0][1]}), Width: {bbox[0][2]}, Height: {bbox[0][3]}, label: {bbox[0][4]}")
    else:
        print("No bounding box detected.")


######################################fine tune
from ultralytics import YOLO
import os
import argparse
from loguru import logger
import yaml


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=200, help='number of epochs')
    parser.add_argument('--imgsz', type=int, default=256, help='image size')
    parser.add_argument('--exp_name', type=str, default=None, help='name of your experiment')
    parser.add_argument('--dataset_path', type=str, required=True, help='dataset path')
    args = parser.parse_args()



    # Create the directory if it doesn't exist
    os.makedirs('prelabel/model_based/YOLO/train_settings', exist_ok=True)

    # Define the content of the YAML file
    yaml_content = {
        'path': args.dataset_path,
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images',
        'nc': 1,
        'names': {
            0: 'defect'
        }
    }

    # Write the content to the YAML file
    with open(f'prelabel/model_based/YOLO/train_settings/{args.exp_name}.yaml', 'w') as yaml_file:
        yaml.dump(yaml_content, yaml_file, default_flow_style=False)


    # Load a model
    model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

    train_results = model.train(data=f"prelabel/model_based/YOLO/train_settings/{args.exp_name}.yaml", epochs=args.epochs, imgsz=args.imgsz, name=args.exp_name)




    # Export the model to ONNX format
    path = model.export(format="onnx")  # return path to exported model

if __name__ == "__main__":
    main()



#########  dataset#################################################


import shutil
from sklearn.model_selection import train_test_split
import json
import os
from PIL import Image
import numpy as np
from torchvision.utils import save_image
import torch
import cv2
import re
import sqlite3
from PIL import Image, ImageDraw
from alignment.alignment import load_defect_pair, get_diff_map
from frontend.image_processor import numpy_to_base64, load_images_from_json, apply_colormap, conv_kernel
import argparse
from loguru import logger
from tqdm import tqdm
import albumentations as A
from albumentations.pytorch import ToTensorV2

def get_augmentation_pipeline(aug_params):
    """
    Create albumentations augmentation pipeline.
    We use "albumentations" because it can handle the augmentations of bounding boxes. For example, when we rotate the image,
    the bounding boxes should also be rotated.
    :param aug_params: dict of augmentation parameters
    :return: albumentations.Compose object with augmentation pipeline
    """
    return A.Compose([
        A.RandomRotate90(p=aug_params.get('rotation_prob', 0.5)),
        A.HorizontalFlip(p=aug_params.get('flip_prob', 0.5)),
        A.VerticalFlip(p=aug_params.get('flip_prob', 0.5)),
        A.Affine(
            scale=aug_params.get('scale', (1.0, 1.2)),
            translate_percent=(0.0, 0.0),
            p=1.0
        ),
        # A.ColorJitter(
        #     brightness=aug_params.get('brightness', 0.2),
        #     contrast=aug_params.get('contrast', 0.2),
        #     p=1.0
        # ),
        ToTensorV2()
    ], bbox_params=A.BboxParams(format='yolo', label_fields=['category_ids']))


def process_and_augment_images(image_array, bounding_boxes, aug_params, img_path, original=False):
    """
    augment the images and bounding boxes.
    """
    # Ensure the image is in RGB format
    if image_array.shape[2] == 4:  # If the image has an alpha channel
        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGRA2RGB)
    else:
        image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)

    # Prepare bounding boxes and labels
    bboxes = []
    category_ids = []
    for bbox in bounding_boxes:
        x_center = (bbox['x']+bbox['width']/2) / image_array.shape[1]
        y_center = (bbox['y']+bbox['height']/2) / image_array.shape[0]
        width = bbox['width'] / image_array.shape[1]
        height = bbox['height'] / image_array.shape[0]
        x_center = min(1.0, max(0.0, x_center))
        y_center = min(1.0, max(0.0, y_center))
        width = min(1.0, max(0.0, width))
        height = min(1.0, max(0.0, height))

        label = bbox['label']
        
        if label == "Defect_right" or label == "Defect_left":
            category_ids.append(0)
        elif label == "4D":
            return None
        
        bboxes.append([x_center, y_center, width, height])

    # Apply augmentations
    if original == True:
        return bboxes, category_ids
    try:
        transform = get_augmentation_pipeline(aug_params)
        augmented = transform(image=image_array, bboxes=bboxes, category_ids=category_ids)
        return augmented['image'], augmented['bboxes'], augmented['category_ids']
    except:
        logger.debug(f"error on {img_path}")
        return None

    # return augmented['image'], augmented['bboxes'], augmented['category_ids']

def check_image_variants(image_path):
    """
    Get the base path and file name without extension (1070L.png ,1070U.png ....)
    """
    logger.info(f"Checking image variants for {image_path}")
    base_path, file_name = os.path.split(image_path)
    file_name_no_ext = os.path.splitext(file_name)[0]
    
    # Define the variants
    variants = ['L', 'R', 'U', 'D']
    
    # Initialize a dictionary to store the paths of existing variants
    # Check for each variant
    for variant in variants:
        variant_path = os.path.join(base_path, f"{file_name_no_ext}{variant}.png")
        if os.path.exists(variant_path):
            return variant_path
        

    
def db_to_metadata(db_file, metadata_file, masked_image_output=None):
    """
    Convert SQLite database to metadata JSON file.
    """
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Query to retrieve all rows from the results_table
    cursor.execute('SELECT image_path, results_json FROM results_table')
    rows = cursor.fetchall()

    # Create output directory if it doesn't exist
    if masked_image_output:
        output_dir = masked_image_output
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    # Initialize a list to store metadata
    metadata_list = []

    for row in rows:
        img_path, results_json = row
        split_img_path = img_path.split(', ')
        data_dir = split_img_path[0]
        lot_id = split_img_path[1]
        image_id = split_img_path[2]
        results = json.loads(results_json)
        # logger.debug(f"img_path: {img_path}")
        # Check if 'areas' key exists in the JSON
        if 'areas' in results:
            t_boxes = []
            rt_boxes = []
            areas = results['areas']
            for area_key, area_value in areas.items():
                if area_value['object'] == 'image3' or area_value['object'] == 'image6':
                    label = area_value['results'][0]['value']['rectanglelabels'][0]
                    x = area_value['x']
                    y = area_value['y']
                    width = area_value['width']
                    height = area_value['height']
                    if area_value['object'] == 'image3':
                    # Append metadata to the list
                        t_boxes.append(
                            {"x": x, "y": y, "width": width, "height": height, "label": label} 
                        )
                    else:
                        rt_boxes.append(
                            {"x": x, "y": y, "width": width, "height": height, "label": label} 
                        )

            metadata_list.append({
                "image_path": f"{data_dir}/{lot_id}/{lot_id}/Images/InstantReviewT/{image_id}.png",
                "bounding_box": t_boxes,
            })
            metadata_list.append({
                "image_path": f"{data_dir}/{lot_id}/{lot_id}/Images/InstantReviewRt/{image_id}.png",
                "bounding_box": rt_boxes,
            })

    # Write metadata to a JSON file
    with open(metadata_file, 'w') as json_file:
        json.dump(metadata_list, json_file, indent=4)

    # Close the database connection
    conn.close()
    # json_to_text('metadata.json')

def create_dataset(db_file ,json_file, dataset_dir, image_size=256, diff_map=False, aug_params = None, use_background = False, max_augment_factor = 8):

    print(f"diff_map: {diff_map}")
    # Create dataset directories
    train_images_dir = os.path.join(dataset_dir, 'train/images')
    train_labels_dir = os.path.join(dataset_dir, 'train/labels')
    val_images_dir = os.path.join(dataset_dir, 'val/images')
    val_labels_dir = os.path.join(dataset_dir, 'val/labels')
    test_images_dir = os.path.join(dataset_dir, 'test/images')
    test_labels_dir = os.path.join(dataset_dir, 'test/labels')
    
    os.makedirs(train_images_dir, exist_ok=True)
    os.makedirs(train_labels_dir, exist_ok=True)
    os.makedirs(val_images_dir, exist_ok=True)
    os.makedirs(val_labels_dir, exist_ok=True)
    os.makedirs(test_images_dir, exist_ok=True)
    os.makedirs(test_labels_dir, exist_ok=True)

    # Load JSON data
    db_to_metadata(db_file, json_file)
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    # Split data into train and validation sets
    # First, split the data into train+val and test sets
    train_val_data, test_data = train_test_split(data, test_size=0.1, random_state=42)

    # Then, split the train+val data into train and val sets
    train_data, val_data = train_test_split(train_val_data, test_size=0.2222, random_state=42)  # 0.2222 is approximately 2/9

    # Now you have train_data (70%), val_data (20%), and test_data (10%)
    
    def process_data(data, images_dir, labels_dir, data_type):
        for item in tqdm(data, desc=f"Processing {data_type} data"):
            image_path = item['image_path']
            if diff_map:
                max_features = 1000
                max_shift = 10
                ransac_reproj_threshold = 0.20
                image_dir = os.path.dirname(image_path)
                # the lot_id doesn't matter here, it just affects the metadata created below, which won't be used in this case.
                selected_lot_id = "DummyVariable"
                vmin_level = -0.60
                conv_kernel_size = 2
                
                # Extract the filename
                filename = os.path.basename(image_path)
                if 'InstantReviewT' in image_path:
                    image_type = "T"
                elif 'InstantReviewRt' in image_path:
                    image_type = "Rt"

                # Extract the number part of the filename using regular expressions
                image_id = re.search(r'\d+', filename).group()

                ref, test, is_median_ref = load_defect_pair(image_dir, image_id)

                """
                I think there's a bottleneck here. When the image isn't able to be aligned with correlate, the correlation alignment function outputs an error which
                takes a makes the progress bar stuck for a while. This is worth checking out when optimizing the dataset pipeline. 
                """

                # Here we use Correlate for all images because it can handle broader cases than SIFT.
                try:
                    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
                        get_diff_map(ref, test, image_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method="Correlate")
                except:
                    metadata, proc_ref, proc_test, proc_diff, max_pos, min_pos = \
                        get_diff_map(ref, test, image_id, selected_lot_id, is_median_ref, int(max_features), int(max_shift), ransac_reproj_threshold, method="SIFT")                  
                #save proc_diff as png image
                diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)
                conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))
                diff_image_png = Image.fromarray(conv_diff_image)

                # Save as PNG
                
                label_file = f"{image_type}_{image_id}_diff.txt"
                label_path = os.path.join(labels_dir, label_file)             
                # remove last filename from image_path

                
                #Start augmentation
                bounding_boxes = item['bounding_box']

                #before augmentation, add original image and label into dataset
                
                if len(bounding_boxes) == 0:
                    if use_background == True:
                        continue
                    with open(label_path, "w") as f:
                        pass
                        
                else:
                    try:
                        aug_bboxes, aug_labels = process_and_augment_images(conv_diff_image, bounding_boxes, aug_params, image_path, original=True)
                    except:
                        continue
                    for box, label in zip(aug_bboxes, aug_labels):
                        with open(label_path, "a") as f:
                            f.write(f"{label} {box[0]} {box[1]} {box[2]} {box[3]}\n")
                diff_image_png.save(os.path.join(images_dir, f"{image_type}_{image_id}_diff.png"))

                num_of_augments = np.random.randint(1, max_augment_factor)
                
                for i in range(1, num_of_augments+1):
                    #random vmin level [-1.70, -0.30]
                    vmin_level = torch.FloatTensor(1).uniform_(-1.70, -0.30).item()
                    diff_colored = apply_colormap(proc_diff, 'seismic', vmin_level, -vmin_level)

                    #apply random convolution kernel
                    conv_kernel_size = torch.randint(1, 4, (1,)).item()
                    conv_diff_image = cv2.filter2D(diff_colored, -1, conv_kernel(int(conv_kernel_size)))

                    #apply albumentation's augmentations.
                    try:
                        aug_image, aug_bboxes, aug_labels = process_and_augment_images(conv_diff_image, bounding_boxes, aug_params, image_path)
                    except:
                        continue
                    png_path = os.path.join(images_dir, f"{image_type}_{image_id}_diff_{str(i)}.png")
                    label_path = os.path.join(labels_dir, f"{image_type}_{image_id}_diff_{str(i)}.txt")

                    np_image = aug_image.permute(1, 2, 0).cpu().numpy()
                    
                    # Convert the NumPy array to a PIL image
                    pil_image = Image.fromarray(np_image)
                    
                    # Save the PIL image
                    pil_image.save(png_path)
                    if len(aug_bboxes) == 0:
                        if use_background == False:
                            continue
                        label = 'None'
                        with open(label_path, 'w') as f:
                            pass
                    else:
                        for bbox, label in zip(aug_bboxes, aug_labels):
                            with open(label_path, 'a') as f:
                                f.write(f"{label} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\n")
            else:
                # Copy image to the dataset directory
                shutil.copy(image_path, images_dir)
                
                # Create label file
                image_name = os.path.basename(image_path)
                label_file = os.path.splitext(image_name)[0] + '.txt'
                label_path = os.path.join(labels_dir, label_file)

    
    # Process train and validation data


    process_data(train_data, train_images_dir, train_labels_dir, "train")
    process_data(val_data, val_images_dir, val_labels_dir, "val")
    process_data(test_data, test_images_dir, test_labels_dir, "test")


def dataset_checker(dataset_path):
    # Directories to check
    directories = ['train', 'val', 'test']
    
    missing_labels = []
    empty_labels = []

    for directory in directories:
        image_dir = os.path.join(dataset_path, directory, 'images')
        label_dir = os.path.join(dataset_path, directory, 'labels')
        
        # Get list of image files
        image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]
        
        for image_file in image_files:
            label_file = os.path.splitext(image_file)[0] + '.txt'
            label_path = os.path.join(label_dir, label_file)
            
            if not os.path.exists(label_path):
                print(f"no label at {image_file}")
                missing_labels.append(os.path.join(directory, 'images', image_file))
            else:
                with open(label_path, 'r') as f:
                    content = f.read().strip()
                    if not content:
                        print(f"empty label at {label_file}")
                        empty_labels.append(os.path.join(directory, 'labels', label_file))
    
    # return missing_labels, empty_labels



def main():
    parser = argparse.ArgumentParser(description="Create YOLO dataset from .db file")
    parser.add_argument("--db_file" , type=str, help="Path to the .db file")
    parser.add_argument("--json_metadata", type=str, help="Path to the JSON metadata file")
    parser.add_argument("--dataset_path", type=str, help="Path to the dataset directory")
    parser.add_argument("--use_diff_map", type=bool, default=True, help="Use diff map as image for the dataset")
    parser.add_argument("--max_augment_factor", type=int, default=8, help="Max number of augmentation data on each image")
    parser.add_argument("--use_background", action='store_true', default=False, help="Use background images for the dataset")
    args = parser.parse_args()

    aug_params = {
        'rotation_prob': 0.5,
        'flip_prob': 0.5,
        'scale': (1.0, 1.2),
        'brightness': 0.2,
        'contrast': 0.2
    }
    create_dataset(args.db_file, args.json_metadata, args.dataset_path, diff_map=args.use_diff_map, aug_params=aug_params, use_background=args.use_background, max_augment_factor=args.max_augment_factor)


if __name__ == "__main__":
    main()
    # dataset_checker("/home/hubert007/Code/label_tool/labeling/toggle_labelstudio/model_based/YOLO/datasets/m_layer_aug_fix")



######yolo prelabel training
from prelabel.model_based.YOLO.inference import inference


def YOLO_prelabel(t_np_image, rt_np_image, model_path, annotations):
        t_bbox = inference(t_np_image, model_path)
        if t_bbox != None:
            for box in t_bbox:
                x = box[0]
                y = box[1]
                width = box[2]
                height = box[3]
                label = box[4]
                for i in range(1, 4):
                    annotation = {
                        'from_name': f'label_image{i}',
                        'to_name': f'image{i}',
                        'type': 'rectanglelabels',
                        'value': {
                            'rectanglelabels': ['Defect_right'],
                            'x': x.item()*100/256,
                            'y': y.item()*100/256,
                            'width': width.item()*100/256,
                            'height': height.item()*100/256,
                        }
                    }       
                    annotations.append(annotation)       
        rt_bbox = inference(rt_np_image, model_path)
        if rt_bbox != None:
            for box in rt_bbox:
                x = box[0]
                y = box[1]
                width = box[2]
                height = box[3]
                label = box[4]
                # The main diference is here: it is using a different range for the loop (image 4,5,6)
                for i in range(4, 7):
                    annotation = {
                        'from_name': f'label_image{i}',
                        'to_name': f'image{i}',
                        'type': 'rectanglelabels',
                        'value': {
                            'rectanglelabels': ['Defect_right'],
                            'x': x.item()*100/256,
                            'y': y.item()*100/256,
                            'width': width.item()*100/256,
                            'height': height.item()*100/256,
                        }
                    }       
                    annotations.append(annotation)      
                return annotations


# YOLO prelabeler training


## Prepare dataset
**Run commands under /toggle_labelstudio**

After finishing labeling, you will get .db file under /toggle_labelstudio.
Convert that .db file into YOLO training format by running the following command:
```bash
python -m prelabel.model_based.YOLO.dataset --db_file example.db --json_metadata example.json --dataset_path dataset_path
```
* `db_file`: .db file path.
* `json_metadata`: desired metadata json file path (will create for you).
* `dataset_path`: output dataset directory (will create for you).

## Train model

### run finetune

```bash
python -m prelabel.model_based.YOLO.finetune --epochs 200 --imgsz 256 --exp_name "experiment name" --dataset_path "dataset_path"
```
Other arguments will be added later(TODO)

###################################################prelabel.py

import cv2
import numpy as np
import base64
from PIL import Image
from io import BytesIO
from prelabel.model_based.YOLO.api import YOLO_prelabel
from prelabel.rule_based.minmax.api import minmax_prelabel
from loguru import logger

def base64_to_numpy(base64_str):
    img_data = base64.b64decode(base64_str)
    img = Image.open(BytesIO(img_data))
    return np.array(img)

def extract_annotations(existing_labels):
    """
    This function extracts the bounding boxes from "existing_labels", which is the dictionary that "st_labelstudio" returns after submit.
    It is quite important to study how "existing_labels" is structured if you want to add new annotation types.
    "polygonlabels" is deprecated currently since the database doesn't handle that format, but as long as you 
    understand "existing_labels" format, you can add it back.
    """
    annotations = []
    if 'areas' in existing_labels:
        # areas describe all the bounded areas (bounding box, polygon..)
        for area_id, area_data in existing_labels['areas'].items():
            if 'results' in area_data:
                for result in area_data['results']:
                    annotation = {
                        "id": result["id"],
                        "from_name": result['from_name'],
                        "to_name": result['to_name'],
                        "type": result['type']
                    }
                    if annotation["type"] == "polygonlabels":
                        annotation["value"] = {
                            "polygonlabels": result['value']['polygonlabels'],
                            "points": [[point['relativeX'], point['relativeY']] for point in area_data['points']],
                        }
                    elif annotation["type"] == "rectanglelabels":
                        annotation["value"] = {
                            "rectanglelabels": result['value']['rectanglelabels'],
                            # converting absolute pixel values into percentage
                            "x": area_data['x']*100/256,
                            "y": area_data['y']*100/256,
                            "width": area_data['width']*100/256,
                            "height": area_data['height']*100/256,
                            "rotation": area_data['rotation'],
                        }
                    annotations.append(annotation)
    return annotations

def generate_prelabels(metadatas, crop_size, method="minmax", model_path=None, t_image=None, rt_image=None, label_type="rectangle"):
    """
    Call your prelabel logics here.
    You can call/define any prelabel method as long as the list "annotations" contains 6 elements each with the following format:
    {
        'from_name': f'label_image{i}',
        'to_name': f'image{i}',
        'type': 'rectanglelabels',
        'value': {
            'rectanglelabels': ['Defect_right'],
            'x': x coord of bounding box's TOP LEFT in PERCENTAGE [0, 100],
            'y': y coord of bounding box's TOP LEFT in PERCENTAGE [0, 100],
            'width': bounding box width in PERCENTAGE [0, 100],
            'height': bounding box height in PERCENTAGE [0, 100],
        }
    } 

    Basically you have to return six bounding boxes for i is from 1~6, representing 6 images (t_ref, t_test, t_diff, rt_ref, rt_test, rt_diff).
    Then these annotations will be rendered onto UI.
    """
    
    annotations = []
    if method == "YOLO":
        # why does YOLO only take in 2 images? Because we only predict on the diff map. t_image is the t_diffmap vice versa.
        t_np_image = base64_to_numpy(t_image)
        rt_np_image = base64_to_numpy(rt_image)
        YOLO_prelabel(t_np_image, rt_np_image, model_path, annotations)
   
    elif method == "minmax":
        # metadatas contains the min/max point, which is acquired from SIFT's alignment functions.
        # crop_size is the size of the bounding box to bound the min/max point. It can be toggled on sidebar.
        minmax_prelabel(metadatas, crop_size, annotations)
    return annotations

def task_generator(images, crop_size, metadatas=None, method="minmax", model_path=None, label_type="rectangle", existing_labels=None):
    """
    This is the MAIN function that handles our customize logic and pass it in to labelstudio as "predictions". 
    Be aware that both "existing labels" and "prelabel predictions" are passed in as "predictions".
    I didn't use "completions", you can ignore that, also you can disable its UI in the UI settings (gear icon).
    """
    task = {
        'completions': [],
        'predictions': [],
        'id': 1,
        'data': {
            'image1': f"data:image/jpeg;base64,{images[0]}",
            'image2': f"data:image/jpeg;base64,{images[1]}",
            'image3': f"data:image/jpeg;base64,{images[2]}",
            'image4': f"data:image/jpeg;base64,{images[3]}",
            'image5': f"data:image/jpeg;base64,{images[4]}",
            'image6': f"data:image/jpeg;base64,{images[5]}"
        }
    }
    if existing_labels:
        annotations = extract_annotations(existing_labels)
        task['predictions'].append({
            'model_version': 'existing_labels',
            'result': annotations
        })
    else:
        print("no labels found, generating prelabels...")
        annotations = generate_prelabels(metadatas, crop_size, method=method, model_path=model_path, t_image=images[2], rt_image=images[5], label_type=label_type)
        task['predictions'].append({
            'model_version': 'prelabeling',
            'result': annotations
        })

    return task


######export_json_and_mask

import sqlite3
import json
from PIL import Image, ImageDraw
import os
from tqdm import tqdm
import argparse

def check_image_variants(image_path):
    # Get the base path and file name without extension
    base_path, file_name = os.path.split(image_path)
    file_name_no_ext = os.path.splitext(file_name)[0]
    
    # Define the variants
    variants = ['L', 'R', 'U', 'D']
    
    # Initialize a dictionary to store the paths of existing variants
    # Check for each variant
    for variant in variants:
        variant_path = os.path.join(base_path, f"{file_name_no_ext}{variant}.png")
        if os.path.exists(variant_path):
            return variant_path
    
def process_db_and_create_masked_images(db_file, output_dir):
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Query to retrieve all rows from the results_table
    cursor.execute('SELECT image_path, results_json FROM results_table')
    rows = cursor.fetchall()

    # Create output directory if it doesn't exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Initialize a list to store metadata
    metadata_list = []
    for row in tqdm(rows, desc=f"Processing images"):
        img_info, results_json = row
        dataset_dir, lot_id, image_id = img_info.replace(" ","").split(',')
        results = json.loads(results_json)
        img_path = os.path.join(dataset_dir, f"{lot_id}/{lot_id}/Images")
        # Check if 'areas' key exists in the JSON
        if 'areas' in results:
            areas = results['areas']
            for area_key, area_value in areas.items():
                if area_value['object'] == 'image3' or area_value['object'] == 'image6':
                    if area_value['object'] == 'image3':
                        full_img_path = os.path.join(img_path, f"InstantReviewT/{image_id}.png")
                    else:
                        full_img_path = os.path.join(img_path, f"InstantReviewRt/{image_id}.png")
                    """
                    If it comes from the right, the defect image is "fn_id.png", if left, it would be "fn_idL.png" or "fn_idU.png" ... etc.
                    """
                    if area_value['results'][0]['value']['rectanglelabels'][0] == "Defect_left":
                        full_img_path = check_image_variants(full_img_path)
                    x = area_value['x']
                    y = area_value['y']
                    width = area_value['width']
                    height = area_value['height']

                    # Open the image
                    img = Image.open(full_img_path).convert("RGBA")
                    masked_img = Image.new("RGBA", img.size, (0, 0, 0, 255))
                    draw = ImageDraw.Draw(masked_img)

                    # Draw the bounding box on the masked image (make it transparent)
                    draw.rectangle([x, y, x + width, y + height], fill=(0, 0, 0, 0))

                    # Composite the original image with the masked image
                    final_img = Image.alpha_composite(img, masked_img)

                    # Save the final masked image
                    output_path = os.path.join(output_dir, f"{lot_id}_{os.path.basename(full_img_path)}")
                    final_img.save(output_path)
                    
                    # Append metadata to the list
                    metadata_list.append({
                        "image_path": full_img_path,
                        "bounding_box": {
                            "x": x,
                            "y": y,
                            "width": width,
                            "height": height
                        }
                    })

    # Write metadata to a JSON file
    with open(os.path.join(output_dir, 'metadata.json'), 'w') as json_file:
        json.dump(metadata_list, json_file, indent=4)

    # Close the database connection
    conn.close()

def main():
    parser = argparse.ArgumentParser(description="Process a SQLite database and create masked images.")
    parser.add_argument('--db_path', type=str, help='Path to the SQLite database file')
    parser.add_argument('--output_dir', type=str, help='Output directory for the masked images')
    args = parser.parse_args()
    process_db_and_create_masked_images(args.db_path, args.output_dir)


if __name__ == "__main__":
    main()



########################################metadata.json
[
    {
        "image_path": "/mnt/fs0/x9u_detection_result/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/771L.png",
        "bounding_box": {
            "x": 137,
            "y": 111,
            "width": 54,
            "height": 23
        }
    },
    {
        "image_path": "/mnt/fs0/x9u_detection_result/N3_M0-16_20240920_145106/N3_M0-16_20240920_145106/Images/InstantReviewT/822L.png",
        "bounding_box": {
            "x": 124,
            "y": 118,
            "width": 12,
            "height": 15
        }
    },]
#################################alightment.py

"""
This file is modified from inspection_sift.py by Carl.
I've removed some functions since the app doesn't use it. 
The most important functions are "load_defect_pair" and "get_diff_map"
"""




import numpy as np
import cv2
import matplotlib.pyplot as plt
import math
import json
import glob
import os
from loguru import logger
from alignment.SIFT.sift import align_images_sift
from alignment.correlation.correlation import align_images_corr



def normalize_images(ref, test):
    """Normalize images using OpenCV functions."""
    # Convert to float32 for processing
    ref_f = ref.astype(np.float32)
    test_f = test.astype(np.float32)

    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    ref_eq = clahe.apply((ref_f * 255).astype(np.uint8)) / 255.0
    test_eq = clahe.apply((test_f * 255).astype(np.uint8)) / 255.0

    # Normalize to zero mean and unit variance
    ref_norm = cv2.normalize(ref_eq, None, 0, 1, cv2.NORM_MINMAX)
    test_norm = cv2.normalize(test_eq, None, 0, 1, cv2.NORM_MINMAX)

    return ref_norm, test_norm

def load_defect_pair(img_path, defect_no):
    """
    This function finds the test and reference image from the dataset directory. (currently from "/mnt/fs0/dataset/Layer_M")
    It handles the suffixes(L, U, Up, Lp ...)
    """
    test = None
    ref_U = None
    ref_L = None
    ref_Up = None
    ref_Lp = None

    images = glob.glob(f'{img_path}/{str(defect_no)}*')
    for image in images:
        if image == f'{img_path}/{str(defect_no)}.png':
            test = image
        elif image == f'{img_path}/{str(defect_no)}U.png':
            ref_U = image
        elif image == f'{img_path}/{str(defect_no)}L.png':
            ref_L = image
        elif image == f'{img_path}/{str(defect_no)}L_p.png':
            ref_Lp = image
        elif image == f'{img_path}/{str(defect_no)}U_p.png':
            ref_Up = image

    if test is None:
        raise ValueError(f'{defect_no}: No Test Image')

    test = cv2.imread(test, cv2.IMREAD_GRAYSCALE)/255

    # Create list to store all available images for median filtering
    all_images = [test]

    ref_u_img = None
    ref_l_img = None
    ref_up_img = None
    ref_lp_img = None
    if ref_U is not None:
        ref_u_img = cv2.imread(ref_U, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_u_img)

    if ref_L is not None:
        ref_l_img = cv2.imread(ref_L, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_l_img)

    if ref_Up is not None:
        ref_up_img = cv2.imread(ref_Up, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_up_img)

    if ref_Lp is not None:
        ref_lp_img = cv2.imread(ref_Lp, cv2.IMREAD_GRAYSCALE)/255
        all_images.append(ref_lp_img)

    # Only use median if we have more than 3 images, otherwise use ref_U as reference
    if len(all_images) > 3:
        reference = np.median(np.stack(all_images), axis=0)
        is_median_ref = True
    else:
        is_median_ref = False
        if ref_u_img is not None:
            reference = ref_u_img
        elif ref_l_img is not None:
            reference = ref_l_img
        elif ref_lp_img is not None:
            reference = ref_lp_img
        elif ref_up_img is not None:
            reference = ref_up_img
        else:
            raise ValueError(f'{defect_no}: No reference image available')

    return reference, test, is_median_ref

def get_diff_map(ref, test, defect_no, lot_id, is_median_ref, max_features=1000, max_shift=10, ransacReprojThreshold=0.0, method="SIFT"):
    """
    input:
    ref : numpy.ndarray, shape(256, 256), grayscale [0,1]
    test : numpy.ndarray, shape(256, 256), grayscale [0,1]

    This function gets you the difference map and its metadata
    You can add your own alignment methods here. As long as you return the following:
    1.aligned_ref : numpy.ndarray, shape(256, 256), grayscale [0,1]
    2.aligned_test : numpy.ndarray, shape(256, 256), grayscale [0,1]
    3.translation : [translation_x, translation_y]  negative means left/up, positive means right/down
    """
    assert(ref.shape == test.shape)

    # logger.debug(f"ref type:{type(ref)}")
    # logger.debug(f"ref shape:{ref.shape}")
    # logger.debug(f"ref:{ref}")
    # logger.debug(f"test type:{type(test)}")
    # logger.debug(f"test shape:{test.shape}")
    # logger.debug(F"test:{test}")
    # Align images using SIFT
    if method == "SIFT":
        if ransacReprojThreshold > 0.0:
            #use RANSAC to filter feature point outliers
            aligned_ref, aligned_test, translation = align_images_sift(ref, test, max_features=max_features, max_shift=max_shift, RANSAC=True, ransacReprojThreshold=ransacReprojThreshold)
        else:
            aligned_ref, aligned_test, translation = align_images_sift(ref, test, max_features=max_features, max_shift=max_shift, RANSAC=False)

        # Convert translation to regular Python float
        translation = [float(translation[0]), float(translation[1])]

    elif method == "Correlate":
        aligned_ref, aligned_test, translation = align_images_corr(ref, test)


    # Normalize images
    processed_ref, processed_test = normalize_images(aligned_ref, aligned_test)
    processed_diff = processed_ref - processed_test
    # Calculate RMSE and max absolute difference
    rmse = float(np.sqrt(np.mean((processed_ref - processed_test) ** 2)))
    max_diff = float(max(abs(np.max(processed_diff)), abs(np.min(processed_diff))))

    def get_ceiling_floor(translation):
        if translation < 0:
            return -max_shift-2+int(math.floor(translation))
        else:
            return max_shift+2+int(math.ceil(translation))
    int_translation = [get_ceiling_floor(t) for t in translation]
    # Define the region of interest excluding the translation area
    roi_start = [max(0, int_translation[0], -int_translation[0]), max(0, int_translation[1], -int_translation[1])]
    roi_end = [min(processed_diff.shape[0], processed_diff.shape[0] + int_translation[0], processed_diff.shape[0] - int_translation[0]),
            min(processed_diff.shape[1], processed_diff.shape[1] + int_translation[1], processed_diff.shape[1] - int_translation[1])]

    # Extract the region of interest

    roi_diff = processed_diff[roi_start[0]:roi_end[0], roi_start[1]:roi_end[1]]

    # Find max/min positions within the region of interest
    max_pos = [int(x) for x in np.unravel_index(roi_diff.argmax(), roi_diff.shape)]
    min_pos = [int(x) for x in np.unravel_index(roi_diff.argmin(), roi_diff.shape)]
    # x,y coord will be swapped because of the way numpy unravels indices
    max_pos = [max_pos[1], max_pos[0]]
    min_pos = [min_pos[1], min_pos[0]]
    # Adjust positions to the original image coordinates
    max_pos = [max_pos[0] + roi_start[0], max_pos[1] + roi_start[1]]
    min_pos = [min_pos[0] + roi_start[0], min_pos[1] + roi_start[1]]

    # Create metadata dictionary with Python native types
    metadata = {
        'lot_id': lot_id,
        'defect_no': int(defect_no),
        'translation': translation,
        'max_difference': float(np.max(processed_diff)),
        'min_difference': float(np.min(processed_diff)),
        'abs_max_difference': max_diff,
        'rmse': rmse,
        'max_pos': max_pos,
        'min_pos': min_pos,
        'is_median_ref': is_median_ref  # Include the median reference flag
    }

    return metadata, processed_ref, processed_test, processed_diff, max_pos, min_pos



